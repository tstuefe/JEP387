<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60529">60529</a> : imported patch jep387-misc.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "aot/aotLoader.hpp"
  27 #include "classfile/classLoaderDataGraph.hpp"
  28 #include "classfile/javaClasses.inline.hpp"
  29 #include "classfile/stringTable.hpp"
  30 #include "classfile/symbolTable.hpp"
  31 #include "classfile/systemDictionary.hpp"
  32 #include "code/codeCache.hpp"
  33 #include "gc/parallel/parallelArguments.hpp"
  34 #include "gc/parallel/parallelScavengeHeap.inline.hpp"
  35 #include "gc/parallel/parMarkBitMap.inline.hpp"
  36 #include "gc/parallel/psAdaptiveSizePolicy.hpp"
  37 #include "gc/parallel/psCompactionManager.inline.hpp"
  38 #include "gc/parallel/psOldGen.hpp"
  39 #include "gc/parallel/psParallelCompact.inline.hpp"
  40 #include "gc/parallel/psPromotionManager.inline.hpp"
  41 #include "gc/parallel/psRootType.hpp"
  42 #include "gc/parallel/psScavenge.hpp"
  43 #include "gc/parallel/psYoungGen.hpp"
  44 #include "gc/shared/gcCause.hpp"
  45 #include "gc/shared/gcHeapSummary.hpp"
  46 #include "gc/shared/gcId.hpp"
  47 #include "gc/shared/gcLocker.hpp"
  48 #include "gc/shared/gcTimer.hpp"
  49 #include "gc/shared/gcTrace.hpp"
  50 #include "gc/shared/gcTraceTime.inline.hpp"
  51 #include "gc/shared/isGCActiveMark.hpp"
  52 #include "gc/shared/oopStorage.inline.hpp"
  53 #include "gc/shared/oopStorageSet.inline.hpp"
  54 #include "gc/shared/oopStorageSetParState.inline.hpp"
  55 #include "gc/shared/referencePolicy.hpp"
  56 #include "gc/shared/referenceProcessor.hpp"
  57 #include "gc/shared/referenceProcessorPhaseTimes.hpp"
  58 #include "gc/shared/spaceDecorator.inline.hpp"
  59 #include "gc/shared/taskTerminator.hpp"
  60 #include "gc/shared/weakProcessor.hpp"
  61 #include "gc/shared/workerPolicy.hpp"
  62 #include "gc/shared/workgroup.hpp"
  63 #include "logging/log.hpp"
  64 #include "memory/iterator.inline.hpp"
  65 #include "memory/resourceArea.hpp"
  66 #include "memory/universe.hpp"
  67 #include "oops/access.inline.hpp"
  68 #include "oops/instanceClassLoaderKlass.inline.hpp"
  69 #include "oops/instanceKlass.inline.hpp"
  70 #include "oops/instanceMirrorKlass.inline.hpp"
  71 #include "oops/methodData.hpp"
  72 #include "oops/objArrayKlass.inline.hpp"
  73 #include "oops/oop.inline.hpp"
  74 #include "runtime/atomic.hpp"
  75 #include "runtime/handles.inline.hpp"
  76 #include "runtime/safepoint.hpp"
  77 #include "runtime/vmThread.hpp"
  78 #include "services/memTracker.hpp"
  79 #include "services/memoryService.hpp"
  80 #include "utilities/align.hpp"
  81 #include "utilities/debug.hpp"
  82 #include "utilities/events.hpp"
  83 #include "utilities/formatBuffer.hpp"
  84 #include "utilities/macros.hpp"
  85 #include "utilities/stack.inline.hpp"
  86 #if INCLUDE_JVMCI
  87 #include "jvmci/jvmci.hpp"
  88 #endif
  89 
  90 #include &lt;math.h&gt;
  91 
  92 // All sizes are in HeapWords.
  93 const size_t ParallelCompactData::Log2RegionSize  = 16; // 64K words
  94 const size_t ParallelCompactData::RegionSize      = (size_t)1 &lt;&lt; Log2RegionSize;
  95 const size_t ParallelCompactData::RegionSizeBytes =
  96   RegionSize &lt;&lt; LogHeapWordSize;
  97 const size_t ParallelCompactData::RegionSizeOffsetMask = RegionSize - 1;
  98 const size_t ParallelCompactData::RegionAddrOffsetMask = RegionSizeBytes - 1;
  99 const size_t ParallelCompactData::RegionAddrMask       = ~RegionAddrOffsetMask;
 100 
 101 const size_t ParallelCompactData::Log2BlockSize   = 7; // 128 words
 102 const size_t ParallelCompactData::BlockSize       = (size_t)1 &lt;&lt; Log2BlockSize;
 103 const size_t ParallelCompactData::BlockSizeBytes  =
 104   BlockSize &lt;&lt; LogHeapWordSize;
 105 const size_t ParallelCompactData::BlockSizeOffsetMask = BlockSize - 1;
 106 const size_t ParallelCompactData::BlockAddrOffsetMask = BlockSizeBytes - 1;
 107 const size_t ParallelCompactData::BlockAddrMask       = ~BlockAddrOffsetMask;
 108 
 109 const size_t ParallelCompactData::BlocksPerRegion = RegionSize / BlockSize;
 110 const size_t ParallelCompactData::Log2BlocksPerRegion =
 111   Log2RegionSize - Log2BlockSize;
 112 
 113 const ParallelCompactData::RegionData::region_sz_t
 114 ParallelCompactData::RegionData::dc_shift = 27;
 115 
 116 const ParallelCompactData::RegionData::region_sz_t
 117 ParallelCompactData::RegionData::dc_mask = ~0U &lt;&lt; dc_shift;
 118 
 119 const ParallelCompactData::RegionData::region_sz_t
 120 ParallelCompactData::RegionData::dc_one = 0x1U &lt;&lt; dc_shift;
 121 
 122 const ParallelCompactData::RegionData::region_sz_t
 123 ParallelCompactData::RegionData::los_mask = ~dc_mask;
 124 
 125 const ParallelCompactData::RegionData::region_sz_t
 126 ParallelCompactData::RegionData::dc_claimed = 0x8U &lt;&lt; dc_shift;
 127 
 128 const ParallelCompactData::RegionData::region_sz_t
 129 ParallelCompactData::RegionData::dc_completed = 0xcU &lt;&lt; dc_shift;
 130 
 131 SpaceInfo PSParallelCompact::_space_info[PSParallelCompact::last_space_id];
 132 
 133 SpanSubjectToDiscoveryClosure PSParallelCompact::_span_based_discoverer;
 134 ReferenceProcessor* PSParallelCompact::_ref_processor = NULL;
 135 
 136 double PSParallelCompact::_dwl_mean;
 137 double PSParallelCompact::_dwl_std_dev;
 138 double PSParallelCompact::_dwl_first_term;
 139 double PSParallelCompact::_dwl_adjustment;
 140 #ifdef  ASSERT
 141 bool   PSParallelCompact::_dwl_initialized = false;
 142 #endif  // #ifdef ASSERT
 143 
 144 void SplitInfo::record(size_t src_region_idx, size_t partial_obj_size,
 145                        HeapWord* destination)
 146 {
 147   assert(src_region_idx != 0, "invalid src_region_idx");
 148   assert(partial_obj_size != 0, "invalid partial_obj_size argument");
 149   assert(destination != NULL, "invalid destination argument");
 150 
 151   _src_region_idx = src_region_idx;
 152   _partial_obj_size = partial_obj_size;
 153   _destination = destination;
 154 
 155   // These fields may not be updated below, so make sure they're clear.
 156   assert(_dest_region_addr == NULL, "should have been cleared");
 157   assert(_first_src_addr == NULL, "should have been cleared");
 158 
 159   // Determine the number of destination regions for the partial object.
 160   HeapWord* const last_word = destination + partial_obj_size - 1;
 161   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
 162   HeapWord* const beg_region_addr = sd.region_align_down(destination);
 163   HeapWord* const end_region_addr = sd.region_align_down(last_word);
 164 
 165   if (beg_region_addr == end_region_addr) {
 166     // One destination region.
 167     _destination_count = 1;
 168     if (end_region_addr == destination) {
 169       // The destination falls on a region boundary, thus the first word of the
 170       // partial object will be the first word copied to the destination region.
 171       _dest_region_addr = end_region_addr;
 172       _first_src_addr = sd.region_to_addr(src_region_idx);
 173     }
 174   } else {
 175     // Two destination regions.  When copied, the partial object will cross a
 176     // destination region boundary, so a word somewhere within the partial
 177     // object will be the first word copied to the second destination region.
 178     _destination_count = 2;
 179     _dest_region_addr = end_region_addr;
 180     const size_t ofs = pointer_delta(end_region_addr, destination);
 181     assert(ofs &lt; _partial_obj_size, "sanity");
 182     _first_src_addr = sd.region_to_addr(src_region_idx) + ofs;
 183   }
 184 }
 185 
 186 void SplitInfo::clear()
 187 {
 188   _src_region_idx = 0;
 189   _partial_obj_size = 0;
 190   _destination = NULL;
 191   _destination_count = 0;
 192   _dest_region_addr = NULL;
 193   _first_src_addr = NULL;
 194   assert(!is_valid(), "sanity");
 195 }
 196 
 197 #ifdef  ASSERT
 198 void SplitInfo::verify_clear()
 199 {
 200   assert(_src_region_idx == 0, "not clear");
 201   assert(_partial_obj_size == 0, "not clear");
 202   assert(_destination == NULL, "not clear");
 203   assert(_destination_count == 0, "not clear");
 204   assert(_dest_region_addr == NULL, "not clear");
 205   assert(_first_src_addr == NULL, "not clear");
 206 }
 207 #endif  // #ifdef ASSERT
 208 
 209 
 210 void PSParallelCompact::print_on_error(outputStream* st) {
 211   _mark_bitmap.print_on_error(st);
 212 }
 213 
 214 #ifndef PRODUCT
 215 const char* PSParallelCompact::space_names[] = {
 216   "old ", "eden", "from", "to  "
 217 };
 218 
 219 void PSParallelCompact::print_region_ranges() {
 220   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 221     return;
 222   }
 223   Log(gc, compaction) log;
 224   ResourceMark rm;
 225   LogStream ls(log.trace());
 226   Universe::print_on(&amp;ls);
 227   log.trace("space  bottom     top        end        new_top");
 228   log.trace("------ ---------- ---------- ---------- ----------");
 229 
 230   for (unsigned int id = 0; id &lt; last_space_id; ++id) {
 231     const MutableSpace* space = _space_info[id].space();
 232     log.trace("%u %s "
 233               SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10) " "
 234               SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10) " ",
 235               id, space_names[id],
 236               summary_data().addr_to_region_idx(space-&gt;bottom()),
 237               summary_data().addr_to_region_idx(space-&gt;top()),
 238               summary_data().addr_to_region_idx(space-&gt;end()),
 239               summary_data().addr_to_region_idx(_space_info[id].new_top()));
 240   }
 241 }
 242 
 243 void
 244 print_generic_summary_region(size_t i, const ParallelCompactData::RegionData* c)
 245 {
 246 #define REGION_IDX_FORMAT        SIZE_FORMAT_W(7)
 247 #define REGION_DATA_FORMAT       SIZE_FORMAT_W(5)
 248 
 249   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
 250   size_t dci = c-&gt;destination() ? sd.addr_to_region_idx(c-&gt;destination()) : 0;
 251   log_develop_trace(gc, compaction)(
 252       REGION_IDX_FORMAT " " PTR_FORMAT " "
 253       REGION_IDX_FORMAT " " PTR_FORMAT " "
 254       REGION_DATA_FORMAT " " REGION_DATA_FORMAT " "
 255       REGION_DATA_FORMAT " " REGION_IDX_FORMAT " %d",
 256       i, p2i(c-&gt;data_location()), dci, p2i(c-&gt;destination()),
 257       c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 258       c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 259 
 260 #undef  REGION_IDX_FORMAT
 261 #undef  REGION_DATA_FORMAT
 262 }
 263 
 264 void
 265 print_generic_summary_data(ParallelCompactData&amp; summary_data,
 266                            HeapWord* const beg_addr,
 267                            HeapWord* const end_addr)
 268 {
 269   size_t total_words = 0;
 270   size_t i = summary_data.addr_to_region_idx(beg_addr);
 271   const size_t last = summary_data.addr_to_region_idx(end_addr);
 272   HeapWord* pdest = 0;
 273 
 274   while (i &lt; last) {
 275     ParallelCompactData::RegionData* c = summary_data.region(i);
 276     if (c-&gt;data_size() != 0 || c-&gt;destination() != pdest) {
 277       print_generic_summary_region(i, c);
 278       total_words += c-&gt;data_size();
 279       pdest = c-&gt;destination();
 280     }
 281     ++i;
 282   }
 283 
 284   log_develop_trace(gc, compaction)("summary_data_bytes=" SIZE_FORMAT, total_words * HeapWordSize);
 285 }
 286 
 287 void
 288 PSParallelCompact::print_generic_summary_data(ParallelCompactData&amp; summary_data,
 289                                               HeapWord* const beg_addr,
 290                                               HeapWord* const end_addr) {
 291   ::print_generic_summary_data(summary_data,beg_addr, end_addr);
 292 }
 293 
 294 void
 295 print_generic_summary_data(ParallelCompactData&amp; summary_data,
 296                            SpaceInfo* space_info)
 297 {
 298   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 299     return;
 300   }
 301 
 302   for (unsigned int id = 0; id &lt; PSParallelCompact::last_space_id; ++id) {
 303     const MutableSpace* space = space_info[id].space();
 304     print_generic_summary_data(summary_data, space-&gt;bottom(),
 305                                MAX2(space-&gt;top(), space_info[id].new_top()));
 306   }
 307 }
 308 
 309 void
 310 print_initial_summary_data(ParallelCompactData&amp; summary_data,
 311                            const MutableSpace* space) {
 312   if (space-&gt;top() == space-&gt;bottom()) {
 313     return;
 314   }
 315 
 316   const size_t region_size = ParallelCompactData::RegionSize;
 317   typedef ParallelCompactData::RegionData RegionData;
 318   HeapWord* const top_aligned_up = summary_data.region_align_up(space-&gt;top());
 319   const size_t end_region = summary_data.addr_to_region_idx(top_aligned_up);
 320   const RegionData* c = summary_data.region(end_region - 1);
 321   HeapWord* end_addr = c-&gt;destination() + c-&gt;data_size();
 322   const size_t live_in_space = pointer_delta(end_addr, space-&gt;bottom());
 323 
 324   // Print (and count) the full regions at the beginning of the space.
 325   size_t full_region_count = 0;
 326   size_t i = summary_data.addr_to_region_idx(space-&gt;bottom());
 327   while (i &lt; end_region &amp;&amp; summary_data.region(i)-&gt;data_size() == region_size) {
 328     ParallelCompactData::RegionData* c = summary_data.region(i);
 329     log_develop_trace(gc, compaction)(
 330         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d",
 331         i, p2i(c-&gt;destination()),
 332         c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 333         c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 334     ++full_region_count;
 335     ++i;
 336   }
 337 
 338   size_t live_to_right = live_in_space - full_region_count * region_size;
 339 
 340   double max_reclaimed_ratio = 0.0;
 341   size_t max_reclaimed_ratio_region = 0;
 342   size_t max_dead_to_right = 0;
 343   size_t max_live_to_right = 0;
 344 
 345   // Print the 'reclaimed ratio' for regions while there is something live in
 346   // the region or to the right of it.  The remaining regions are empty (and
 347   // uninteresting), and computing the ratio will result in division by 0.
 348   while (i &lt; end_region &amp;&amp; live_to_right &gt; 0) {
 349     c = summary_data.region(i);
 350     HeapWord* const region_addr = summary_data.region_to_addr(i);
 351     const size_t used_to_right = pointer_delta(space-&gt;top(), region_addr);
 352     const size_t dead_to_right = used_to_right - live_to_right;
 353     const double reclaimed_ratio = double(dead_to_right) / live_to_right;
 354 
 355     if (reclaimed_ratio &gt; max_reclaimed_ratio) {
 356             max_reclaimed_ratio = reclaimed_ratio;
 357             max_reclaimed_ratio_region = i;
 358             max_dead_to_right = dead_to_right;
 359             max_live_to_right = live_to_right;
 360     }
 361 
 362     ParallelCompactData::RegionData* c = summary_data.region(i);
 363     log_develop_trace(gc, compaction)(
 364         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d"
 365         "%12.10f " SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10),
 366         i, p2i(c-&gt;destination()),
 367         c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 368         c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count(),
 369         reclaimed_ratio, dead_to_right, live_to_right);
 370 
 371 
 372     live_to_right -= c-&gt;data_size();
 373     ++i;
 374   }
 375 
 376   // Any remaining regions are empty.  Print one more if there is one.
 377   if (i &lt; end_region) {
 378     ParallelCompactData::RegionData* c = summary_data.region(i);
 379     log_develop_trace(gc, compaction)(
 380         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d",
 381          i, p2i(c-&gt;destination()),
 382          c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 383          c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 384   }
 385 
 386   log_develop_trace(gc, compaction)("max:  " SIZE_FORMAT_W(4) " d2r=" SIZE_FORMAT_W(10) " l2r=" SIZE_FORMAT_W(10) " max_ratio=%14.12f",
 387                                     max_reclaimed_ratio_region, max_dead_to_right, max_live_to_right, max_reclaimed_ratio);
 388 }
 389 
 390 void
 391 print_initial_summary_data(ParallelCompactData&amp; summary_data,
 392                            SpaceInfo* space_info) {
 393   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 394     return;
 395   }
 396 
 397   unsigned int id = PSParallelCompact::old_space_id;
 398   const MutableSpace* space;
 399   do {
 400     space = space_info[id].space();
 401     print_initial_summary_data(summary_data, space);
 402   } while (++id &lt; PSParallelCompact::eden_space_id);
 403 
 404   do {
 405     space = space_info[id].space();
 406     print_generic_summary_data(summary_data, space-&gt;bottom(), space-&gt;top());
 407   } while (++id &lt; PSParallelCompact::last_space_id);
 408 }
 409 #endif  // #ifndef PRODUCT
 410 
 411 #ifdef  ASSERT
 412 size_t add_obj_count;
 413 size_t add_obj_size;
 414 size_t mark_bitmap_count;
 415 size_t mark_bitmap_size;
 416 #endif  // #ifdef ASSERT
 417 
 418 ParallelCompactData::ParallelCompactData() :
 419   _region_start(NULL),
 420   DEBUG_ONLY(_region_end(NULL) COMMA)
 421   _region_vspace(NULL),
 422   _reserved_byte_size(0),
 423   _region_data(NULL),
 424   _region_count(0),
 425   _block_vspace(NULL),
 426   _block_data(NULL),
 427   _block_count(0) {}
 428 
 429 bool ParallelCompactData::initialize(MemRegion covered_region)
 430 {
 431   _region_start = covered_region.start();
 432   const size_t region_size = covered_region.word_size();
 433   DEBUG_ONLY(_region_end = _region_start + region_size;)
 434 
 435   assert(region_align_down(_region_start) == _region_start,
 436          "region start not aligned");
 437   assert((region_size &amp; RegionSizeOffsetMask) == 0,
 438          "region size not a multiple of RegionSize");
 439 
 440   bool result = initialize_region_data(region_size) &amp;&amp; initialize_block_data();
 441   return result;
 442 }
 443 
 444 PSVirtualSpace*
 445 ParallelCompactData::create_vspace(size_t count, size_t element_size)
 446 {
 447   const size_t raw_bytes = count * element_size;
 448   const size_t page_sz = os::page_size_for_region_aligned(raw_bytes, 10);
 449   const size_t granularity = os::vm_allocation_granularity();
 450   _reserved_byte_size = align_up(raw_bytes, MAX2(page_sz, granularity));
 451 
 452   const size_t rs_align = page_sz == (size_t) os::vm_page_size() ? 0 :
 453     MAX2(page_sz, granularity);
 454   ReservedSpace rs(_reserved_byte_size, rs_align, rs_align &gt; 0);
 455   os::trace_page_sizes("Parallel Compact Data", raw_bytes, raw_bytes, page_sz, rs.base(),
 456                        rs.size());
 457 
 458   MemTracker::record_virtual_memory_type((address)rs.base(), mtGC);
 459 
 460   PSVirtualSpace* vspace = new PSVirtualSpace(rs, page_sz);
 461   if (vspace != 0) {
 462     if (vspace-&gt;expand_by(_reserved_byte_size)) {
 463       return vspace;
 464     }
 465     delete vspace;
 466     // Release memory reserved in the space.
 467     rs.release();
 468   }
 469 
 470   return 0;
 471 }
 472 
 473 bool ParallelCompactData::initialize_region_data(size_t region_size)
 474 {
 475   const size_t count = (region_size + RegionSizeOffsetMask) &gt;&gt; Log2RegionSize;
 476   _region_vspace = create_vspace(count, sizeof(RegionData));
 477   if (_region_vspace != 0) {
 478     _region_data = (RegionData*)_region_vspace-&gt;reserved_low_addr();
 479     _region_count = count;
 480     return true;
 481   }
 482   return false;
 483 }
 484 
 485 bool ParallelCompactData::initialize_block_data()
 486 {
 487   assert(_region_count != 0, "region data must be initialized first");
 488   const size_t count = _region_count &lt;&lt; Log2BlocksPerRegion;
 489   _block_vspace = create_vspace(count, sizeof(BlockData));
 490   if (_block_vspace != 0) {
 491     _block_data = (BlockData*)_block_vspace-&gt;reserved_low_addr();
 492     _block_count = count;
 493     return true;
 494   }
 495   return false;
 496 }
 497 
 498 void ParallelCompactData::clear()
 499 {
 500   memset(_region_data, 0, _region_vspace-&gt;committed_size());
 501   memset(_block_data, 0, _block_vspace-&gt;committed_size());
 502 }
 503 
 504 void ParallelCompactData::clear_range(size_t beg_region, size_t end_region) {
 505   assert(beg_region &lt;= _region_count, "beg_region out of range");
 506   assert(end_region &lt;= _region_count, "end_region out of range");
 507   assert(RegionSize % BlockSize == 0, "RegionSize not a multiple of BlockSize");
 508 
 509   const size_t region_cnt = end_region - beg_region;
 510   memset(_region_data + beg_region, 0, region_cnt * sizeof(RegionData));
 511 
 512   const size_t beg_block = beg_region * BlocksPerRegion;
 513   const size_t block_cnt = region_cnt * BlocksPerRegion;
 514   memset(_block_data + beg_block, 0, block_cnt * sizeof(BlockData));
 515 }
 516 
 517 HeapWord* ParallelCompactData::partial_obj_end(size_t region_idx) const
 518 {
 519   const RegionData* cur_cp = region(region_idx);
 520   const RegionData* const end_cp = region(region_count() - 1);
 521 
 522   HeapWord* result = region_to_addr(region_idx);
 523   if (cur_cp &lt; end_cp) {
 524     do {
 525       result += cur_cp-&gt;partial_obj_size();
 526     } while (cur_cp-&gt;partial_obj_size() == RegionSize &amp;&amp; ++cur_cp &lt; end_cp);
 527   }
 528   return result;
 529 }
 530 
 531 void ParallelCompactData::add_obj(HeapWord* addr, size_t len)
 532 {
 533   const size_t obj_ofs = pointer_delta(addr, _region_start);
 534   const size_t beg_region = obj_ofs &gt;&gt; Log2RegionSize;
 535   const size_t end_region = (obj_ofs + len - 1) &gt;&gt; Log2RegionSize;
 536 
 537   DEBUG_ONLY(Atomic::inc(&amp;add_obj_count);)
 538   DEBUG_ONLY(Atomic::add(&amp;add_obj_size, len);)
 539 
 540   if (beg_region == end_region) {
 541     // All in one region.
 542     _region_data[beg_region].add_live_obj(len);
 543     return;
 544   }
 545 
 546   // First region.
 547   const size_t beg_ofs = region_offset(addr);
 548   _region_data[beg_region].add_live_obj(RegionSize - beg_ofs);
 549 
 550   Klass* klass = ((oop)addr)-&gt;klass();
 551   // Middle regions--completely spanned by this object.
 552   for (size_t region = beg_region + 1; region &lt; end_region; ++region) {
 553     _region_data[region].set_partial_obj_size(RegionSize);
 554     _region_data[region].set_partial_obj_addr(addr);
 555   }
 556 
 557   // Last region.
 558   const size_t end_ofs = region_offset(addr + len - 1);
 559   _region_data[end_region].set_partial_obj_size(end_ofs + 1);
 560   _region_data[end_region].set_partial_obj_addr(addr);
 561 }
 562 
 563 void
 564 ParallelCompactData::summarize_dense_prefix(HeapWord* beg, HeapWord* end)
 565 {
 566   assert(region_offset(beg) == 0, "not RegionSize aligned");
 567   assert(region_offset(end) == 0, "not RegionSize aligned");
 568 
 569   size_t cur_region = addr_to_region_idx(beg);
 570   const size_t end_region = addr_to_region_idx(end);
 571   HeapWord* addr = beg;
 572   while (cur_region &lt; end_region) {
 573     _region_data[cur_region].set_destination(addr);
 574     _region_data[cur_region].set_destination_count(0);
 575     _region_data[cur_region].set_source_region(cur_region);
 576     _region_data[cur_region].set_data_location(addr);
 577 
 578     // Update live_obj_size so the region appears completely full.
 579     size_t live_size = RegionSize - _region_data[cur_region].partial_obj_size();
 580     _region_data[cur_region].set_live_obj_size(live_size);
 581 
 582     ++cur_region;
 583     addr += RegionSize;
 584   }
 585 }
 586 
 587 // Find the point at which a space can be split and, if necessary, record the
 588 // split point.
 589 //
 590 // If the current src region (which overflowed the destination space) doesn't
 591 // have a partial object, the split point is at the beginning of the current src
 592 // region (an "easy" split, no extra bookkeeping required).
 593 //
 594 // If the current src region has a partial object, the split point is in the
 595 // region where that partial object starts (call it the split_region).  If
 596 // split_region has a partial object, then the split point is just after that
 597 // partial object (a "hard" split where we have to record the split data and
 598 // zero the partial_obj_size field).  With a "hard" split, we know that the
 599 // partial_obj ends within split_region because the partial object that caused
 600 // the overflow starts in split_region.  If split_region doesn't have a partial
 601 // obj, then the split is at the beginning of split_region (another "easy"
 602 // split).
 603 HeapWord*
 604 ParallelCompactData::summarize_split_space(size_t src_region,
 605                                            SplitInfo&amp; split_info,
 606                                            HeapWord* destination,
 607                                            HeapWord* target_end,
 608                                            HeapWord** target_next)
 609 {
 610   assert(destination &lt;= target_end, "sanity");
 611   assert(destination + _region_data[src_region].data_size() &gt; target_end,
 612     "region should not fit into target space");
 613   assert(is_region_aligned(target_end), "sanity");
 614 
 615   size_t split_region = src_region;
 616   HeapWord* split_destination = destination;
 617   size_t partial_obj_size = _region_data[src_region].partial_obj_size();
 618 
 619   if (destination + partial_obj_size &gt; target_end) {
 620     // The split point is just after the partial object (if any) in the
 621     // src_region that contains the start of the object that overflowed the
 622     // destination space.
 623     //
 624     // Find the start of the "overflow" object and set split_region to the
 625     // region containing it.
 626     HeapWord* const overflow_obj = _region_data[src_region].partial_obj_addr();
 627     split_region = addr_to_region_idx(overflow_obj);
 628 
 629     // Clear the source_region field of all destination regions whose first word
 630     // came from data after the split point (a non-null source_region field
 631     // implies a region must be filled).
 632     //
 633     // An alternative to the simple loop below:  clear during post_compact(),
 634     // which uses memcpy instead of individual stores, and is easy to
 635     // parallelize.  (The downside is that it clears the entire RegionData
 636     // object as opposed to just one field.)
 637     //
 638     // post_compact() would have to clear the summary data up to the highest
 639     // address that was written during the summary phase, which would be
 640     //
 641     //         max(top, max(new_top, clear_top))
 642     //
 643     // where clear_top is a new field in SpaceInfo.  Would have to set clear_top
 644     // to target_end.
 645     const RegionData* const sr = region(split_region);
 646     const size_t beg_idx =
 647       addr_to_region_idx(region_align_up(sr-&gt;destination() +
 648                                          sr-&gt;partial_obj_size()));
 649     const size_t end_idx = addr_to_region_idx(target_end);
 650 
 651     log_develop_trace(gc, compaction)("split:  clearing source_region field in [" SIZE_FORMAT ", " SIZE_FORMAT ")", beg_idx, end_idx);
 652     for (size_t idx = beg_idx; idx &lt; end_idx; ++idx) {
 653       _region_data[idx].set_source_region(0);
 654     }
 655 
 656     // Set split_destination and partial_obj_size to reflect the split region.
 657     split_destination = sr-&gt;destination();
 658     partial_obj_size = sr-&gt;partial_obj_size();
 659   }
 660 
 661   // The split is recorded only if a partial object extends onto the region.
 662   if (partial_obj_size != 0) {
 663     _region_data[split_region].set_partial_obj_size(0);
 664     split_info.record(split_region, partial_obj_size, split_destination);
 665   }
 666 
 667   // Setup the continuation addresses.
 668   *target_next = split_destination + partial_obj_size;
 669   HeapWord* const source_next = region_to_addr(split_region) + partial_obj_size;
 670 
 671   if (log_develop_is_enabled(Trace, gc, compaction)) {
 672     const char * split_type = partial_obj_size == 0 ? "easy" : "hard";
 673     log_develop_trace(gc, compaction)("%s split:  src=" PTR_FORMAT " src_c=" SIZE_FORMAT " pos=" SIZE_FORMAT,
 674                                       split_type, p2i(source_next), split_region, partial_obj_size);
 675     log_develop_trace(gc, compaction)("%s split:  dst=" PTR_FORMAT " dst_c=" SIZE_FORMAT " tn=" PTR_FORMAT,
 676                                       split_type, p2i(split_destination),
 677                                       addr_to_region_idx(split_destination),
 678                                       p2i(*target_next));
 679 
 680     if (partial_obj_size != 0) {
 681       HeapWord* const po_beg = split_info.destination();
 682       HeapWord* const po_end = po_beg + split_info.partial_obj_size();
 683       log_develop_trace(gc, compaction)("%s split:  po_beg=" PTR_FORMAT " " SIZE_FORMAT " po_end=" PTR_FORMAT " " SIZE_FORMAT,
 684                                         split_type,
 685                                         p2i(po_beg), addr_to_region_idx(po_beg),
 686                                         p2i(po_end), addr_to_region_idx(po_end));
 687     }
 688   }
 689 
 690   return source_next;
 691 }
 692 
 693 bool ParallelCompactData::summarize(SplitInfo&amp; split_info,
 694                                     HeapWord* source_beg, HeapWord* source_end,
 695                                     HeapWord** source_next,
 696                                     HeapWord* target_beg, HeapWord* target_end,
 697                                     HeapWord** target_next)
 698 {
 699   HeapWord* const source_next_val = source_next == NULL ? NULL : *source_next;
 700   log_develop_trace(gc, compaction)(
 701       "sb=" PTR_FORMAT " se=" PTR_FORMAT " sn=" PTR_FORMAT
 702       "tb=" PTR_FORMAT " te=" PTR_FORMAT " tn=" PTR_FORMAT,
 703       p2i(source_beg), p2i(source_end), p2i(source_next_val),
 704       p2i(target_beg), p2i(target_end), p2i(*target_next));
 705 
 706   size_t cur_region = addr_to_region_idx(source_beg);
 707   const size_t end_region = addr_to_region_idx(region_align_up(source_end));
 708 
 709   HeapWord *dest_addr = target_beg;
 710   while (cur_region &lt; end_region) {
 711     // The destination must be set even if the region has no data.
 712     _region_data[cur_region].set_destination(dest_addr);
 713 
 714     size_t words = _region_data[cur_region].data_size();
 715     if (words &gt; 0) {
 716       // If cur_region does not fit entirely into the target space, find a point
 717       // at which the source space can be 'split' so that part is copied to the
 718       // target space and the rest is copied elsewhere.
 719       if (dest_addr + words &gt; target_end) {
 720         assert(source_next != NULL, "source_next is NULL when splitting");
 721         *source_next = summarize_split_space(cur_region, split_info, dest_addr,
 722                                              target_end, target_next);
 723         return false;
 724       }
 725 
 726       // Compute the destination_count for cur_region, and if necessary, update
 727       // source_region for a destination region.  The source_region field is
 728       // updated if cur_region is the first (left-most) region to be copied to a
 729       // destination region.
 730       //
 731       // The destination_count calculation is a bit subtle.  A region that has
 732       // data that compacts into itself does not count itself as a destination.
 733       // This maintains the invariant that a zero count means the region is
 734       // available and can be claimed and then filled.
 735       uint destination_count = 0;
 736       if (split_info.is_split(cur_region)) {
 737         // The current region has been split:  the partial object will be copied
 738         // to one destination space and the remaining data will be copied to
 739         // another destination space.  Adjust the initial destination_count and,
 740         // if necessary, set the source_region field if the partial object will
 741         // cross a destination region boundary.
 742         destination_count = split_info.destination_count();
 743         if (destination_count == 2) {
 744           size_t dest_idx = addr_to_region_idx(split_info.dest_region_addr());
 745           _region_data[dest_idx].set_source_region(cur_region);
 746         }
 747       }
 748 
 749       HeapWord* const last_addr = dest_addr + words - 1;
 750       const size_t dest_region_1 = addr_to_region_idx(dest_addr);
 751       const size_t dest_region_2 = addr_to_region_idx(last_addr);
 752 
 753       // Initially assume that the destination regions will be the same and
 754       // adjust the value below if necessary.  Under this assumption, if
 755       // cur_region == dest_region_2, then cur_region will be compacted
 756       // completely into itself.
 757       destination_count += cur_region == dest_region_2 ? 0 : 1;
 758       if (dest_region_1 != dest_region_2) {
 759         // Destination regions differ; adjust destination_count.
 760         destination_count += 1;
 761         // Data from cur_region will be copied to the start of dest_region_2.
 762         _region_data[dest_region_2].set_source_region(cur_region);
 763       } else if (region_offset(dest_addr) == 0) {
 764         // Data from cur_region will be copied to the start of the destination
 765         // region.
 766         _region_data[dest_region_1].set_source_region(cur_region);
 767       }
 768 
 769       _region_data[cur_region].set_destination_count(destination_count);
 770       _region_data[cur_region].set_data_location(region_to_addr(cur_region));
 771       dest_addr += words;
 772     }
 773 
 774     ++cur_region;
 775   }
 776 
 777   *target_next = dest_addr;
 778   return true;
 779 }
 780 
 781 HeapWord* ParallelCompactData::calc_new_pointer(HeapWord* addr, ParCompactionManager* cm) {
 782   assert(addr != NULL, "Should detect NULL oop earlier");
 783   assert(ParallelScavengeHeap::heap()-&gt;is_in(addr), "not in heap");
 784   assert(PSParallelCompact::mark_bitmap()-&gt;is_marked(addr), "not marked");
 785 
 786   // Region covering the object.
 787   RegionData* const region_ptr = addr_to_region_ptr(addr);
 788   HeapWord* result = region_ptr-&gt;destination();
 789 
 790   // If the entire Region is live, the new location is region-&gt;destination + the
 791   // offset of the object within in the Region.
 792 
 793   // Run some performance tests to determine if this special case pays off.  It
 794   // is worth it for pointers into the dense prefix.  If the optimization to
 795   // avoid pointer updates in regions that only point to the dense prefix is
 796   // ever implemented, this should be revisited.
 797   if (region_ptr-&gt;data_size() == RegionSize) {
 798     result += region_offset(addr);
 799     return result;
 800   }
 801 
 802   // Otherwise, the new location is region-&gt;destination + block offset + the
 803   // number of live words in the Block that are (a) to the left of addr and (b)
 804   // due to objects that start in the Block.
 805 
 806   // Fill in the block table if necessary.  This is unsynchronized, so multiple
 807   // threads may fill the block table for a region (harmless, since it is
 808   // idempotent).
 809   if (!region_ptr-&gt;blocks_filled()) {
 810     PSParallelCompact::fill_blocks(addr_to_region_idx(addr));
 811     region_ptr-&gt;set_blocks_filled();
 812   }
 813 
 814   HeapWord* const search_start = block_align_down(addr);
 815   const size_t block_offset = addr_to_block_ptr(addr)-&gt;offset();
 816 
 817   const ParMarkBitMap* bitmap = PSParallelCompact::mark_bitmap();
 818   const size_t live = bitmap-&gt;live_words_in_range(cm, search_start, oop(addr));
 819   result += block_offset + live;
 820   DEBUG_ONLY(PSParallelCompact::check_new_location(addr, result));
 821   return result;
 822 }
 823 
 824 #ifdef ASSERT
 825 void ParallelCompactData::verify_clear(const PSVirtualSpace* vspace)
 826 {
 827   const size_t* const beg = (const size_t*)vspace-&gt;committed_low_addr();
 828   const size_t* const end = (const size_t*)vspace-&gt;committed_high_addr();
 829   for (const size_t* p = beg; p &lt; end; ++p) {
 830     assert(*p == 0, "not zero");
 831   }
 832 }
 833 
 834 void ParallelCompactData::verify_clear()
 835 {
 836   verify_clear(_region_vspace);
 837   verify_clear(_block_vspace);
 838 }
 839 #endif  // #ifdef ASSERT
 840 
 841 STWGCTimer          PSParallelCompact::_gc_timer;
 842 ParallelOldTracer   PSParallelCompact::_gc_tracer;
 843 elapsedTimer        PSParallelCompact::_accumulated_time;
 844 unsigned int        PSParallelCompact::_total_invocations = 0;
 845 unsigned int        PSParallelCompact::_maximum_compaction_gc_num = 0;
 846 CollectorCounters*  PSParallelCompact::_counters = NULL;
 847 ParMarkBitMap       PSParallelCompact::_mark_bitmap;
 848 ParallelCompactData PSParallelCompact::_summary_data;
 849 
 850 PSParallelCompact::IsAliveClosure PSParallelCompact::_is_alive_closure;
 851 
 852 bool PSParallelCompact::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()-&gt;is_marked(p); }
 853 
 854 class PCReferenceProcessor: public ReferenceProcessor {
 855 public:
 856   PCReferenceProcessor(
 857     BoolObjectClosure* is_subject_to_discovery,
 858     BoolObjectClosure* is_alive_non_header) :
 859       ReferenceProcessor(is_subject_to_discovery,
 860       ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1), // mt processing
 861       ParallelGCThreads,   // mt processing degree
 862       true,                // mt discovery
 863       ParallelGCThreads,   // mt discovery degree
 864       true,                // atomic_discovery
 865       is_alive_non_header) {
 866   }
 867 
 868   template&lt;typename T&gt; bool discover(oop obj, ReferenceType type) {
 869     T* referent_addr = (T*) java_lang_ref_Reference::referent_addr_raw(obj);
 870     T heap_oop = RawAccess&lt;&gt;::oop_load(referent_addr);
 871     oop referent = CompressedOops::decode_not_null(heap_oop);
 872     return PSParallelCompact::mark_bitmap()-&gt;is_unmarked(referent)
 873         &amp;&amp; ReferenceProcessor::discover_reference(obj, type);
 874   }
 875   virtual bool discover_reference(oop obj, ReferenceType type) {
 876     if (UseCompressedOops) {
 877       return discover&lt;narrowOop&gt;(obj, type);
 878     } else {
 879       return discover&lt;oop&gt;(obj, type);
 880     }
 881   }
 882 };
 883 
 884 void PSParallelCompact::post_initialize() {
 885   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 886   _span_based_discoverer.set_span(heap-&gt;reserved_region());
 887   _ref_processor =
 888     new PCReferenceProcessor(&amp;_span_based_discoverer,
 889                              &amp;_is_alive_closure); // non-header is alive closure
 890 
 891   _counters = new CollectorCounters("Parallel full collection pauses", 1);
 892 
 893   // Initialize static fields in ParCompactionManager.
 894   ParCompactionManager::initialize(mark_bitmap());
 895 }
 896 
 897 bool PSParallelCompact::initialize() {
 898   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 899   MemRegion mr = heap-&gt;reserved_region();
 900 
 901   // Was the old gen get allocated successfully?
 902   if (!heap-&gt;old_gen()-&gt;is_allocated()) {
 903     return false;
 904   }
 905 
 906   initialize_space_info();
 907   initialize_dead_wood_limiter();
 908 
 909   if (!_mark_bitmap.initialize(mr)) {
 910     vm_shutdown_during_initialization(
 911       err_msg("Unable to allocate " SIZE_FORMAT "KB bitmaps for parallel "
 912       "garbage collection for the requested " SIZE_FORMAT "KB heap.",
 913       _mark_bitmap.reserved_byte_size()/K, mr.byte_size()/K));
 914     return false;
 915   }
 916 
 917   if (!_summary_data.initialize(mr)) {
 918     vm_shutdown_during_initialization(
 919       err_msg("Unable to allocate " SIZE_FORMAT "KB card tables for parallel "
 920       "garbage collection for the requested " SIZE_FORMAT "KB heap.",
 921       _summary_data.reserved_byte_size()/K, mr.byte_size()/K));
 922     return false;
 923   }
 924 
 925   return true;
 926 }
 927 
 928 void PSParallelCompact::initialize_space_info()
 929 {
 930   memset(&amp;_space_info, 0, sizeof(_space_info));
 931 
 932   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 933   PSYoungGen* young_gen = heap-&gt;young_gen();
 934 
 935   _space_info[old_space_id].set_space(heap-&gt;old_gen()-&gt;object_space());
 936   _space_info[eden_space_id].set_space(young_gen-&gt;eden_space());
 937   _space_info[from_space_id].set_space(young_gen-&gt;from_space());
 938   _space_info[to_space_id].set_space(young_gen-&gt;to_space());
 939 
 940   _space_info[old_space_id].set_start_array(heap-&gt;old_gen()-&gt;start_array());
 941 }
 942 
 943 void PSParallelCompact::initialize_dead_wood_limiter()
 944 {
 945   const size_t max = 100;
 946   _dwl_mean = double(MIN2(ParallelOldDeadWoodLimiterMean, max)) / 100.0;
 947   _dwl_std_dev = double(MIN2(ParallelOldDeadWoodLimiterStdDev, max)) / 100.0;
 948   _dwl_first_term = 1.0 / (sqrt(2.0 * M_PI) * _dwl_std_dev);
 949   DEBUG_ONLY(_dwl_initialized = true;)
 950   _dwl_adjustment = normal_distribution(1.0);
 951 }
 952 
 953 void
 954 PSParallelCompact::clear_data_covering_space(SpaceId id)
 955 {
 956   // At this point, top is the value before GC, new_top() is the value that will
 957   // be set at the end of GC.  The marking bitmap is cleared to top; nothing
 958   // should be marked above top.  The summary data is cleared to the larger of
 959   // top &amp; new_top.
 960   MutableSpace* const space = _space_info[id].space();
 961   HeapWord* const bot = space-&gt;bottom();
 962   HeapWord* const top = space-&gt;top();
 963   HeapWord* const max_top = MAX2(top, _space_info[id].new_top());
 964 
 965   const idx_t beg_bit = _mark_bitmap.addr_to_bit(bot);
 966   const idx_t end_bit = _mark_bitmap.align_range_end(_mark_bitmap.addr_to_bit(top));
 967   _mark_bitmap.clear_range(beg_bit, end_bit);
 968 
 969   const size_t beg_region = _summary_data.addr_to_region_idx(bot);
 970   const size_t end_region =
 971     _summary_data.addr_to_region_idx(_summary_data.region_align_up(max_top));
 972   _summary_data.clear_range(beg_region, end_region);
 973 
 974   // Clear the data used to 'split' regions.
 975   SplitInfo&amp; split_info = _space_info[id].split_info();
 976   if (split_info.is_valid()) {
 977     split_info.clear();
 978   }
 979   DEBUG_ONLY(split_info.verify_clear();)
 980 }
 981 
 982 void PSParallelCompact::pre_compact()
 983 {
 984   // Update the from &amp; to space pointers in space_info, since they are swapped
 985   // at each young gen gc.  Do the update unconditionally (even though a
 986   // promotion failure does not swap spaces) because an unknown number of young
 987   // collections will have swapped the spaces an unknown number of times.
 988   GCTraceTime(Debug, gc, phases) tm("Pre Compact", &amp;_gc_timer);
 989   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 990   _space_info[from_space_id].set_space(heap-&gt;young_gen()-&gt;from_space());
 991   _space_info[to_space_id].set_space(heap-&gt;young_gen()-&gt;to_space());
 992 
 993   DEBUG_ONLY(add_obj_count = add_obj_size = 0;)
 994   DEBUG_ONLY(mark_bitmap_count = mark_bitmap_size = 0;)
 995 
 996   // Increment the invocation count
 997   heap-&gt;increment_total_collections(true);
 998 
 999   // We need to track unique mark sweep invocations as well.
1000   _total_invocations++;
1001 
1002   heap-&gt;print_heap_before_gc();
1003   heap-&gt;trace_heap_before_gc(&amp;_gc_tracer);
1004 
1005   // Fill in TLABs
1006   heap-&gt;ensure_parsability(true);  // retire TLABs
1007 
1008   if (VerifyBeforeGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
1009     Universe::verify("Before GC");
1010   }
1011 
1012   // Verify object start arrays
1013   if (VerifyObjectStartArray &amp;&amp;
1014       VerifyBeforeGC) {
1015     heap-&gt;old_gen()-&gt;verify_object_start_array();
1016   }
1017 
1018   DEBUG_ONLY(mark_bitmap()-&gt;verify_clear();)
1019   DEBUG_ONLY(summary_data().verify_clear();)
1020 
1021   ParCompactionManager::reset_all_bitmap_query_caches();
1022 }
1023 
1024 void PSParallelCompact::post_compact()
1025 {
1026   GCTraceTime(Info, gc, phases) tm("Post Compact", &amp;_gc_timer);
1027   ParCompactionManager::remove_all_shadow_regions();
1028 
1029   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
1030     // Clear the marking bitmap, summary data and split info.
1031     clear_data_covering_space(SpaceId(id));
1032     // Update top().  Must be done after clearing the bitmap and summary data.
1033     _space_info[id].publish_new_top();
1034   }
1035 
1036   MutableSpace* const eden_space = _space_info[eden_space_id].space();
1037   MutableSpace* const from_space = _space_info[from_space_id].space();
1038   MutableSpace* const to_space   = _space_info[to_space_id].space();
1039 
1040   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1041   bool eden_empty = eden_space-&gt;is_empty();
1042 
1043   // Update heap occupancy information which is used as input to the soft ref
1044   // clearing policy at the next gc.
1045   Universe::update_heap_info_at_gc();
1046 
1047   bool young_gen_empty = eden_empty &amp;&amp; from_space-&gt;is_empty() &amp;&amp;
1048     to_space-&gt;is_empty();
1049 
1050   PSCardTable* ct = heap-&gt;card_table();
1051   MemRegion old_mr = heap-&gt;old_gen()-&gt;reserved();
1052   if (young_gen_empty) {
1053     ct-&gt;clear(MemRegion(old_mr.start(), old_mr.end()));
1054   } else {
1055     ct-&gt;invalidate(MemRegion(old_mr.start(), old_mr.end()));
1056   }
1057 
1058   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1059   ClassLoaderDataGraph::purge();
<a name="1" id="anc1"></a><span class="changed">1060   DEBUG_ONLY(MetaspaceUtils::verify(false);)</span>
1061 
1062   heap-&gt;prune_scavengable_nmethods();
1063 
1064 #if COMPILER2_OR_JVMCI
1065   DerivedPointerTable::update_pointers();
1066 #endif
1067 
1068   if (ZapUnusedHeapArea) {
1069     heap-&gt;gen_mangle_unused_area();
1070   }
1071 
1072   // Signal that we have completed a visit to all live objects.
1073   Universe::heap()-&gt;record_whole_heap_examined_timestamp();
1074 }
1075 
1076 HeapWord*
1077 PSParallelCompact::compute_dense_prefix_via_density(const SpaceId id,
1078                                                     bool maximum_compaction)
1079 {
1080   const size_t region_size = ParallelCompactData::RegionSize;
1081   const ParallelCompactData&amp; sd = summary_data();
1082 
1083   const MutableSpace* const space = _space_info[id].space();
1084   HeapWord* const top_aligned_up = sd.region_align_up(space-&gt;top());
1085   const RegionData* const beg_cp = sd.addr_to_region_ptr(space-&gt;bottom());
1086   const RegionData* const end_cp = sd.addr_to_region_ptr(top_aligned_up);
1087 
1088   // Skip full regions at the beginning of the space--they are necessarily part
1089   // of the dense prefix.
1090   size_t full_count = 0;
1091   const RegionData* cp;
1092   for (cp = beg_cp; cp &lt; end_cp &amp;&amp; cp-&gt;data_size() == region_size; ++cp) {
1093     ++full_count;
1094   }
1095 
1096   assert(total_invocations() &gt;= _maximum_compaction_gc_num, "sanity");
1097   const size_t gcs_since_max = total_invocations() - _maximum_compaction_gc_num;
1098   const bool interval_ended = gcs_since_max &gt; HeapMaximumCompactionInterval;
1099   if (maximum_compaction || cp == end_cp || interval_ended) {
1100     _maximum_compaction_gc_num = total_invocations();
1101     return sd.region_to_addr(cp);
1102   }
1103 
1104   HeapWord* const new_top = _space_info[id].new_top();
1105   const size_t space_live = pointer_delta(new_top, space-&gt;bottom());
1106   const size_t space_used = space-&gt;used_in_words();
1107   const size_t space_capacity = space-&gt;capacity_in_words();
1108 
1109   const double cur_density = double(space_live) / space_capacity;
1110   const double deadwood_density =
1111     (1.0 - cur_density) * (1.0 - cur_density) * cur_density * cur_density;
1112   const size_t deadwood_goal = size_t(space_capacity * deadwood_density);
1113 
1114   log_develop_debug(gc, compaction)(
1115       "cur_dens=%5.3f dw_dens=%5.3f dw_goal=" SIZE_FORMAT,
1116       cur_density, deadwood_density, deadwood_goal);
1117   log_develop_debug(gc, compaction)(
1118       "space_live=" SIZE_FORMAT " space_used=" SIZE_FORMAT " "
1119       "space_cap=" SIZE_FORMAT,
1120       space_live, space_used,
1121       space_capacity);
1122 
1123   // XXX - Use binary search?
1124   HeapWord* dense_prefix = sd.region_to_addr(cp);
1125   const RegionData* full_cp = cp;
1126   const RegionData* const top_cp = sd.addr_to_region_ptr(space-&gt;top() - 1);
1127   while (cp &lt; end_cp) {
1128     HeapWord* region_destination = cp-&gt;destination();
1129     const size_t cur_deadwood = pointer_delta(dense_prefix, region_destination);
1130 
1131     log_develop_trace(gc, compaction)(
1132         "c#=" SIZE_FORMAT_W(4) " dst=" PTR_FORMAT " "
1133         "dp=" PTR_FORMAT " cdw=" SIZE_FORMAT_W(8),
1134         sd.region(cp), p2i(region_destination),
1135         p2i(dense_prefix), cur_deadwood);
1136 
1137     if (cur_deadwood &gt;= deadwood_goal) {
1138       // Found the region that has the correct amount of deadwood to the left.
1139       // This typically occurs after crossing a fairly sparse set of regions, so
1140       // iterate backwards over those sparse regions, looking for the region
1141       // that has the lowest density of live objects 'to the right.'
1142       size_t space_to_left = sd.region(cp) * region_size;
1143       size_t live_to_left = space_to_left - cur_deadwood;
1144       size_t space_to_right = space_capacity - space_to_left;
1145       size_t live_to_right = space_live - live_to_left;
1146       double density_to_right = double(live_to_right) / space_to_right;
1147       while (cp &gt; full_cp) {
1148         --cp;
1149         const size_t prev_region_live_to_right = live_to_right -
1150           cp-&gt;data_size();
1151         const size_t prev_region_space_to_right = space_to_right + region_size;
1152         double prev_region_density_to_right =
1153           double(prev_region_live_to_right) / prev_region_space_to_right;
1154         if (density_to_right &lt;= prev_region_density_to_right) {
1155           return dense_prefix;
1156         }
1157 
1158         log_develop_trace(gc, compaction)(
1159             "backing up from c=" SIZE_FORMAT_W(4) " d2r=%10.8f "
1160             "pc_d2r=%10.8f",
1161             sd.region(cp), density_to_right,
1162             prev_region_density_to_right);
1163 
1164         dense_prefix -= region_size;
1165         live_to_right = prev_region_live_to_right;
1166         space_to_right = prev_region_space_to_right;
1167         density_to_right = prev_region_density_to_right;
1168       }
1169       return dense_prefix;
1170     }
1171 
1172     dense_prefix += region_size;
1173     ++cp;
1174   }
1175 
1176   return dense_prefix;
1177 }
1178 
1179 #ifndef PRODUCT
1180 void PSParallelCompact::print_dense_prefix_stats(const char* const algorithm,
1181                                                  const SpaceId id,
1182                                                  const bool maximum_compaction,
1183                                                  HeapWord* const addr)
1184 {
1185   const size_t region_idx = summary_data().addr_to_region_idx(addr);
1186   RegionData* const cp = summary_data().region(region_idx);
1187   const MutableSpace* const space = _space_info[id].space();
1188   HeapWord* const new_top = _space_info[id].new_top();
1189 
1190   const size_t space_live = pointer_delta(new_top, space-&gt;bottom());
1191   const size_t dead_to_left = pointer_delta(addr, cp-&gt;destination());
1192   const size_t space_cap = space-&gt;capacity_in_words();
1193   const double dead_to_left_pct = double(dead_to_left) / space_cap;
1194   const size_t live_to_right = new_top - cp-&gt;destination();
1195   const size_t dead_to_right = space-&gt;top() - addr - live_to_right;
1196 
1197   log_develop_debug(gc, compaction)(
1198       "%s=" PTR_FORMAT " dpc=" SIZE_FORMAT_W(5) " "
1199       "spl=" SIZE_FORMAT " "
1200       "d2l=" SIZE_FORMAT " d2l%%=%6.4f "
1201       "d2r=" SIZE_FORMAT " l2r=" SIZE_FORMAT " "
1202       "ratio=%10.8f",
1203       algorithm, p2i(addr), region_idx,
1204       space_live,
1205       dead_to_left, dead_to_left_pct,
1206       dead_to_right, live_to_right,
1207       double(dead_to_right) / live_to_right);
1208 }
1209 #endif  // #ifndef PRODUCT
1210 
1211 // Return a fraction indicating how much of the generation can be treated as
1212 // "dead wood" (i.e., not reclaimed).  The function uses a normal distribution
1213 // based on the density of live objects in the generation to determine a limit,
1214 // which is then adjusted so the return value is min_percent when the density is
1215 // 1.
1216 //
1217 // The following table shows some return values for a different values of the
1218 // standard deviation (ParallelOldDeadWoodLimiterStdDev); the mean is 0.5 and
1219 // min_percent is 1.
1220 //
1221 //                          fraction allowed as dead wood
1222 //         -----------------------------------------------------------------
1223 // density std_dev=70 std_dev=75 std_dev=80 std_dev=85 std_dev=90 std_dev=95
1224 // ------- ---------- ---------- ---------- ---------- ---------- ----------
1225 // 0.00000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000
1226 // 0.05000 0.03193096 0.02836880 0.02550828 0.02319280 0.02130337 0.01974941
1227 // 0.10000 0.05247504 0.04547452 0.03988045 0.03537016 0.03170171 0.02869272
1228 // 0.15000 0.07135702 0.06111390 0.05296419 0.04641639 0.04110601 0.03676066
1229 // 0.20000 0.08831616 0.07509618 0.06461766 0.05622444 0.04943437 0.04388975
1230 // 0.25000 0.10311208 0.08724696 0.07471205 0.06469760 0.05661313 0.05002313
1231 // 0.30000 0.11553050 0.09741183 0.08313394 0.07175114 0.06257797 0.05511132
1232 // 0.35000 0.12538832 0.10545958 0.08978741 0.07731366 0.06727491 0.05911289
1233 // 0.40000 0.13253818 0.11128511 0.09459590 0.08132834 0.07066107 0.06199500
1234 // 0.45000 0.13687208 0.11481163 0.09750361 0.08375387 0.07270534 0.06373386
1235 // 0.50000 0.13832410 0.11599237 0.09847664 0.08456518 0.07338887 0.06431510
1236 // 0.55000 0.13687208 0.11481163 0.09750361 0.08375387 0.07270534 0.06373386
1237 // 0.60000 0.13253818 0.11128511 0.09459590 0.08132834 0.07066107 0.06199500
1238 // 0.65000 0.12538832 0.10545958 0.08978741 0.07731366 0.06727491 0.05911289
1239 // 0.70000 0.11553050 0.09741183 0.08313394 0.07175114 0.06257797 0.05511132
1240 // 0.75000 0.10311208 0.08724696 0.07471205 0.06469760 0.05661313 0.05002313
1241 // 0.80000 0.08831616 0.07509618 0.06461766 0.05622444 0.04943437 0.04388975
1242 // 0.85000 0.07135702 0.06111390 0.05296419 0.04641639 0.04110601 0.03676066
1243 // 0.90000 0.05247504 0.04547452 0.03988045 0.03537016 0.03170171 0.02869272
1244 // 0.95000 0.03193096 0.02836880 0.02550828 0.02319280 0.02130337 0.01974941
1245 // 1.00000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000
1246 
1247 double PSParallelCompact::dead_wood_limiter(double density, size_t min_percent)
1248 {
1249   assert(_dwl_initialized, "uninitialized");
1250 
1251   // The raw limit is the value of the normal distribution at x = density.
1252   const double raw_limit = normal_distribution(density);
1253 
1254   // Adjust the raw limit so it becomes the minimum when the density is 1.
1255   //
1256   // First subtract the adjustment value (which is simply the precomputed value
1257   // normal_distribution(1.0)); this yields a value of 0 when the density is 1.
1258   // Then add the minimum value, so the minimum is returned when the density is
1259   // 1.  Finally, prevent negative values, which occur when the mean is not 0.5.
1260   const double min = double(min_percent) / 100.0;
1261   const double limit = raw_limit - _dwl_adjustment + min;
1262   return MAX2(limit, 0.0);
1263 }
1264 
1265 ParallelCompactData::RegionData*
1266 PSParallelCompact::first_dead_space_region(const RegionData* beg,
1267                                            const RegionData* end)
1268 {
1269   const size_t region_size = ParallelCompactData::RegionSize;
1270   ParallelCompactData&amp; sd = summary_data();
1271   size_t left = sd.region(beg);
1272   size_t right = end &gt; beg ? sd.region(end) - 1 : left;
1273 
1274   // Binary search.
1275   while (left &lt; right) {
1276     // Equivalent to (left + right) / 2, but does not overflow.
1277     const size_t middle = left + (right - left) / 2;
1278     RegionData* const middle_ptr = sd.region(middle);
1279     HeapWord* const dest = middle_ptr-&gt;destination();
1280     HeapWord* const addr = sd.region_to_addr(middle);
1281     assert(dest != NULL, "sanity");
1282     assert(dest &lt;= addr, "must move left");
1283 
1284     if (middle &gt; left &amp;&amp; dest &lt; addr) {
1285       right = middle - 1;
1286     } else if (middle &lt; right &amp;&amp; middle_ptr-&gt;data_size() == region_size) {
1287       left = middle + 1;
1288     } else {
1289       return middle_ptr;
1290     }
1291   }
1292   return sd.region(left);
1293 }
1294 
1295 ParallelCompactData::RegionData*
1296 PSParallelCompact::dead_wood_limit_region(const RegionData* beg,
1297                                           const RegionData* end,
1298                                           size_t dead_words)
1299 {
1300   ParallelCompactData&amp; sd = summary_data();
1301   size_t left = sd.region(beg);
1302   size_t right = end &gt; beg ? sd.region(end) - 1 : left;
1303 
1304   // Binary search.
1305   while (left &lt; right) {
1306     // Equivalent to (left + right) / 2, but does not overflow.
1307     const size_t middle = left + (right - left) / 2;
1308     RegionData* const middle_ptr = sd.region(middle);
1309     HeapWord* const dest = middle_ptr-&gt;destination();
1310     HeapWord* const addr = sd.region_to_addr(middle);
1311     assert(dest != NULL, "sanity");
1312     assert(dest &lt;= addr, "must move left");
1313 
1314     const size_t dead_to_left = pointer_delta(addr, dest);
1315     if (middle &gt; left &amp;&amp; dead_to_left &gt; dead_words) {
1316       right = middle - 1;
1317     } else if (middle &lt; right &amp;&amp; dead_to_left &lt; dead_words) {
1318       left = middle + 1;
1319     } else {
1320       return middle_ptr;
1321     }
1322   }
1323   return sd.region(left);
1324 }
1325 
1326 // The result is valid during the summary phase, after the initial summarization
1327 // of each space into itself, and before final summarization.
1328 inline double
1329 PSParallelCompact::reclaimed_ratio(const RegionData* const cp,
1330                                    HeapWord* const bottom,
1331                                    HeapWord* const top,
1332                                    HeapWord* const new_top)
1333 {
1334   ParallelCompactData&amp; sd = summary_data();
1335 
1336   assert(cp != NULL, "sanity");
1337   assert(bottom != NULL, "sanity");
1338   assert(top != NULL, "sanity");
1339   assert(new_top != NULL, "sanity");
1340   assert(top &gt;= new_top, "summary data problem?");
1341   assert(new_top &gt; bottom, "space is empty; should not be here");
1342   assert(new_top &gt;= cp-&gt;destination(), "sanity");
1343   assert(top &gt;= sd.region_to_addr(cp), "sanity");
1344 
1345   HeapWord* const destination = cp-&gt;destination();
1346   const size_t dense_prefix_live  = pointer_delta(destination, bottom);
1347   const size_t compacted_region_live = pointer_delta(new_top, destination);
1348   const size_t compacted_region_used = pointer_delta(top,
1349                                                      sd.region_to_addr(cp));
1350   const size_t reclaimable = compacted_region_used - compacted_region_live;
1351 
1352   const double divisor = dense_prefix_live + 1.25 * compacted_region_live;
1353   return double(reclaimable) / divisor;
1354 }
1355 
1356 // Return the address of the end of the dense prefix, a.k.a. the start of the
1357 // compacted region.  The address is always on a region boundary.
1358 //
1359 // Completely full regions at the left are skipped, since no compaction can
1360 // occur in those regions.  Then the maximum amount of dead wood to allow is
1361 // computed, based on the density (amount live / capacity) of the generation;
1362 // the region with approximately that amount of dead space to the left is
1363 // identified as the limit region.  Regions between the last completely full
1364 // region and the limit region are scanned and the one that has the best
1365 // (maximum) reclaimed_ratio() is selected.
1366 HeapWord*
1367 PSParallelCompact::compute_dense_prefix(const SpaceId id,
1368                                         bool maximum_compaction)
1369 {
1370   const size_t region_size = ParallelCompactData::RegionSize;
1371   const ParallelCompactData&amp; sd = summary_data();
1372 
1373   const MutableSpace* const space = _space_info[id].space();
1374   HeapWord* const top = space-&gt;top();
1375   HeapWord* const top_aligned_up = sd.region_align_up(top);
1376   HeapWord* const new_top = _space_info[id].new_top();
1377   HeapWord* const new_top_aligned_up = sd.region_align_up(new_top);
1378   HeapWord* const bottom = space-&gt;bottom();
1379   const RegionData* const beg_cp = sd.addr_to_region_ptr(bottom);
1380   const RegionData* const top_cp = sd.addr_to_region_ptr(top_aligned_up);
1381   const RegionData* const new_top_cp =
1382     sd.addr_to_region_ptr(new_top_aligned_up);
1383 
1384   // Skip full regions at the beginning of the space--they are necessarily part
1385   // of the dense prefix.
1386   const RegionData* const full_cp = first_dead_space_region(beg_cp, new_top_cp);
1387   assert(full_cp-&gt;destination() == sd.region_to_addr(full_cp) ||
1388          space-&gt;is_empty(), "no dead space allowed to the left");
1389   assert(full_cp-&gt;data_size() &lt; region_size || full_cp == new_top_cp - 1,
1390          "region must have dead space");
1391 
1392   // The gc number is saved whenever a maximum compaction is done, and used to
1393   // determine when the maximum compaction interval has expired.  This avoids
1394   // successive max compactions for different reasons.
1395   assert(total_invocations() &gt;= _maximum_compaction_gc_num, "sanity");
1396   const size_t gcs_since_max = total_invocations() - _maximum_compaction_gc_num;
1397   const bool interval_ended = gcs_since_max &gt; HeapMaximumCompactionInterval ||
1398     total_invocations() == HeapFirstMaximumCompactionCount;
1399   if (maximum_compaction || full_cp == top_cp || interval_ended) {
1400     _maximum_compaction_gc_num = total_invocations();
1401     return sd.region_to_addr(full_cp);
1402   }
1403 
1404   const size_t space_live = pointer_delta(new_top, bottom);
1405   const size_t space_used = space-&gt;used_in_words();
1406   const size_t space_capacity = space-&gt;capacity_in_words();
1407 
1408   const double density = double(space_live) / double(space_capacity);
1409   const size_t min_percent_free = MarkSweepDeadRatio;
1410   const double limiter = dead_wood_limiter(density, min_percent_free);
1411   const size_t dead_wood_max = space_used - space_live;
1412   const size_t dead_wood_limit = MIN2(size_t(space_capacity * limiter),
1413                                       dead_wood_max);
1414 
1415   log_develop_debug(gc, compaction)(
1416       "space_live=" SIZE_FORMAT " space_used=" SIZE_FORMAT " "
1417       "space_cap=" SIZE_FORMAT,
1418       space_live, space_used,
1419       space_capacity);
1420   log_develop_debug(gc, compaction)(
1421       "dead_wood_limiter(%6.4f, " SIZE_FORMAT ")=%6.4f "
1422       "dead_wood_max=" SIZE_FORMAT " dead_wood_limit=" SIZE_FORMAT,
1423       density, min_percent_free, limiter,
1424       dead_wood_max, dead_wood_limit);
1425 
1426   // Locate the region with the desired amount of dead space to the left.
1427   const RegionData* const limit_cp =
1428     dead_wood_limit_region(full_cp, top_cp, dead_wood_limit);
1429 
1430   // Scan from the first region with dead space to the limit region and find the
1431   // one with the best (largest) reclaimed ratio.
1432   double best_ratio = 0.0;
1433   const RegionData* best_cp = full_cp;
1434   for (const RegionData* cp = full_cp; cp &lt; limit_cp; ++cp) {
1435     double tmp_ratio = reclaimed_ratio(cp, bottom, top, new_top);
1436     if (tmp_ratio &gt; best_ratio) {
1437       best_cp = cp;
1438       best_ratio = tmp_ratio;
1439     }
1440   }
1441 
1442   return sd.region_to_addr(best_cp);
1443 }
1444 
1445 void PSParallelCompact::summarize_spaces_quick()
1446 {
1447   for (unsigned int i = 0; i &lt; last_space_id; ++i) {
1448     const MutableSpace* space = _space_info[i].space();
1449     HeapWord** nta = _space_info[i].new_top_addr();
1450     bool result = _summary_data.summarize(_space_info[i].split_info(),
1451                                           space-&gt;bottom(), space-&gt;top(), NULL,
1452                                           space-&gt;bottom(), space-&gt;end(), nta);
1453     assert(result, "space must fit into itself");
1454     _space_info[i].set_dense_prefix(space-&gt;bottom());
1455   }
1456 }
1457 
1458 void PSParallelCompact::fill_dense_prefix_end(SpaceId id)
1459 {
1460   HeapWord* const dense_prefix_end = dense_prefix(id);
1461   const RegionData* region = _summary_data.addr_to_region_ptr(dense_prefix_end);
1462   const idx_t dense_prefix_bit = _mark_bitmap.addr_to_bit(dense_prefix_end);
1463   if (dead_space_crosses_boundary(region, dense_prefix_bit)) {
1464     // Only enough dead space is filled so that any remaining dead space to the
1465     // left is larger than the minimum filler object.  (The remainder is filled
1466     // during the copy/update phase.)
1467     //
1468     // The size of the dead space to the right of the boundary is not a
1469     // concern, since compaction will be able to use whatever space is
1470     // available.
1471     //
1472     // Here '||' is the boundary, 'x' represents a don't care bit and a box
1473     // surrounds the space to be filled with an object.
1474     //
1475     // In the 32-bit VM, each bit represents two 32-bit words:
1476     //                              +---+
1477     // a) beg_bits:  ...  x   x   x | 0 | ||   0   x  x  ...
1478     //    end_bits:  ...  x   x   x | 0 | ||   0   x  x  ...
1479     //                              +---+
1480     //
1481     // In the 64-bit VM, each bit represents one 64-bit word:
1482     //                              +------------+
1483     // b) beg_bits:  ...  x   x   x | 0   ||   0 | x  x  ...
1484     //    end_bits:  ...  x   x   1 | 0   ||   0 | x  x  ...
1485     //                              +------------+
1486     //                          +-------+
1487     // c) beg_bits:  ...  x   x | 0   0 | ||   0   x  x  ...
1488     //    end_bits:  ...  x   1 | 0   0 | ||   0   x  x  ...
1489     //                          +-------+
1490     //                      +-----------+
1491     // d) beg_bits:  ...  x | 0   0   0 | ||   0   x  x  ...
1492     //    end_bits:  ...  1 | 0   0   0 | ||   0   x  x  ...
1493     //                      +-----------+
1494     //                          +-------+
1495     // e) beg_bits:  ...  0   0 | 0   0 | ||   0   x  x  ...
1496     //    end_bits:  ...  0   0 | 0   0 | ||   0   x  x  ...
1497     //                          +-------+
1498 
1499     // Initially assume case a, c or e will apply.
1500     size_t obj_len = CollectedHeap::min_fill_size();
1501     HeapWord* obj_beg = dense_prefix_end - obj_len;
1502 
1503 #ifdef  _LP64
1504     if (MinObjAlignment &gt; 1) { // object alignment &gt; heap word size
1505       // Cases a, c or e.
1506     } else if (_mark_bitmap.is_obj_end(dense_prefix_bit - 2)) {
1507       // Case b above.
1508       obj_beg = dense_prefix_end - 1;
1509     } else if (!_mark_bitmap.is_obj_end(dense_prefix_bit - 3) &amp;&amp;
1510                _mark_bitmap.is_obj_end(dense_prefix_bit - 4)) {
1511       // Case d above.
1512       obj_beg = dense_prefix_end - 3;
1513       obj_len = 3;
1514     }
1515 #endif  // #ifdef _LP64
1516 
1517     CollectedHeap::fill_with_object(obj_beg, obj_len);
1518     _mark_bitmap.mark_obj(obj_beg, obj_len);
1519     _summary_data.add_obj(obj_beg, obj_len);
1520     assert(start_array(id) != NULL, "sanity");
1521     start_array(id)-&gt;allocate_block(obj_beg);
1522   }
1523 }
1524 
1525 void
1526 PSParallelCompact::summarize_space(SpaceId id, bool maximum_compaction)
1527 {
1528   assert(id &lt; last_space_id, "id out of range");
1529   assert(_space_info[id].dense_prefix() == _space_info[id].space()-&gt;bottom(),
1530          "should have been reset in summarize_spaces_quick()");
1531 
1532   const MutableSpace* space = _space_info[id].space();
1533   if (_space_info[id].new_top() != space-&gt;bottom()) {
1534     HeapWord* dense_prefix_end = compute_dense_prefix(id, maximum_compaction);
1535     _space_info[id].set_dense_prefix(dense_prefix_end);
1536 
1537 #ifndef PRODUCT
1538     if (log_is_enabled(Debug, gc, compaction)) {
1539       print_dense_prefix_stats("ratio", id, maximum_compaction,
1540                                dense_prefix_end);
1541       HeapWord* addr = compute_dense_prefix_via_density(id, maximum_compaction);
1542       print_dense_prefix_stats("density", id, maximum_compaction, addr);
1543     }
1544 #endif  // #ifndef PRODUCT
1545 
1546     // Recompute the summary data, taking into account the dense prefix.  If
1547     // every last byte will be reclaimed, then the existing summary data which
1548     // compacts everything can be left in place.
1549     if (!maximum_compaction &amp;&amp; dense_prefix_end != space-&gt;bottom()) {
1550       // If dead space crosses the dense prefix boundary, it is (at least
1551       // partially) filled with a dummy object, marked live and added to the
1552       // summary data.  This simplifies the copy/update phase and must be done
1553       // before the final locations of objects are determined, to prevent
1554       // leaving a fragment of dead space that is too small to fill.
1555       fill_dense_prefix_end(id);
1556 
1557       // Compute the destination of each Region, and thus each object.
1558       _summary_data.summarize_dense_prefix(space-&gt;bottom(), dense_prefix_end);
1559       _summary_data.summarize(_space_info[id].split_info(),
1560                               dense_prefix_end, space-&gt;top(), NULL,
1561                               dense_prefix_end, space-&gt;end(),
1562                               _space_info[id].new_top_addr());
1563     }
1564   }
1565 
1566   if (log_develop_is_enabled(Trace, gc, compaction)) {
1567     const size_t region_size = ParallelCompactData::RegionSize;
1568     HeapWord* const dense_prefix_end = _space_info[id].dense_prefix();
1569     const size_t dp_region = _summary_data.addr_to_region_idx(dense_prefix_end);
1570     const size_t dp_words = pointer_delta(dense_prefix_end, space-&gt;bottom());
1571     HeapWord* const new_top = _space_info[id].new_top();
1572     const HeapWord* nt_aligned_up = _summary_data.region_align_up(new_top);
1573     const size_t cr_words = pointer_delta(nt_aligned_up, dense_prefix_end);
1574     log_develop_trace(gc, compaction)(
1575         "id=%d cap=" SIZE_FORMAT " dp=" PTR_FORMAT " "
1576         "dp_region=" SIZE_FORMAT " " "dp_count=" SIZE_FORMAT " "
1577         "cr_count=" SIZE_FORMAT " " "nt=" PTR_FORMAT,
1578         id, space-&gt;capacity_in_words(), p2i(dense_prefix_end),
1579         dp_region, dp_words / region_size,
1580         cr_words / region_size, p2i(new_top));
1581   }
1582 }
1583 
1584 #ifndef PRODUCT
1585 void PSParallelCompact::summary_phase_msg(SpaceId dst_space_id,
1586                                           HeapWord* dst_beg, HeapWord* dst_end,
1587                                           SpaceId src_space_id,
1588                                           HeapWord* src_beg, HeapWord* src_end)
1589 {
1590   log_develop_trace(gc, compaction)(
1591       "Summarizing %d [%s] into %d [%s]:  "
1592       "src=" PTR_FORMAT "-" PTR_FORMAT " "
1593       SIZE_FORMAT "-" SIZE_FORMAT " "
1594       "dst=" PTR_FORMAT "-" PTR_FORMAT " "
1595       SIZE_FORMAT "-" SIZE_FORMAT,
1596       src_space_id, space_names[src_space_id],
1597       dst_space_id, space_names[dst_space_id],
1598       p2i(src_beg), p2i(src_end),
1599       _summary_data.addr_to_region_idx(src_beg),
1600       _summary_data.addr_to_region_idx(src_end),
1601       p2i(dst_beg), p2i(dst_end),
1602       _summary_data.addr_to_region_idx(dst_beg),
1603       _summary_data.addr_to_region_idx(dst_end));
1604 }
1605 #endif  // #ifndef PRODUCT
1606 
1607 void PSParallelCompact::summary_phase(ParCompactionManager* cm,
1608                                       bool maximum_compaction)
1609 {
1610   GCTraceTime(Info, gc, phases) tm("Summary Phase", &amp;_gc_timer);
1611 
1612   log_develop_debug(gc, marking)(
1613       "add_obj_count=" SIZE_FORMAT " "
1614       "add_obj_bytes=" SIZE_FORMAT,
1615       add_obj_count,
1616       add_obj_size * HeapWordSize);
1617   log_develop_debug(gc, marking)(
1618       "mark_bitmap_count=" SIZE_FORMAT " "
1619       "mark_bitmap_bytes=" SIZE_FORMAT,
1620       mark_bitmap_count,
1621       mark_bitmap_size * HeapWordSize);
1622 
1623   // Quick summarization of each space into itself, to see how much is live.
1624   summarize_spaces_quick();
1625 
1626   log_develop_trace(gc, compaction)("summary phase:  after summarizing each space to self");
1627   NOT_PRODUCT(print_region_ranges());
1628   NOT_PRODUCT(print_initial_summary_data(_summary_data, _space_info));
1629 
1630   // The amount of live data that will end up in old space (assuming it fits).
1631   size_t old_space_total_live = 0;
1632   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
1633     old_space_total_live += pointer_delta(_space_info[id].new_top(),
1634                                           _space_info[id].space()-&gt;bottom());
1635   }
1636 
1637   MutableSpace* const old_space = _space_info[old_space_id].space();
1638   const size_t old_capacity = old_space-&gt;capacity_in_words();
1639   if (old_space_total_live &gt; old_capacity) {
1640     // XXX - should also try to expand
1641     maximum_compaction = true;
1642   }
1643 
1644   // Old generations.
1645   summarize_space(old_space_id, maximum_compaction);
1646 
1647   // Summarize the remaining spaces in the young gen.  The initial target space
1648   // is the old gen.  If a space does not fit entirely into the target, then the
1649   // remainder is compacted into the space itself and that space becomes the new
1650   // target.
1651   SpaceId dst_space_id = old_space_id;
1652   HeapWord* dst_space_end = old_space-&gt;end();
1653   HeapWord** new_top_addr = _space_info[dst_space_id].new_top_addr();
1654   for (unsigned int id = eden_space_id; id &lt; last_space_id; ++id) {
1655     const MutableSpace* space = _space_info[id].space();
1656     const size_t live = pointer_delta(_space_info[id].new_top(),
1657                                       space-&gt;bottom());
1658     const size_t available = pointer_delta(dst_space_end, *new_top_addr);
1659 
1660     NOT_PRODUCT(summary_phase_msg(dst_space_id, *new_top_addr, dst_space_end,
1661                                   SpaceId(id), space-&gt;bottom(), space-&gt;top());)
1662     if (live &gt; 0 &amp;&amp; live &lt;= available) {
1663       // All the live data will fit.
1664       bool done = _summary_data.summarize(_space_info[id].split_info(),
1665                                           space-&gt;bottom(), space-&gt;top(),
1666                                           NULL,
1667                                           *new_top_addr, dst_space_end,
1668                                           new_top_addr);
1669       assert(done, "space must fit into old gen");
1670 
1671       // Reset the new_top value for the space.
1672       _space_info[id].set_new_top(space-&gt;bottom());
1673     } else if (live &gt; 0) {
1674       // Attempt to fit part of the source space into the target space.
1675       HeapWord* next_src_addr = NULL;
1676       bool done = _summary_data.summarize(_space_info[id].split_info(),
1677                                           space-&gt;bottom(), space-&gt;top(),
1678                                           &amp;next_src_addr,
1679                                           *new_top_addr, dst_space_end,
1680                                           new_top_addr);
1681       assert(!done, "space should not fit into old gen");
1682       assert(next_src_addr != NULL, "sanity");
1683 
1684       // The source space becomes the new target, so the remainder is compacted
1685       // within the space itself.
1686       dst_space_id = SpaceId(id);
1687       dst_space_end = space-&gt;end();
1688       new_top_addr = _space_info[id].new_top_addr();
1689       NOT_PRODUCT(summary_phase_msg(dst_space_id,
1690                                     space-&gt;bottom(), dst_space_end,
1691                                     SpaceId(id), next_src_addr, space-&gt;top());)
1692       done = _summary_data.summarize(_space_info[id].split_info(),
1693                                      next_src_addr, space-&gt;top(),
1694                                      NULL,
1695                                      space-&gt;bottom(), dst_space_end,
1696                                      new_top_addr);
1697       assert(done, "space must fit when compacted into itself");
1698       assert(*new_top_addr &lt;= space-&gt;top(), "usage should not grow");
1699     }
1700   }
1701 
1702   log_develop_trace(gc, compaction)("Summary_phase:  after final summarization");
1703   NOT_PRODUCT(print_region_ranges());
1704   NOT_PRODUCT(print_initial_summary_data(_summary_data, _space_info));
1705 }
1706 
1707 // This method should contain all heap-specific policy for invoking a full
1708 // collection.  invoke_no_policy() will only attempt to compact the heap; it
1709 // will do nothing further.  If we need to bail out for policy reasons, scavenge
1710 // before full gc, or any other specialized behavior, it needs to be added here.
1711 //
1712 // Note that this method should only be called from the vm_thread while at a
1713 // safepoint.
1714 //
1715 // Note that the all_soft_refs_clear flag in the soft ref policy
1716 // may be true because this method can be called without intervening
1717 // activity.  For example when the heap space is tight and full measure
1718 // are being taken to free space.
1719 void PSParallelCompact::invoke(bool maximum_heap_compaction) {
1720   assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
1721   assert(Thread::current() == (Thread*)VMThread::vm_thread(),
1722          "should be in vm thread");
1723 
1724   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1725   GCCause::Cause gc_cause = heap-&gt;gc_cause();
1726   assert(!heap-&gt;is_gc_active(), "not reentrant");
1727 
1728   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
1729   IsGCActiveMark mark;
1730 
1731   if (ScavengeBeforeFullGC) {
1732     PSScavenge::invoke_no_policy();
1733   }
1734 
1735   const bool clear_all_soft_refs =
1736     heap-&gt;soft_ref_policy()-&gt;should_clear_all_soft_refs();
1737 
1738   PSParallelCompact::invoke_no_policy(clear_all_soft_refs ||
1739                                       maximum_heap_compaction);
1740 }
1741 
1742 // This method contains no policy. You should probably
1743 // be calling invoke() instead.
1744 bool PSParallelCompact::invoke_no_policy(bool maximum_heap_compaction) {
1745   assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");
1746   assert(ref_processor() != NULL, "Sanity");
1747 
1748   if (GCLocker::check_active_before_gc()) {
1749     return false;
1750   }
1751 
1752   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1753 
1754   GCIdMark gc_id_mark;
1755   _gc_timer.register_gc_start();
1756   _gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer.gc_start());
1757 
1758   TimeStamp marking_start;
1759   TimeStamp compaction_start;
1760   TimeStamp collection_exit;
1761 
1762   GCCause::Cause gc_cause = heap-&gt;gc_cause();
1763   PSYoungGen* young_gen = heap-&gt;young_gen();
1764   PSOldGen* old_gen = heap-&gt;old_gen();
1765   PSAdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
1766 
1767   // The scope of casr should end after code that can change
1768   // SoftRefPolicy::_should_clear_all_soft_refs.
1769   ClearedAllSoftRefs casr(maximum_heap_compaction,
1770                           heap-&gt;soft_ref_policy());
1771 
1772   if (ZapUnusedHeapArea) {
1773     // Save information needed to minimize mangling
1774     heap-&gt;record_gen_tops_before_GC();
1775   }
1776 
1777   // Make sure data structures are sane, make the heap parsable, and do other
1778   // miscellaneous bookkeeping.
1779   pre_compact();
1780 
1781   const PreGenGCValues pre_gc_values = heap-&gt;get_pre_gc_values();
1782 
1783   // Get the compaction manager reserved for the VM thread.
1784   ParCompactionManager* const vmthread_cm =
1785     ParCompactionManager::manager_array(ParallelScavengeHeap::heap()-&gt;workers().total_workers());
1786 
1787   {
1788     ResourceMark rm;
1789 
1790     const uint active_workers =
1791       WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()-&gt;workers().total_workers(),
1792                                         ParallelScavengeHeap::heap()-&gt;workers().active_workers(),
1793                                         Threads::number_of_non_daemon_threads());
1794     ParallelScavengeHeap::heap()-&gt;workers().update_active_workers(active_workers);
1795 
1796     GCTraceCPUTime tcpu;
1797     GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause, true);
1798 
1799     heap-&gt;pre_full_gc_dump(&amp;_gc_timer);
1800 
1801     TraceCollectorStats tcs(counters());
1802     TraceMemoryManagerStats tms(heap-&gt;old_gc_manager(), gc_cause);
1803 
1804     if (log_is_enabled(Debug, gc, heap, exit)) {
1805       accumulated_time()-&gt;start();
1806     }
1807 
1808     // Let the size policy know we're starting
1809     size_policy-&gt;major_collection_begin();
1810 
1811 #if COMPILER2_OR_JVMCI
1812     DerivedPointerTable::clear();
1813 #endif
1814 
1815     ref_processor()-&gt;enable_discovery();
1816     ref_processor()-&gt;setup_policy(maximum_heap_compaction);
1817 
1818     bool marked_for_unloading = false;
1819 
1820     marking_start.update();
1821     marking_phase(vmthread_cm, maximum_heap_compaction, &amp;_gc_tracer);
1822 
1823     bool max_on_system_gc = UseMaximumCompactionOnSystemGC
1824       &amp;&amp; GCCause::is_user_requested_gc(gc_cause);
1825     summary_phase(vmthread_cm, maximum_heap_compaction || max_on_system_gc);
1826 
1827 #if COMPILER2_OR_JVMCI
1828     assert(DerivedPointerTable::is_active(), "Sanity");
1829     DerivedPointerTable::set_active(false);
1830 #endif
1831 
1832     // adjust_roots() updates Universe::_intArrayKlassObj which is
1833     // needed by the compaction for filling holes in the dense prefix.
1834     adjust_roots(vmthread_cm);
1835 
1836     compaction_start.update();
1837     compact();
1838 
1839     // Reset the mark bitmap, summary data, and do other bookkeeping.  Must be
1840     // done before resizing.
1841     post_compact();
1842 
1843     // Let the size policy know we're done
1844     size_policy-&gt;major_collection_end(old_gen-&gt;used_in_bytes(), gc_cause);
1845 
1846     if (UseAdaptiveSizePolicy) {
1847       log_debug(gc, ergo)("AdaptiveSizeStart: collection: %d ", heap-&gt;total_collections());
1848       log_trace(gc, ergo)("old_gen_capacity: " SIZE_FORMAT " young_gen_capacity: " SIZE_FORMAT,
1849                           old_gen-&gt;capacity_in_bytes(), young_gen-&gt;capacity_in_bytes());
1850 
1851       // Don't check if the size_policy is ready here.  Let
1852       // the size_policy check that internally.
1853       if (UseAdaptiveGenerationSizePolicyAtMajorCollection &amp;&amp;
1854           AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {
1855         // Swap the survivor spaces if from_space is empty. The
1856         // resize_young_gen() called below is normally used after
1857         // a successful young GC and swapping of survivor spaces;
1858         // otherwise, it will fail to resize the young gen with
1859         // the current implementation.
1860         if (young_gen-&gt;from_space()-&gt;is_empty()) {
1861           young_gen-&gt;from_space()-&gt;clear(SpaceDecorator::Mangle);
1862           young_gen-&gt;swap_spaces();
1863         }
1864 
1865         // Calculate optimal free space amounts
1866         assert(young_gen-&gt;max_gen_size() &gt;
1867           young_gen-&gt;from_space()-&gt;capacity_in_bytes() +
1868           young_gen-&gt;to_space()-&gt;capacity_in_bytes(),
1869           "Sizes of space in young gen are out-of-bounds");
1870 
1871         size_t young_live = young_gen-&gt;used_in_bytes();
1872         size_t eden_live = young_gen-&gt;eden_space()-&gt;used_in_bytes();
1873         size_t old_live = old_gen-&gt;used_in_bytes();
1874         size_t cur_eden = young_gen-&gt;eden_space()-&gt;capacity_in_bytes();
1875         size_t max_old_gen_size = old_gen-&gt;max_gen_size();
1876         size_t max_eden_size = young_gen-&gt;max_gen_size() -
1877           young_gen-&gt;from_space()-&gt;capacity_in_bytes() -
1878           young_gen-&gt;to_space()-&gt;capacity_in_bytes();
1879 
1880         // Used for diagnostics
1881         size_policy-&gt;clear_generation_free_space_flags();
1882 
1883         size_policy-&gt;compute_generations_free_space(young_live,
1884                                                     eden_live,
1885                                                     old_live,
1886                                                     cur_eden,
1887                                                     max_old_gen_size,
1888                                                     max_eden_size,
1889                                                     true /* full gc*/);
1890 
1891         size_policy-&gt;check_gc_overhead_limit(eden_live,
1892                                              max_old_gen_size,
1893                                              max_eden_size,
1894                                              true /* full gc*/,
1895                                              gc_cause,
1896                                              heap-&gt;soft_ref_policy());
1897 
1898         size_policy-&gt;decay_supplemental_growth(true /* full gc*/);
1899 
1900         heap-&gt;resize_old_gen(
1901           size_policy-&gt;calculated_old_free_size_in_bytes());
1902 
1903         heap-&gt;resize_young_gen(size_policy-&gt;calculated_eden_size_in_bytes(),
1904                                size_policy-&gt;calculated_survivor_size_in_bytes());
1905       }
1906 
1907       log_debug(gc, ergo)("AdaptiveSizeStop: collection: %d ", heap-&gt;total_collections());
1908     }
1909 
1910     if (UsePerfData) {
1911       PSGCAdaptivePolicyCounters* const counters = heap-&gt;gc_policy_counters();
1912       counters-&gt;update_counters();
1913       counters-&gt;update_old_capacity(old_gen-&gt;capacity_in_bytes());
1914       counters-&gt;update_young_capacity(young_gen-&gt;capacity_in_bytes());
1915     }
1916 
1917     heap-&gt;resize_all_tlabs();
1918 
1919     // Resize the metaspace capacity after a collection
1920     MetaspaceGC::compute_new_size();
1921 
1922     if (log_is_enabled(Debug, gc, heap, exit)) {
1923       accumulated_time()-&gt;stop();
1924     }
1925 
1926     heap-&gt;print_heap_change(pre_gc_values);
1927 
1928     // Track memory usage and detect low memory
1929     MemoryService::track_memory_usage();
1930     heap-&gt;update_counters();
1931 
1932     heap-&gt;post_full_gc_dump(&amp;_gc_timer);
1933   }
1934 
1935 #ifdef ASSERT
1936   for (size_t i = 0; i &lt; ParallelGCThreads + 1; ++i) {
1937     ParCompactionManager* const cm =
1938       ParCompactionManager::manager_array(int(i));
1939     assert(cm-&gt;marking_stack()-&gt;is_empty(),       "should be empty");
1940     assert(cm-&gt;region_stack()-&gt;is_empty(), "Region stack " SIZE_FORMAT " is not empty", i);
1941   }
1942 #endif // ASSERT
1943 
1944   if (VerifyAfterGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
1945     Universe::verify("After GC");
1946   }
1947 
1948   // Re-verify object start arrays
1949   if (VerifyObjectStartArray &amp;&amp;
1950       VerifyAfterGC) {
1951     old_gen-&gt;verify_object_start_array();
1952   }
1953 
1954   if (ZapUnusedHeapArea) {
1955     old_gen-&gt;object_space()-&gt;check_mangled_unused_area_complete();
1956   }
1957 
1958   NOT_PRODUCT(ref_processor()-&gt;verify_no_references_recorded());
1959 
1960   collection_exit.update();
1961 
1962   heap-&gt;print_heap_after_gc();
1963   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
1964 
1965   log_debug(gc, task, time)("VM-Thread " JLONG_FORMAT " " JLONG_FORMAT " " JLONG_FORMAT,
1966                          marking_start.ticks(), compaction_start.ticks(),
1967                          collection_exit.ticks());
1968 
1969   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
1970 
1971   _gc_timer.register_gc_end();
1972 
1973   _gc_tracer.report_dense_prefix(dense_prefix(old_space_id));
1974   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
1975 
1976   return true;
1977 }
1978 
1979 class PCAddThreadRootsMarkingTaskClosure : public ThreadClosure {
1980 private:
1981   uint _worker_id;
1982 
1983 public:
1984   PCAddThreadRootsMarkingTaskClosure(uint worker_id) : _worker_id(worker_id) { }
1985   void do_thread(Thread* thread) {
1986     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
1987 
1988     ResourceMark rm;
1989 
1990     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(_worker_id);
1991 
1992     PCMarkAndPushClosure mark_and_push_closure(cm);
1993     MarkingCodeBlobClosure mark_and_push_in_blobs(&amp;mark_and_push_closure, !CodeBlobToOopClosure::FixRelocations);
1994 
1995     thread-&gt;oops_do(&amp;mark_and_push_closure, &amp;mark_and_push_in_blobs);
1996 
1997     // Do the real work
1998     cm-&gt;follow_marking_stacks();
1999   }
2000 };
2001 
2002 static void mark_from_roots_work(ParallelRootType::Value root_type, uint worker_id) {
2003   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2004 
2005   ParCompactionManager* cm =
2006     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2007   PCMarkAndPushClosure mark_and_push_closure(cm);
2008 
2009   switch (root_type) {
2010     case ParallelRootType::object_synchronizer:
2011       ObjectSynchronizer::oops_do(&amp;mark_and_push_closure);
2012       break;
2013 
2014     case ParallelRootType::class_loader_data:
2015       {
2016         CLDToOopClosure cld_closure(&amp;mark_and_push_closure, ClassLoaderData::_claim_strong);
2017         ClassLoaderDataGraph::always_strong_cld_do(&amp;cld_closure);
2018       }
2019       break;
2020 
2021     case ParallelRootType::code_cache:
2022       // Do not treat nmethods as strong roots for mark/sweep, since we can unload them.
2023       //ScavengableNMethods::scavengable_nmethods_do(CodeBlobToOopClosure(&amp;mark_and_push_closure));
2024       AOTLoader::oops_do(&amp;mark_and_push_closure);
2025       break;
2026 
2027     case ParallelRootType::sentinel:
2028     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
2029       fatal("Bad enumeration value: %u", root_type);
2030       break;
2031   }
2032 
2033   // Do the real work
2034   cm-&gt;follow_marking_stacks();
2035 }
2036 
2037 static void steal_marking_work(TaskTerminator&amp; terminator, uint worker_id) {
2038   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2039 
2040   ParCompactionManager* cm =
2041     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2042 
2043   oop obj = NULL;
2044   ObjArrayTask task;
2045   do {
2046     while (ParCompactionManager::steal_objarray(worker_id,  task)) {
2047       cm-&gt;follow_array((objArrayOop)task.obj(), task.index());
2048       cm-&gt;follow_marking_stacks();
2049     }
2050     while (ParCompactionManager::steal(worker_id, obj)) {
2051       cm-&gt;follow_contents(obj);
2052       cm-&gt;follow_marking_stacks();
2053     }
2054   } while (!terminator.offer_termination());
2055 }
2056 
2057 class MarkFromRootsTask : public AbstractGangTask {
2058   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2059   StrongRootsScope _strong_roots_scope; // needed for Threads::possibly_parallel_threads_do
2060   OopStorageSetStrongParState&lt;false /* concurrent */, false /* is_const */&gt; _oop_storage_set_par_state;
2061   SequentialSubTasksDone _subtasks;
2062   TaskTerminator _terminator;
2063   uint _active_workers;
2064 
2065 public:
2066   MarkFromRootsTask(uint active_workers) :
2067       AbstractGangTask("MarkFromRootsTask"),
2068       _strong_roots_scope(active_workers),
2069       _subtasks(),
2070       _terminator(active_workers, ParCompactionManager::oop_task_queues()),
2071       _active_workers(active_workers) {
2072     _subtasks.set_n_threads(active_workers);
2073     _subtasks.set_n_tasks(ParallelRootType::sentinel);
2074   }
2075 
2076   virtual void work(uint worker_id) {
2077     for (uint task = 0; _subtasks.try_claim_task(task); /*empty*/ ) {
2078       mark_from_roots_work(static_cast&lt;ParallelRootType::Value&gt;(task), worker_id);
2079     }
2080     _subtasks.all_tasks_completed();
2081 
2082     PCAddThreadRootsMarkingTaskClosure closure(worker_id);
2083     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
2084 
2085     // Mark from OopStorages
2086     {
2087       ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2088       PCMarkAndPushClosure closure(cm);
2089       _oop_storage_set_par_state.oops_do(&amp;closure);
2090       // Do the real work
2091       cm-&gt;follow_marking_stacks();
2092     }
2093 
2094     if (_active_workers &gt; 1) {
2095       steal_marking_work(_terminator, worker_id);
2096     }
2097   }
2098 };
2099 
2100 class PCRefProcTask : public AbstractGangTask {
2101   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2102   ProcessTask&amp; _task;
2103   uint _ergo_workers;
2104   TaskTerminator _terminator;
2105 
2106 public:
2107   PCRefProcTask(ProcessTask&amp; task, uint ergo_workers) :
2108       AbstractGangTask("PCRefProcTask"),
2109       _task(task),
2110       _ergo_workers(ergo_workers),
2111       _terminator(_ergo_workers, ParCompactionManager::oop_task_queues()) {
2112   }
2113 
2114   virtual void work(uint worker_id) {
2115     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2116     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2117 
2118     ParCompactionManager* cm =
2119       ParCompactionManager::gc_thread_compaction_manager(worker_id);
2120     PCMarkAndPushClosure mark_and_push_closure(cm);
2121     ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2122     _task.work(worker_id, *PSParallelCompact::is_alive_closure(),
2123                mark_and_push_closure, follow_stack_closure);
2124 
2125     steal_marking_work(_terminator, worker_id);
2126   }
2127 };
2128 
2129 class RefProcTaskExecutor: public AbstractRefProcTaskExecutor {
2130   void execute(ProcessTask&amp; process_task, uint ergo_workers) {
2131     assert(ParallelScavengeHeap::heap()-&gt;workers().active_workers() == ergo_workers,
2132            "Ergonomically chosen workers (%u) must be equal to active workers (%u)",
2133            ergo_workers, ParallelScavengeHeap::heap()-&gt;workers().active_workers());
2134 
2135     PCRefProcTask task(process_task, ergo_workers);
2136     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2137   }
2138 };
2139 
2140 void PSParallelCompact::marking_phase(ParCompactionManager* cm,
2141                                       bool maximum_heap_compaction,
2142                                       ParallelOldTracer *gc_tracer) {
2143   // Recursively traverse all live objects and mark them
2144   GCTraceTime(Info, gc, phases) tm("Marking Phase", &amp;_gc_timer);
2145 
2146   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2147   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2148 
2149   PCMarkAndPushClosure mark_and_push_closure(cm);
2150   ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2151 
2152   // Need new claim bits before marking starts.
2153   ClassLoaderDataGraph::clear_claimed_marks();
2154 
2155   {
2156     GCTraceTime(Debug, gc, phases) tm("Par Mark", &amp;_gc_timer);
2157 
2158     MarkFromRootsTask task(active_gc_threads);
2159     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2160   }
2161 
2162   // Process reference objects found during marking
2163   {
2164     GCTraceTime(Debug, gc, phases) tm("Reference Processing", &amp;_gc_timer);
2165 
2166     ReferenceProcessorStats stats;
2167     ReferenceProcessorPhaseTimes pt(&amp;_gc_timer, ref_processor()-&gt;max_num_queues());
2168 
2169     if (ref_processor()-&gt;processing_is_mt()) {
2170       ref_processor()-&gt;set_active_mt_degree(active_gc_threads);
2171 
2172       RefProcTaskExecutor task_executor;
2173       stats = ref_processor()-&gt;process_discovered_references(
2174         is_alive_closure(), &amp;mark_and_push_closure, &amp;follow_stack_closure,
2175         &amp;task_executor, &amp;pt);
2176     } else {
2177       stats = ref_processor()-&gt;process_discovered_references(
2178         is_alive_closure(), &amp;mark_and_push_closure, &amp;follow_stack_closure, NULL,
2179         &amp;pt);
2180     }
2181 
2182     gc_tracer-&gt;report_gc_reference_stats(stats);
2183     pt.print_all_references();
2184   }
2185 
2186   // This is the point where the entire marking should have completed.
2187   assert(cm-&gt;marking_stacks_empty(), "Marking should have completed");
2188 
2189   {
2190     GCTraceTime(Debug, gc, phases) tm("Weak Processing", &amp;_gc_timer);
2191     WeakProcessor::weak_oops_do(is_alive_closure(), &amp;do_nothing_cl);
2192   }
2193 
2194   {
2195     GCTraceTime(Debug, gc, phases) tm_m("Class Unloading", &amp;_gc_timer);
2196 
2197     // Follow system dictionary roots and unload classes.
2198     bool purged_class = SystemDictionary::do_unloading(&amp;_gc_timer);
2199 
2200     // Unload nmethods.
2201     CodeCache::do_unloading(is_alive_closure(), purged_class);
2202 
2203     // Prune dead klasses from subklass/sibling/implementor lists.
2204     Klass::clean_weak_klass_links(purged_class);
2205 
2206     // Clean JVMCI metadata handles.
2207     JVMCI_ONLY(JVMCI::do_unloading(purged_class));
2208   }
2209 
2210   _gc_tracer.report_object_count_after_gc(is_alive_closure());
2211 }
2212 
2213 void PSParallelCompact::adjust_roots(ParCompactionManager* cm) {
2214   // Adjust the pointers to reflect the new locations
2215   GCTraceTime(Info, gc, phases) tm("Adjust Roots", &amp;_gc_timer);
2216 
2217   // Need new claim bits when tracing through and adjusting pointers.
2218   ClassLoaderDataGraph::clear_claimed_marks();
2219 
2220   PCAdjustPointerClosure oop_closure(cm);
2221 
2222   // General strong roots.
2223   Threads::oops_do(&amp;oop_closure, NULL);
2224   ObjectSynchronizer::oops_do(&amp;oop_closure);
2225   OopStorageSet::strong_oops_do(&amp;oop_closure);
2226   CLDToOopClosure cld_closure(&amp;oop_closure, ClassLoaderData::_claim_strong);
2227   ClassLoaderDataGraph::cld_do(&amp;cld_closure);
2228 
2229   // Now adjust pointers in remaining weak roots.  (All of which should
2230   // have been cleared if they pointed to non-surviving objects.)
2231   WeakProcessor::oops_do(&amp;oop_closure);
2232 
2233   CodeBlobToOopClosure adjust_from_blobs(&amp;oop_closure, CodeBlobToOopClosure::FixRelocations);
2234   CodeCache::blobs_do(&amp;adjust_from_blobs);
2235   AOT_ONLY(AOTLoader::oops_do(&amp;oop_closure);)
2236 
2237   ref_processor()-&gt;weak_oops_do(&amp;oop_closure);
2238   // Roots were visited so references into the young gen in roots
2239   // may have been scanned.  Process them also.
2240   // Should the reference processor have a span that excludes
2241   // young gen objects?
2242   PSScavenge::reference_processor()-&gt;weak_oops_do(&amp;oop_closure);
2243 }
2244 
2245 // Helper class to print 8 region numbers per line and then print the total at the end.
2246 class FillableRegionLogger : public StackObj {
2247 private:
2248   Log(gc, compaction) log;
2249   static const int LineLength = 8;
2250   size_t _regions[LineLength];
2251   int _next_index;
2252   bool _enabled;
2253   size_t _total_regions;
2254 public:
2255   FillableRegionLogger() : _next_index(0), _enabled(log_develop_is_enabled(Trace, gc, compaction)), _total_regions(0) { }
2256   ~FillableRegionLogger() {
2257     log.trace(SIZE_FORMAT " initially fillable regions", _total_regions);
2258   }
2259 
2260   void print_line() {
2261     if (!_enabled || _next_index == 0) {
2262       return;
2263     }
2264     FormatBuffer&lt;&gt; line("Fillable: ");
2265     for (int i = 0; i &lt; _next_index; i++) {
2266       line.append(" " SIZE_FORMAT_W(7), _regions[i]);
2267     }
2268     log.trace("%s", line.buffer());
2269     _next_index = 0;
2270   }
2271 
2272   void handle(size_t region) {
2273     if (!_enabled) {
2274       return;
2275     }
2276     _regions[_next_index++] = region;
2277     if (_next_index == LineLength) {
2278       print_line();
2279     }
2280     _total_regions++;
2281   }
2282 };
2283 
2284 void PSParallelCompact::prepare_region_draining_tasks(uint parallel_gc_threads)
2285 {
2286   GCTraceTime(Trace, gc, phases) tm("Drain Task Setup", &amp;_gc_timer);
2287 
2288   // Find the threads that are active
2289   uint worker_id = 0;
2290 
2291   // Find all regions that are available (can be filled immediately) and
2292   // distribute them to the thread stacks.  The iteration is done in reverse
2293   // order (high to low) so the regions will be removed in ascending order.
2294 
2295   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2296 
2297   // id + 1 is used to test termination so unsigned  can
2298   // be used with an old_space_id == 0.
2299   FillableRegionLogger region_logger;
2300   for (unsigned int id = to_space_id; id + 1 &gt; old_space_id; --id) {
2301     SpaceInfo* const space_info = _space_info + id;
2302     MutableSpace* const space = space_info-&gt;space();
2303     HeapWord* const new_top = space_info-&gt;new_top();
2304 
2305     const size_t beg_region = sd.addr_to_region_idx(space_info-&gt;dense_prefix());
2306     const size_t end_region =
2307       sd.addr_to_region_idx(sd.region_align_up(new_top));
2308 
2309     for (size_t cur = end_region - 1; cur + 1 &gt; beg_region; --cur) {
2310       if (sd.region(cur)-&gt;claim_unsafe()) {
2311         ParCompactionManager* cm = ParCompactionManager::manager_array(worker_id);
2312         bool result = sd.region(cur)-&gt;mark_normal();
2313         assert(result, "Must succeed at this point.");
2314         cm-&gt;region_stack()-&gt;push(cur);
2315         region_logger.handle(cur);
2316         // Assign regions to tasks in round-robin fashion.
2317         if (++worker_id == parallel_gc_threads) {
2318           worker_id = 0;
2319         }
2320       }
2321     }
2322     region_logger.print_line();
2323   }
2324 }
2325 
2326 class TaskQueue : StackObj {
2327   volatile uint _counter;
2328   uint _size;
2329   uint _insert_index;
2330   PSParallelCompact::UpdateDensePrefixTask* _backing_array;
2331 public:
2332   explicit TaskQueue(uint size) : _counter(0), _size(size), _insert_index(0), _backing_array(NULL) {
2333     _backing_array = NEW_C_HEAP_ARRAY(PSParallelCompact::UpdateDensePrefixTask, _size, mtGC);
2334   }
2335   ~TaskQueue() {
2336     assert(_counter &gt;= _insert_index, "not all queue elements were claimed");
2337     FREE_C_HEAP_ARRAY(T, _backing_array);
2338   }
2339 
2340   void push(const PSParallelCompact::UpdateDensePrefixTask&amp; value) {
2341     assert(_insert_index &lt; _size, "too small backing array");
2342     _backing_array[_insert_index++] = value;
2343   }
2344 
2345   bool try_claim(PSParallelCompact::UpdateDensePrefixTask&amp; reference) {
2346     uint claimed = Atomic::fetch_and_add(&amp;_counter, 1u);
2347     if (claimed &lt; _insert_index) {
2348       reference = _backing_array[claimed];
2349       return true;
2350     } else {
2351       return false;
2352     }
2353   }
2354 };
2355 
2356 #define PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING 4
2357 
2358 void PSParallelCompact::enqueue_dense_prefix_tasks(TaskQueue&amp; task_queue,
2359                                                    uint parallel_gc_threads) {
2360   GCTraceTime(Trace, gc, phases) tm("Dense Prefix Task Setup", &amp;_gc_timer);
2361 
2362   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2363 
2364   // Iterate over all the spaces adding tasks for updating
2365   // regions in the dense prefix.  Assume that 1 gc thread
2366   // will work on opening the gaps and the remaining gc threads
2367   // will work on the dense prefix.
2368   unsigned int space_id;
2369   for (space_id = old_space_id; space_id &lt; last_space_id; ++ space_id) {
2370     HeapWord* const dense_prefix_end = _space_info[space_id].dense_prefix();
2371     const MutableSpace* const space = _space_info[space_id].space();
2372 
2373     if (dense_prefix_end == space-&gt;bottom()) {
2374       // There is no dense prefix for this space.
2375       continue;
2376     }
2377 
2378     // The dense prefix is before this region.
2379     size_t region_index_end_dense_prefix =
2380         sd.addr_to_region_idx(dense_prefix_end);
2381     RegionData* const dense_prefix_cp =
2382       sd.region(region_index_end_dense_prefix);
2383     assert(dense_prefix_end == space-&gt;end() ||
2384            dense_prefix_cp-&gt;available() ||
2385            dense_prefix_cp-&gt;claimed(),
2386            "The region after the dense prefix should always be ready to fill");
2387 
2388     size_t region_index_start = sd.addr_to_region_idx(space-&gt;bottom());
2389 
2390     // Is there dense prefix work?
2391     size_t total_dense_prefix_regions =
2392       region_index_end_dense_prefix - region_index_start;
2393     // How many regions of the dense prefix should be given to
2394     // each thread?
2395     if (total_dense_prefix_regions &gt; 0) {
2396       uint tasks_for_dense_prefix = 1;
2397       if (total_dense_prefix_regions &lt;=
2398           (parallel_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)) {
2399         // Don't over partition.  This assumes that
2400         // PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING is a small integer value
2401         // so there are not many regions to process.
2402         tasks_for_dense_prefix = parallel_gc_threads;
2403       } else {
2404         // Over partition
2405         tasks_for_dense_prefix = parallel_gc_threads *
2406           PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING;
2407       }
2408       size_t regions_per_thread = total_dense_prefix_regions /
2409         tasks_for_dense_prefix;
2410       // Give each thread at least 1 region.
2411       if (regions_per_thread == 0) {
2412         regions_per_thread = 1;
2413       }
2414 
2415       for (uint k = 0; k &lt; tasks_for_dense_prefix; k++) {
2416         if (region_index_start &gt;= region_index_end_dense_prefix) {
2417           break;
2418         }
2419         // region_index_end is not processed
2420         size_t region_index_end = MIN2(region_index_start + regions_per_thread,
2421                                        region_index_end_dense_prefix);
2422         task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),
2423                                               region_index_start,
2424                                               region_index_end));
2425         region_index_start = region_index_end;
2426       }
2427     }
2428     // This gets any part of the dense prefix that did not
2429     // fit evenly.
2430     if (region_index_start &lt; region_index_end_dense_prefix) {
2431       task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),
2432                                             region_index_start,
2433                                             region_index_end_dense_prefix));
2434     }
2435   }
2436 }
2437 
2438 #ifdef ASSERT
2439 // Write a histogram of the number of times the block table was filled for a
2440 // region.
2441 void PSParallelCompact::write_block_fill_histogram()
2442 {
2443   if (!log_develop_is_enabled(Trace, gc, compaction)) {
2444     return;
2445   }
2446 
2447   Log(gc, compaction) log;
2448   ResourceMark rm;
2449   LogStream ls(log.trace());
2450   outputStream* out = &amp;ls;
2451 
2452   typedef ParallelCompactData::RegionData rd_t;
2453   ParallelCompactData&amp; sd = summary_data();
2454 
2455   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2456     MutableSpace* const spc = _space_info[id].space();
2457     if (spc-&gt;bottom() != spc-&gt;top()) {
2458       const rd_t* const beg = sd.addr_to_region_ptr(spc-&gt;bottom());
2459       HeapWord* const top_aligned_up = sd.region_align_up(spc-&gt;top());
2460       const rd_t* const end = sd.addr_to_region_ptr(top_aligned_up);
2461 
2462       size_t histo[5] = { 0, 0, 0, 0, 0 };
2463       const size_t histo_len = sizeof(histo) / sizeof(size_t);
2464       const size_t region_cnt = pointer_delta(end, beg, sizeof(rd_t));
2465 
2466       for (const rd_t* cur = beg; cur &lt; end; ++cur) {
2467         ++histo[MIN2(cur-&gt;blocks_filled_count(), histo_len - 1)];
2468       }
2469       out-&gt;print("Block fill histogram: %u %-4s" SIZE_FORMAT_W(5), id, space_names[id], region_cnt);
2470       for (size_t i = 0; i &lt; histo_len; ++i) {
2471         out-&gt;print(" " SIZE_FORMAT_W(5) " %5.1f%%",
2472                    histo[i], 100.0 * histo[i] / region_cnt);
2473       }
2474       out-&gt;cr();
2475     }
2476   }
2477 }
2478 #endif // #ifdef ASSERT
2479 
2480 static void compaction_with_stealing_work(TaskTerminator* terminator, uint worker_id) {
2481   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2482 
2483   ParCompactionManager* cm =
2484     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2485 
2486   // Drain the stacks that have been preloaded with regions
2487   // that are ready to fill.
2488 
2489   cm-&gt;drain_region_stacks();
2490 
2491   guarantee(cm-&gt;region_stack()-&gt;is_empty(), "Not empty");
2492 
2493   size_t region_index = 0;
2494 
2495   while (true) {
2496     if (ParCompactionManager::steal(worker_id, region_index)) {
2497       PSParallelCompact::fill_and_update_region(cm, region_index);
2498       cm-&gt;drain_region_stacks();
2499     } else if (PSParallelCompact::steal_unavailable_region(cm, region_index)) {
2500       // Fill and update an unavailable region with the help of a shadow region
2501       PSParallelCompact::fill_and_update_shadow_region(cm, region_index);
2502       cm-&gt;drain_region_stacks();
2503     } else {
2504       if (terminator-&gt;offer_termination()) {
2505         break;
2506       }
2507       // Go around again.
2508     }
2509   }
2510   return;
2511 }
2512 
2513 class UpdateDensePrefixAndCompactionTask: public AbstractGangTask {
2514   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2515   TaskQueue&amp; _tq;
2516   TaskTerminator _terminator;
2517   uint _active_workers;
2518 
2519 public:
2520   UpdateDensePrefixAndCompactionTask(TaskQueue&amp; tq, uint active_workers) :
2521       AbstractGangTask("UpdateDensePrefixAndCompactionTask"),
2522       _tq(tq),
2523       _terminator(active_workers, ParCompactionManager::region_task_queues()),
2524       _active_workers(active_workers) {
2525   }
2526   virtual void work(uint worker_id) {
2527     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2528 
2529     for (PSParallelCompact::UpdateDensePrefixTask task; _tq.try_claim(task); /* empty */) {
2530       PSParallelCompact::update_and_deadwood_in_dense_prefix(cm,
2531                                                              task._space_id,
2532                                                              task._region_index_start,
2533                                                              task._region_index_end);
2534     }
2535 
2536     // Once a thread has drained it's stack, it should try to steal regions from
2537     // other threads.
2538     compaction_with_stealing_work(&amp;_terminator, worker_id);
2539   }
2540 };
2541 
2542 void PSParallelCompact::compact() {
2543   GCTraceTime(Info, gc, phases) tm("Compaction Phase", &amp;_gc_timer);
2544 
2545   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2546   PSOldGen* old_gen = heap-&gt;old_gen();
2547   old_gen-&gt;start_array()-&gt;reset();
2548   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2549 
2550   // for [0..last_space_id)
2551   //     for [0..active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)
2552   //         push
2553   //     push
2554   //
2555   // max push count is thus: last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1)
2556   TaskQueue task_queue(last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1));
2557   initialize_shadow_regions(active_gc_threads);
2558   prepare_region_draining_tasks(active_gc_threads);
2559   enqueue_dense_prefix_tasks(task_queue, active_gc_threads);
2560 
2561   {
2562     GCTraceTime(Trace, gc, phases) tm("Par Compact", &amp;_gc_timer);
2563 
2564     UpdateDensePrefixAndCompactionTask task(task_queue, active_gc_threads);
2565     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2566 
2567 #ifdef  ASSERT
2568     // Verify that all regions have been processed before the deferred updates.
2569     for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2570       verify_complete(SpaceId(id));
2571     }
2572 #endif
2573   }
2574 
2575   {
2576     // Update the deferred objects, if any.  Any compaction manager can be used.
2577     GCTraceTime(Trace, gc, phases) tm("Deferred Updates", &amp;_gc_timer);
2578     ParCompactionManager* cm = ParCompactionManager::manager_array(0);
2579     for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2580       update_deferred_objects(cm, SpaceId(id));
2581     }
2582   }
2583 
2584   DEBUG_ONLY(write_block_fill_histogram());
2585 }
2586 
2587 #ifdef  ASSERT
2588 void PSParallelCompact::verify_complete(SpaceId space_id) {
2589   // All Regions between space bottom() to new_top() should be marked as filled
2590   // and all Regions between new_top() and top() should be available (i.e.,
2591   // should have been emptied).
2592   ParallelCompactData&amp; sd = summary_data();
2593   SpaceInfo si = _space_info[space_id];
2594   HeapWord* new_top_addr = sd.region_align_up(si.new_top());
2595   HeapWord* old_top_addr = sd.region_align_up(si.space()-&gt;top());
2596   const size_t beg_region = sd.addr_to_region_idx(si.space()-&gt;bottom());
2597   const size_t new_top_region = sd.addr_to_region_idx(new_top_addr);
2598   const size_t old_top_region = sd.addr_to_region_idx(old_top_addr);
2599 
2600   bool issued_a_warning = false;
2601 
2602   size_t cur_region;
2603   for (cur_region = beg_region; cur_region &lt; new_top_region; ++cur_region) {
2604     const RegionData* const c = sd.region(cur_region);
2605     if (!c-&gt;completed()) {
2606       log_warning(gc)("region " SIZE_FORMAT " not filled: destination_count=%u",
2607                       cur_region, c-&gt;destination_count());
2608       issued_a_warning = true;
2609     }
2610   }
2611 
2612   for (cur_region = new_top_region; cur_region &lt; old_top_region; ++cur_region) {
2613     const RegionData* const c = sd.region(cur_region);
2614     if (!c-&gt;available()) {
2615       log_warning(gc)("region " SIZE_FORMAT " not empty: destination_count=%u",
2616                       cur_region, c-&gt;destination_count());
2617       issued_a_warning = true;
2618     }
2619   }
2620 
2621   if (issued_a_warning) {
2622     print_region_ranges();
2623   }
2624 }
2625 #endif  // #ifdef ASSERT
2626 
2627 inline void UpdateOnlyClosure::do_addr(HeapWord* addr) {
2628   _start_array-&gt;allocate_block(addr);
2629   compaction_manager()-&gt;update_contents(oop(addr));
2630 }
2631 
2632 // Update interior oops in the ranges of regions [beg_region, end_region).
2633 void
2634 PSParallelCompact::update_and_deadwood_in_dense_prefix(ParCompactionManager* cm,
2635                                                        SpaceId space_id,
2636                                                        size_t beg_region,
2637                                                        size_t end_region) {
2638   ParallelCompactData&amp; sd = summary_data();
2639   ParMarkBitMap* const mbm = mark_bitmap();
2640 
2641   HeapWord* beg_addr = sd.region_to_addr(beg_region);
2642   HeapWord* const end_addr = sd.region_to_addr(end_region);
2643   assert(beg_region &lt;= end_region, "bad region range");
2644   assert(end_addr &lt;= dense_prefix(space_id), "not in the dense prefix");
2645 
2646 #ifdef  ASSERT
2647   // Claim the regions to avoid triggering an assert when they are marked as
2648   // filled.
2649   for (size_t claim_region = beg_region; claim_region &lt; end_region; ++claim_region) {
2650     assert(sd.region(claim_region)-&gt;claim_unsafe(), "claim() failed");
2651   }
2652 #endif  // #ifdef ASSERT
2653 
2654   if (beg_addr != space(space_id)-&gt;bottom()) {
2655     // Find the first live object or block of dead space that *starts* in this
2656     // range of regions.  If a partial object crosses onto the region, skip it;
2657     // it will be marked for 'deferred update' when the object head is
2658     // processed.  If dead space crosses onto the region, it is also skipped; it
2659     // will be filled when the prior region is processed.  If neither of those
2660     // apply, the first word in the region is the start of a live object or dead
2661     // space.
2662     assert(beg_addr &gt; space(space_id)-&gt;bottom(), "sanity");
2663     const RegionData* const cp = sd.region(beg_region);
2664     if (cp-&gt;partial_obj_size() != 0) {
2665       beg_addr = sd.partial_obj_end(beg_region);
2666     } else if (dead_space_crosses_boundary(cp, mbm-&gt;addr_to_bit(beg_addr))) {
2667       beg_addr = mbm-&gt;find_obj_beg(beg_addr, end_addr);
2668     }
2669   }
2670 
2671   if (beg_addr &lt; end_addr) {
2672     // A live object or block of dead space starts in this range of Regions.
2673      HeapWord* const dense_prefix_end = dense_prefix(space_id);
2674 
2675     // Create closures and iterate.
2676     UpdateOnlyClosure update_closure(mbm, cm, space_id);
2677     FillClosure fill_closure(cm, space_id);
2678     ParMarkBitMap::IterationStatus status;
2679     status = mbm-&gt;iterate(&amp;update_closure, &amp;fill_closure, beg_addr, end_addr,
2680                           dense_prefix_end);
2681     if (status == ParMarkBitMap::incomplete) {
2682       update_closure.do_addr(update_closure.source());
2683     }
2684   }
2685 
2686   // Mark the regions as filled.
2687   RegionData* const beg_cp = sd.region(beg_region);
2688   RegionData* const end_cp = sd.region(end_region);
2689   for (RegionData* cp = beg_cp; cp &lt; end_cp; ++cp) {
2690     cp-&gt;set_completed();
2691   }
2692 }
2693 
2694 // Return the SpaceId for the space containing addr.  If addr is not in the
2695 // heap, last_space_id is returned.  In debug mode it expects the address to be
2696 // in the heap and asserts such.
2697 PSParallelCompact::SpaceId PSParallelCompact::space_id(HeapWord* addr) {
2698   assert(ParallelScavengeHeap::heap()-&gt;is_in_reserved(addr), "addr not in the heap");
2699 
2700   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2701     if (_space_info[id].space()-&gt;contains(addr)) {
2702       return SpaceId(id);
2703     }
2704   }
2705 
2706   assert(false, "no space contains the addr");
2707   return last_space_id;
2708 }
2709 
2710 void PSParallelCompact::update_deferred_objects(ParCompactionManager* cm,
2711                                                 SpaceId id) {
2712   assert(id &lt; last_space_id, "bad space id");
2713 
2714   ParallelCompactData&amp; sd = summary_data();
2715   const SpaceInfo* const space_info = _space_info + id;
2716   ObjectStartArray* const start_array = space_info-&gt;start_array();
2717 
2718   const MutableSpace* const space = space_info-&gt;space();
2719   assert(space_info-&gt;dense_prefix() &gt;= space-&gt;bottom(), "dense_prefix not set");
2720   HeapWord* const beg_addr = space_info-&gt;dense_prefix();
2721   HeapWord* const end_addr = sd.region_align_up(space_info-&gt;new_top());
2722 
2723   const RegionData* const beg_region = sd.addr_to_region_ptr(beg_addr);
2724   const RegionData* const end_region = sd.addr_to_region_ptr(end_addr);
2725   const RegionData* cur_region;
2726   for (cur_region = beg_region; cur_region &lt; end_region; ++cur_region) {
2727     HeapWord* const addr = cur_region-&gt;deferred_obj_addr();
2728     if (addr != NULL) {
2729       if (start_array != NULL) {
2730         start_array-&gt;allocate_block(addr);
2731       }
2732       cm-&gt;update_contents(oop(addr));
2733       assert(oopDesc::is_oop_or_null(oop(addr)), "Expected an oop or NULL at " PTR_FORMAT, p2i(oop(addr)));
2734     }
2735   }
2736 }
2737 
2738 // Skip over count live words starting from beg, and return the address of the
2739 // next live word.  Unless marked, the word corresponding to beg is assumed to
2740 // be dead.  Callers must either ensure beg does not correspond to the middle of
2741 // an object, or account for those live words in some other way.  Callers must
2742 // also ensure that there are enough live words in the range [beg, end) to skip.
2743 HeapWord*
2744 PSParallelCompact::skip_live_words(HeapWord* beg, HeapWord* end, size_t count)
2745 {
2746   assert(count &gt; 0, "sanity");
2747 
2748   ParMarkBitMap* m = mark_bitmap();
2749   idx_t bits_to_skip = m-&gt;words_to_bits(count);
2750   idx_t cur_beg = m-&gt;addr_to_bit(beg);
2751   const idx_t search_end = m-&gt;align_range_end(m-&gt;addr_to_bit(end));
2752 
2753   do {
2754     cur_beg = m-&gt;find_obj_beg(cur_beg, search_end);
2755     idx_t cur_end = m-&gt;find_obj_end(cur_beg, search_end);
2756     const size_t obj_bits = cur_end - cur_beg + 1;
2757     if (obj_bits &gt; bits_to_skip) {
2758       return m-&gt;bit_to_addr(cur_beg + bits_to_skip);
2759     }
2760     bits_to_skip -= obj_bits;
2761     cur_beg = cur_end + 1;
2762   } while (bits_to_skip &gt; 0);
2763 
2764   // Skipping the desired number of words landed just past the end of an object.
2765   // Find the start of the next object.
2766   cur_beg = m-&gt;find_obj_beg(cur_beg, search_end);
2767   assert(cur_beg &lt; m-&gt;addr_to_bit(end), "not enough live words to skip");
2768   return m-&gt;bit_to_addr(cur_beg);
2769 }
2770 
2771 HeapWord* PSParallelCompact::first_src_addr(HeapWord* const dest_addr,
2772                                             SpaceId src_space_id,
2773                                             size_t src_region_idx)
2774 {
2775   assert(summary_data().is_region_aligned(dest_addr), "not aligned");
2776 
2777   const SplitInfo&amp; split_info = _space_info[src_space_id].split_info();
2778   if (split_info.dest_region_addr() == dest_addr) {
2779     // The partial object ending at the split point contains the first word to
2780     // be copied to dest_addr.
2781     return split_info.first_src_addr();
2782   }
2783 
2784   const ParallelCompactData&amp; sd = summary_data();
2785   ParMarkBitMap* const bitmap = mark_bitmap();
2786   const size_t RegionSize = ParallelCompactData::RegionSize;
2787 
2788   assert(sd.is_region_aligned(dest_addr), "not aligned");
2789   const RegionData* const src_region_ptr = sd.region(src_region_idx);
2790   const size_t partial_obj_size = src_region_ptr-&gt;partial_obj_size();
2791   HeapWord* const src_region_destination = src_region_ptr-&gt;destination();
2792 
2793   assert(dest_addr &gt;= src_region_destination, "wrong src region");
2794   assert(src_region_ptr-&gt;data_size() &gt; 0, "src region cannot be empty");
2795 
2796   HeapWord* const src_region_beg = sd.region_to_addr(src_region_idx);
2797   HeapWord* const src_region_end = src_region_beg + RegionSize;
2798 
2799   HeapWord* addr = src_region_beg;
2800   if (dest_addr == src_region_destination) {
2801     // Return the first live word in the source region.
2802     if (partial_obj_size == 0) {
2803       addr = bitmap-&gt;find_obj_beg(addr, src_region_end);
2804       assert(addr &lt; src_region_end, "no objects start in src region");
2805     }
2806     return addr;
2807   }
2808 
2809   // Must skip some live data.
2810   size_t words_to_skip = dest_addr - src_region_destination;
2811   assert(src_region_ptr-&gt;data_size() &gt; words_to_skip, "wrong src region");
2812 
2813   if (partial_obj_size &gt;= words_to_skip) {
2814     // All the live words to skip are part of the partial object.
2815     addr += words_to_skip;
2816     if (partial_obj_size == words_to_skip) {
2817       // Find the first live word past the partial object.
2818       addr = bitmap-&gt;find_obj_beg(addr, src_region_end);
2819       assert(addr &lt; src_region_end, "wrong src region");
2820     }
2821     return addr;
2822   }
2823 
2824   // Skip over the partial object (if any).
2825   if (partial_obj_size != 0) {
2826     words_to_skip -= partial_obj_size;
2827     addr += partial_obj_size;
2828   }
2829 
2830   // Skip over live words due to objects that start in the region.
2831   addr = skip_live_words(addr, src_region_end, words_to_skip);
2832   assert(addr &lt; src_region_end, "wrong src region");
2833   return addr;
2834 }
2835 
2836 void PSParallelCompact::decrement_destination_counts(ParCompactionManager* cm,
2837                                                      SpaceId src_space_id,
2838                                                      size_t beg_region,
2839                                                      HeapWord* end_addr)
2840 {
2841   ParallelCompactData&amp; sd = summary_data();
2842 
2843 #ifdef ASSERT
2844   MutableSpace* const src_space = _space_info[src_space_id].space();
2845   HeapWord* const beg_addr = sd.region_to_addr(beg_region);
2846   assert(src_space-&gt;contains(beg_addr) || beg_addr == src_space-&gt;end(),
2847          "src_space_id does not match beg_addr");
2848   assert(src_space-&gt;contains(end_addr) || end_addr == src_space-&gt;end(),
2849          "src_space_id does not match end_addr");
2850 #endif // #ifdef ASSERT
2851 
2852   RegionData* const beg = sd.region(beg_region);
2853   RegionData* const end = sd.addr_to_region_ptr(sd.region_align_up(end_addr));
2854 
2855   // Regions up to new_top() are enqueued if they become available.
2856   HeapWord* const new_top = _space_info[src_space_id].new_top();
2857   RegionData* const enqueue_end =
2858     sd.addr_to_region_ptr(sd.region_align_up(new_top));
2859 
2860   for (RegionData* cur = beg; cur &lt; end; ++cur) {
2861     assert(cur-&gt;data_size() &gt; 0, "region must have live data");
2862     cur-&gt;decrement_destination_count();
2863     if (cur &lt; enqueue_end &amp;&amp; cur-&gt;available() &amp;&amp; cur-&gt;claim()) {
2864       if (cur-&gt;mark_normal()) {
2865         cm-&gt;push_region(sd.region(cur));
2866       } else if (cur-&gt;mark_copied()) {
2867         // Try to copy the content of the shadow region back to its corresponding
2868         // heap region if the shadow region is filled. Otherwise, the GC thread
2869         // fills the shadow region will copy the data back (see
2870         // MoveAndUpdateShadowClosure::complete_region).
2871         copy_back(sd.region_to_addr(cur-&gt;shadow_region()), sd.region_to_addr(cur));
2872         ParCompactionManager::push_shadow_region_mt_safe(cur-&gt;shadow_region());
2873         cur-&gt;set_completed();
2874       }
2875     }
2876   }
2877 }
2878 
2879 size_t PSParallelCompact::next_src_region(MoveAndUpdateClosure&amp; closure,
2880                                           SpaceId&amp; src_space_id,
2881                                           HeapWord*&amp; src_space_top,
2882                                           HeapWord* end_addr)
2883 {
2884   typedef ParallelCompactData::RegionData RegionData;
2885 
2886   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2887   const size_t region_size = ParallelCompactData::RegionSize;
2888 
2889   size_t src_region_idx = 0;
2890 
2891   // Skip empty regions (if any) up to the top of the space.
2892   HeapWord* const src_aligned_up = sd.region_align_up(end_addr);
2893   RegionData* src_region_ptr = sd.addr_to_region_ptr(src_aligned_up);
2894   HeapWord* const top_aligned_up = sd.region_align_up(src_space_top);
2895   const RegionData* const top_region_ptr =
2896     sd.addr_to_region_ptr(top_aligned_up);
2897   while (src_region_ptr &lt; top_region_ptr &amp;&amp; src_region_ptr-&gt;data_size() == 0) {
2898     ++src_region_ptr;
2899   }
2900 
2901   if (src_region_ptr &lt; top_region_ptr) {
2902     // The next source region is in the current space.  Update src_region_idx
2903     // and the source address to match src_region_ptr.
2904     src_region_idx = sd.region(src_region_ptr);
2905     HeapWord* const src_region_addr = sd.region_to_addr(src_region_idx);
2906     if (src_region_addr &gt; closure.source()) {
2907       closure.set_source(src_region_addr);
2908     }
2909     return src_region_idx;
2910   }
2911 
2912   // Switch to a new source space and find the first non-empty region.
2913   unsigned int space_id = src_space_id + 1;
2914   assert(space_id &lt; last_space_id, "not enough spaces");
2915 
2916   HeapWord* const destination = closure.destination();
2917 
2918   do {
2919     MutableSpace* space = _space_info[space_id].space();
2920     HeapWord* const bottom = space-&gt;bottom();
2921     const RegionData* const bottom_cp = sd.addr_to_region_ptr(bottom);
2922 
2923     // Iterate over the spaces that do not compact into themselves.
2924     if (bottom_cp-&gt;destination() != bottom) {
2925       HeapWord* const top_aligned_up = sd.region_align_up(space-&gt;top());
2926       const RegionData* const top_cp = sd.addr_to_region_ptr(top_aligned_up);
2927 
2928       for (const RegionData* src_cp = bottom_cp; src_cp &lt; top_cp; ++src_cp) {
2929         if (src_cp-&gt;live_obj_size() &gt; 0) {
2930           // Found it.
2931           assert(src_cp-&gt;destination() == destination,
2932                  "first live obj in the space must match the destination");
2933           assert(src_cp-&gt;partial_obj_size() == 0,
2934                  "a space cannot begin with a partial obj");
2935 
2936           src_space_id = SpaceId(space_id);
2937           src_space_top = space-&gt;top();
2938           const size_t src_region_idx = sd.region(src_cp);
2939           closure.set_source(sd.region_to_addr(src_region_idx));
2940           return src_region_idx;
2941         } else {
2942           assert(src_cp-&gt;data_size() == 0, "sanity");
2943         }
2944       }
2945     }
2946   } while (++space_id &lt; last_space_id);
2947 
2948   assert(false, "no source region was found");
2949   return 0;
2950 }
2951 
2952 void PSParallelCompact::fill_region(ParCompactionManager* cm, MoveAndUpdateClosure&amp; closure, size_t region_idx)
2953 {
2954   typedef ParMarkBitMap::IterationStatus IterationStatus;
2955   ParMarkBitMap* const bitmap = mark_bitmap();
2956   ParallelCompactData&amp; sd = summary_data();
2957   RegionData* const region_ptr = sd.region(region_idx);
2958 
2959   // Get the source region and related info.
2960   size_t src_region_idx = region_ptr-&gt;source_region();
2961   SpaceId src_space_id = space_id(sd.region_to_addr(src_region_idx));
2962   HeapWord* src_space_top = _space_info[src_space_id].space()-&gt;top();
2963   HeapWord* dest_addr = sd.region_to_addr(region_idx);
2964 
2965   closure.set_source(first_src_addr(dest_addr, src_space_id, src_region_idx));
2966 
2967   // Adjust src_region_idx to prepare for decrementing destination counts (the
2968   // destination count is not decremented when a region is copied to itself).
2969   if (src_region_idx == region_idx) {
2970     src_region_idx += 1;
2971   }
2972 
2973   if (bitmap-&gt;is_unmarked(closure.source())) {
2974     // The first source word is in the middle of an object; copy the remainder
2975     // of the object or as much as will fit.  The fact that pointer updates were
2976     // deferred will be noted when the object header is processed.
2977     HeapWord* const old_src_addr = closure.source();
2978     closure.copy_partial_obj();
2979     if (closure.is_full()) {
2980       decrement_destination_counts(cm, src_space_id, src_region_idx,
2981                                    closure.source());
2982       region_ptr-&gt;set_deferred_obj_addr(NULL);
2983       closure.complete_region(cm, dest_addr, region_ptr);
2984       return;
2985     }
2986 
2987     HeapWord* const end_addr = sd.region_align_down(closure.source());
2988     if (sd.region_align_down(old_src_addr) != end_addr) {
2989       // The partial object was copied from more than one source region.
2990       decrement_destination_counts(cm, src_space_id, src_region_idx, end_addr);
2991 
2992       // Move to the next source region, possibly switching spaces as well.  All
2993       // args except end_addr may be modified.
2994       src_region_idx = next_src_region(closure, src_space_id, src_space_top,
2995                                        end_addr);
2996     }
2997   }
2998 
2999   do {
3000     HeapWord* const cur_addr = closure.source();
3001     HeapWord* const end_addr = MIN2(sd.region_align_up(cur_addr + 1),
3002                                     src_space_top);
3003     IterationStatus status = bitmap-&gt;iterate(&amp;closure, cur_addr, end_addr);
3004 
3005     if (status == ParMarkBitMap::incomplete) {
3006       // The last obj that starts in the source region does not end in the
3007       // region.
3008       assert(closure.source() &lt; end_addr, "sanity");
3009       HeapWord* const obj_beg = closure.source();
3010       HeapWord* const range_end = MIN2(obj_beg + closure.words_remaining(),
3011                                        src_space_top);
3012       HeapWord* const obj_end = bitmap-&gt;find_obj_end(obj_beg, range_end);
3013       if (obj_end &lt; range_end) {
3014         // The end was found; the entire object will fit.
3015         status = closure.do_addr(obj_beg, bitmap-&gt;obj_size(obj_beg, obj_end));
3016         assert(status != ParMarkBitMap::would_overflow, "sanity");
3017       } else {
3018         // The end was not found; the object will not fit.
3019         assert(range_end &lt; src_space_top, "obj cannot cross space boundary");
3020         status = ParMarkBitMap::would_overflow;
3021       }
3022     }
3023 
3024     if (status == ParMarkBitMap::would_overflow) {
3025       // The last object did not fit.  Note that interior oop updates were
3026       // deferred, then copy enough of the object to fill the region.
3027       region_ptr-&gt;set_deferred_obj_addr(closure.destination());
3028       status = closure.copy_until_full(); // copies from closure.source()
3029 
3030       decrement_destination_counts(cm, src_space_id, src_region_idx,
3031                                    closure.source());
3032       closure.complete_region(cm, dest_addr, region_ptr);
3033       return;
3034     }
3035 
3036     if (status == ParMarkBitMap::full) {
3037       decrement_destination_counts(cm, src_space_id, src_region_idx,
3038                                    closure.source());
3039       region_ptr-&gt;set_deferred_obj_addr(NULL);
3040       closure.complete_region(cm, dest_addr, region_ptr);
3041       return;
3042     }
3043 
3044     decrement_destination_counts(cm, src_space_id, src_region_idx, end_addr);
3045 
3046     // Move to the next source region, possibly switching spaces as well.  All
3047     // args except end_addr may be modified.
3048     src_region_idx = next_src_region(closure, src_space_id, src_space_top,
3049                                      end_addr);
3050   } while (true);
3051 }
3052 
3053 void PSParallelCompact::fill_and_update_region(ParCompactionManager* cm, size_t region_idx)
3054 {
3055   MoveAndUpdateClosure cl(mark_bitmap(), cm, region_idx);
3056   fill_region(cm, cl, region_idx);
3057 }
3058 
3059 void PSParallelCompact::fill_and_update_shadow_region(ParCompactionManager* cm, size_t region_idx)
3060 {
3061   // Get a shadow region first
3062   ParallelCompactData&amp; sd = summary_data();
3063   RegionData* const region_ptr = sd.region(region_idx);
3064   size_t shadow_region = ParCompactionManager::pop_shadow_region_mt_safe(region_ptr);
3065   // The InvalidShadow return value indicates the corresponding heap region is available,
3066   // so use MoveAndUpdateClosure to fill the normal region. Otherwise, use
3067   // MoveAndUpdateShadowClosure to fill the acquired shadow region.
3068   if (shadow_region == ParCompactionManager::InvalidShadow) {
3069     MoveAndUpdateClosure cl(mark_bitmap(), cm, region_idx);
3070     region_ptr-&gt;shadow_to_normal();
3071     return fill_region(cm, cl, region_idx);
3072   } else {
3073     MoveAndUpdateShadowClosure cl(mark_bitmap(), cm, region_idx, shadow_region);
3074     return fill_region(cm, cl, region_idx);
3075   }
3076 }
3077 
3078 void PSParallelCompact::copy_back(HeapWord *shadow_addr, HeapWord *region_addr)
3079 {
3080   Copy::aligned_conjoint_words(shadow_addr, region_addr, _summary_data.RegionSize);
3081 }
3082 
3083 bool PSParallelCompact::steal_unavailable_region(ParCompactionManager* cm, size_t &amp;region_idx)
3084 {
3085   size_t next = cm-&gt;next_shadow_region();
3086   ParallelCompactData&amp; sd = summary_data();
3087   size_t old_new_top = sd.addr_to_region_idx(_space_info[old_space_id].new_top());
3088   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
3089 
3090   while (next &lt; old_new_top) {
3091     if (sd.region(next)-&gt;mark_shadow()) {
3092       region_idx = next;
3093       return true;
3094     }
3095     next = cm-&gt;move_next_shadow_region_by(active_gc_threads);
3096   }
3097 
3098   return false;
3099 }
3100 
3101 // The shadow region is an optimization to address region dependencies in full GC. The basic
3102 // idea is making more regions available by temporally storing their live objects in empty
3103 // shadow regions to resolve dependencies between them and the destination regions. Therefore,
3104 // GC threads need not wait destination regions to be available before processing sources.
3105 //
3106 // A typical workflow would be:
3107 // After draining its own stack and failing to steal from others, a GC worker would pick an
3108 // unavailable region (destination count &gt; 0) and get a shadow region. Then the worker fills
3109 // the shadow region by copying live objects from source regions of the unavailable one. Once
3110 // the unavailable region becomes available, the data in the shadow region will be copied back.
3111 // Shadow regions are empty regions in the to-space and regions between top and end of other spaces.
3112 //
3113 // For more details, please refer to 4.2 of the VEE'19 paper:
3114 // Haoyu Li, Mingyu Wu, Binyu Zang, and Haibo Chen. 2019. ScissorGC: scalable and efficient
3115 // compaction for Java full garbage collection. In Proceedings of the 15th ACM SIGPLAN/SIGOPS
3116 // International Conference on Virtual Execution Environments (VEE 2019). ACM, New York, NY, USA,
3117 // 108-121. DOI: https://doi.org/10.1145/3313808.3313820
3118 void PSParallelCompact::initialize_shadow_regions(uint parallel_gc_threads)
3119 {
3120   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
3121 
3122   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
3123     SpaceInfo* const space_info = _space_info + id;
3124     MutableSpace* const space = space_info-&gt;space();
3125 
3126     const size_t beg_region =
3127       sd.addr_to_region_idx(sd.region_align_up(MAX2(space_info-&gt;new_top(), space-&gt;top())));
3128     const size_t end_region =
3129       sd.addr_to_region_idx(sd.region_align_down(space-&gt;end()));
3130 
3131     for (size_t cur = beg_region; cur &lt; end_region; ++cur) {
3132       ParCompactionManager::push_shadow_region(cur);
3133     }
3134   }
3135 
3136   size_t beg_region = sd.addr_to_region_idx(_space_info[old_space_id].dense_prefix());
3137   for (uint i = 0; i &lt; parallel_gc_threads; i++) {
3138     ParCompactionManager *cm = ParCompactionManager::manager_array(i);
3139     cm-&gt;set_next_shadow_region(beg_region + i);
3140   }
3141 }
3142 
3143 void PSParallelCompact::fill_blocks(size_t region_idx)
3144 {
3145   // Fill in the block table elements for the specified region.  Each block
3146   // table element holds the number of live words in the region that are to the
3147   // left of the first object that starts in the block.  Thus only blocks in
3148   // which an object starts need to be filled.
3149   //
3150   // The algorithm scans the section of the bitmap that corresponds to the
3151   // region, keeping a running total of the live words.  When an object start is
3152   // found, if it's the first to start in the block that contains it, the
3153   // current total is written to the block table element.
3154   const size_t Log2BlockSize = ParallelCompactData::Log2BlockSize;
3155   const size_t Log2RegionSize = ParallelCompactData::Log2RegionSize;
3156   const size_t RegionSize = ParallelCompactData::RegionSize;
3157 
3158   ParallelCompactData&amp; sd = summary_data();
3159   const size_t partial_obj_size = sd.region(region_idx)-&gt;partial_obj_size();
3160   if (partial_obj_size &gt;= RegionSize) {
3161     return; // No objects start in this region.
3162   }
3163 
3164   // Ensure the first loop iteration decides that the block has changed.
3165   size_t cur_block = sd.block_count();
3166 
3167   const ParMarkBitMap* const bitmap = mark_bitmap();
3168 
3169   const size_t Log2BitsPerBlock = Log2BlockSize - LogMinObjAlignment;
3170   assert((size_t)1 &lt;&lt; Log2BitsPerBlock ==
3171          bitmap-&gt;words_to_bits(ParallelCompactData::BlockSize), "sanity");
3172 
3173   size_t beg_bit = bitmap-&gt;words_to_bits(region_idx &lt;&lt; Log2RegionSize);
3174   const size_t range_end = beg_bit + bitmap-&gt;words_to_bits(RegionSize);
3175   size_t live_bits = bitmap-&gt;words_to_bits(partial_obj_size);
3176   beg_bit = bitmap-&gt;find_obj_beg(beg_bit + live_bits, range_end);
3177   while (beg_bit &lt; range_end) {
3178     const size_t new_block = beg_bit &gt;&gt; Log2BitsPerBlock;
3179     if (new_block != cur_block) {
3180       cur_block = new_block;
3181       sd.block(cur_block)-&gt;set_offset(bitmap-&gt;bits_to_words(live_bits));
3182     }
3183 
3184     const size_t end_bit = bitmap-&gt;find_obj_end(beg_bit, range_end);
3185     if (end_bit &lt; range_end - 1) {
3186       live_bits += end_bit - beg_bit + 1;
3187       beg_bit = bitmap-&gt;find_obj_beg(end_bit + 1, range_end);
3188     } else {
3189       return;
3190     }
3191   }
3192 }
3193 
3194 ParMarkBitMap::IterationStatus MoveAndUpdateClosure::copy_until_full()
3195 {
3196   if (source() != copy_destination()) {
3197     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3198     Copy::aligned_conjoint_words(source(), copy_destination(), words_remaining());
3199   }
3200   update_state(words_remaining());
3201   assert(is_full(), "sanity");
3202   return ParMarkBitMap::full;
3203 }
3204 
3205 void MoveAndUpdateClosure::copy_partial_obj()
3206 {
3207   size_t words = words_remaining();
3208 
3209   HeapWord* const range_end = MIN2(source() + words, bitmap()-&gt;region_end());
3210   HeapWord* const end_addr = bitmap()-&gt;find_obj_end(source(), range_end);
3211   if (end_addr &lt; range_end) {
3212     words = bitmap()-&gt;obj_size(source(), end_addr);
3213   }
3214 
3215   // This test is necessary; if omitted, the pointer updates to a partial object
3216   // that crosses the dense prefix boundary could be overwritten.
3217   if (source() != copy_destination()) {
3218     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3219     Copy::aligned_conjoint_words(source(), copy_destination(), words);
3220   }
3221   update_state(words);
3222 }
3223 
3224 void MoveAndUpdateClosure::complete_region(ParCompactionManager *cm, HeapWord *dest_addr,
3225                                            PSParallelCompact::RegionData *region_ptr) {
3226   assert(region_ptr-&gt;shadow_state() == ParallelCompactData::RegionData::NormalRegion, "Region should be finished");
3227   region_ptr-&gt;set_completed();
3228 }
3229 
3230 ParMarkBitMapClosure::IterationStatus
3231 MoveAndUpdateClosure::do_addr(HeapWord* addr, size_t words) {
3232   assert(destination() != NULL, "sanity");
3233   assert(bitmap()-&gt;obj_size(addr) == words, "bad size");
3234 
3235   _source = addr;
3236   assert(PSParallelCompact::summary_data().calc_new_pointer(source(), compaction_manager()) ==
3237          destination(), "wrong destination");
3238 
3239   if (words &gt; words_remaining()) {
3240     return ParMarkBitMap::would_overflow;
3241   }
3242 
3243   // The start_array must be updated even if the object is not moving.
3244   if (_start_array != NULL) {
3245     _start_array-&gt;allocate_block(destination());
3246   }
3247 
3248   if (copy_destination() != source()) {
3249     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3250     Copy::aligned_conjoint_words(source(), copy_destination(), words);
3251   }
3252 
3253   oop moved_oop = (oop) copy_destination();
3254   compaction_manager()-&gt;update_contents(moved_oop);
3255   assert(oopDesc::is_oop_or_null(moved_oop), "Expected an oop or NULL at " PTR_FORMAT, p2i(moved_oop));
3256 
3257   update_state(words);
3258   assert(copy_destination() == cast_from_oop&lt;HeapWord*&gt;(moved_oop) + moved_oop-&gt;size(), "sanity");
3259   return is_full() ? ParMarkBitMap::full : ParMarkBitMap::incomplete;
3260 }
3261 
3262 void MoveAndUpdateShadowClosure::complete_region(ParCompactionManager *cm, HeapWord *dest_addr,
3263                                                  PSParallelCompact::RegionData *region_ptr) {
3264   assert(region_ptr-&gt;shadow_state() == ParallelCompactData::RegionData::ShadowRegion, "Region should be shadow");
3265   // Record the shadow region index
3266   region_ptr-&gt;set_shadow_region(_shadow);
3267   // Mark the shadow region as filled to indicate the data is ready to be
3268   // copied back
3269   region_ptr-&gt;mark_filled();
3270   // Try to copy the content of the shadow region back to its corresponding
3271   // heap region if available; the GC thread that decreases the destination
3272   // count to zero will do the copying otherwise (see
3273   // PSParallelCompact::decrement_destination_counts).
3274   if (((region_ptr-&gt;available() &amp;&amp; region_ptr-&gt;claim()) || region_ptr-&gt;claimed()) &amp;&amp; region_ptr-&gt;mark_copied()) {
3275     region_ptr-&gt;set_completed();
3276     PSParallelCompact::copy_back(PSParallelCompact::summary_data().region_to_addr(_shadow), dest_addr);
3277     ParCompactionManager::push_shadow_region_mt_safe(_shadow);
3278   }
3279 }
3280 
3281 UpdateOnlyClosure::UpdateOnlyClosure(ParMarkBitMap* mbm,
3282                                      ParCompactionManager* cm,
3283                                      PSParallelCompact::SpaceId space_id) :
3284   ParMarkBitMapClosure(mbm, cm),
3285   _space_id(space_id),
3286   _start_array(PSParallelCompact::start_array(space_id))
3287 {
3288 }
3289 
3290 // Updates the references in the object to their new values.
3291 ParMarkBitMapClosure::IterationStatus
3292 UpdateOnlyClosure::do_addr(HeapWord* addr, size_t words) {
3293   do_addr(addr);
3294   return ParMarkBitMap::incomplete;
3295 }
3296 
3297 FillClosure::FillClosure(ParCompactionManager* cm, PSParallelCompact::SpaceId space_id) :
3298   ParMarkBitMapClosure(PSParallelCompact::mark_bitmap(), cm),
3299   _start_array(PSParallelCompact::start_array(space_id))
3300 {
3301   assert(space_id == PSParallelCompact::old_space_id,
3302          "cannot use FillClosure in the young gen");
3303 }
3304 
3305 ParMarkBitMapClosure::IterationStatus
3306 FillClosure::do_addr(HeapWord* addr, size_t size) {
3307   CollectedHeap::fill_with_objects(addr, size);
3308   HeapWord* const end = addr + size;
3309   do {
3310     _start_array-&gt;allocate_block(addr);
3311     addr += oop(addr)-&gt;size();
3312   } while (addr &lt; end);
3313   return ParMarkBitMap::incomplete;
3314 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>

<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60818">60818</a> : imported patch jep387-all.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "aot/aotLoader.hpp"
  27 #include "classfile/classLoaderDataGraph.hpp"
  28 #include "classfile/javaClasses.inline.hpp"
  29 #include "classfile/stringTable.hpp"
  30 #include "classfile/symbolTable.hpp"
  31 #include "classfile/systemDictionary.hpp"
  32 #include "code/codeCache.hpp"
  33 #include "gc/parallel/parallelArguments.hpp"
  34 #include "gc/parallel/parallelScavengeHeap.inline.hpp"
  35 #include "gc/parallel/parMarkBitMap.inline.hpp"
  36 #include "gc/parallel/psAdaptiveSizePolicy.hpp"
  37 #include "gc/parallel/psCompactionManager.inline.hpp"
  38 #include "gc/parallel/psOldGen.hpp"
  39 #include "gc/parallel/psParallelCompact.inline.hpp"
  40 #include "gc/parallel/psPromotionManager.inline.hpp"
  41 #include "gc/parallel/psRootType.hpp"
  42 #include "gc/parallel/psScavenge.hpp"
  43 #include "gc/parallel/psYoungGen.hpp"
  44 #include "gc/shared/gcCause.hpp"
  45 #include "gc/shared/gcHeapSummary.hpp"
  46 #include "gc/shared/gcId.hpp"
  47 #include "gc/shared/gcLocker.hpp"
  48 #include "gc/shared/gcTimer.hpp"
  49 #include "gc/shared/gcTrace.hpp"
  50 #include "gc/shared/gcTraceTime.inline.hpp"
  51 #include "gc/shared/isGCActiveMark.hpp"
  52 #include "gc/shared/oopStorage.inline.hpp"
  53 #include "gc/shared/oopStorageSet.inline.hpp"
  54 #include "gc/shared/oopStorageSetParState.inline.hpp"
  55 #include "gc/shared/referencePolicy.hpp"
  56 #include "gc/shared/referenceProcessor.hpp"
  57 #include "gc/shared/referenceProcessorPhaseTimes.hpp"
  58 #include "gc/shared/spaceDecorator.inline.hpp"
  59 #include "gc/shared/taskTerminator.hpp"
  60 #include "gc/shared/weakProcessor.hpp"
  61 #include "gc/shared/workerPolicy.hpp"
  62 #include "gc/shared/workgroup.hpp"
  63 #include "logging/log.hpp"
  64 #include "memory/iterator.inline.hpp"
  65 #include "memory/resourceArea.hpp"
  66 #include "memory/universe.hpp"
  67 #include "oops/access.inline.hpp"
  68 #include "oops/instanceClassLoaderKlass.inline.hpp"
  69 #include "oops/instanceKlass.inline.hpp"
  70 #include "oops/instanceMirrorKlass.inline.hpp"
  71 #include "oops/methodData.hpp"
  72 #include "oops/objArrayKlass.inline.hpp"
  73 #include "oops/oop.inline.hpp"
  74 #include "runtime/atomic.hpp"
  75 #include "runtime/handles.inline.hpp"
  76 #include "runtime/java.hpp"
  77 #include "runtime/safepoint.hpp"
  78 #include "runtime/vmThread.hpp"
  79 #include "services/memTracker.hpp"
  80 #include "services/memoryService.hpp"
  81 #include "utilities/align.hpp"
  82 #include "utilities/debug.hpp"
  83 #include "utilities/events.hpp"
  84 #include "utilities/formatBuffer.hpp"
  85 #include "utilities/macros.hpp"
  86 #include "utilities/stack.inline.hpp"
  87 #if INCLUDE_JVMCI
  88 #include "jvmci/jvmci.hpp"
  89 #endif
  90 
  91 #include &lt;math.h&gt;
  92 
  93 // All sizes are in HeapWords.
  94 const size_t ParallelCompactData::Log2RegionSize  = 16; // 64K words
  95 const size_t ParallelCompactData::RegionSize      = (size_t)1 &lt;&lt; Log2RegionSize;
  96 const size_t ParallelCompactData::RegionSizeBytes =
  97   RegionSize &lt;&lt; LogHeapWordSize;
  98 const size_t ParallelCompactData::RegionSizeOffsetMask = RegionSize - 1;
  99 const size_t ParallelCompactData::RegionAddrOffsetMask = RegionSizeBytes - 1;
 100 const size_t ParallelCompactData::RegionAddrMask       = ~RegionAddrOffsetMask;
 101 
 102 const size_t ParallelCompactData::Log2BlockSize   = 7; // 128 words
 103 const size_t ParallelCompactData::BlockSize       = (size_t)1 &lt;&lt; Log2BlockSize;
 104 const size_t ParallelCompactData::BlockSizeBytes  =
 105   BlockSize &lt;&lt; LogHeapWordSize;
 106 const size_t ParallelCompactData::BlockSizeOffsetMask = BlockSize - 1;
 107 const size_t ParallelCompactData::BlockAddrOffsetMask = BlockSizeBytes - 1;
 108 const size_t ParallelCompactData::BlockAddrMask       = ~BlockAddrOffsetMask;
 109 
 110 const size_t ParallelCompactData::BlocksPerRegion = RegionSize / BlockSize;
 111 const size_t ParallelCompactData::Log2BlocksPerRegion =
 112   Log2RegionSize - Log2BlockSize;
 113 
 114 const ParallelCompactData::RegionData::region_sz_t
 115 ParallelCompactData::RegionData::dc_shift = 27;
 116 
 117 const ParallelCompactData::RegionData::region_sz_t
 118 ParallelCompactData::RegionData::dc_mask = ~0U &lt;&lt; dc_shift;
 119 
 120 const ParallelCompactData::RegionData::region_sz_t
 121 ParallelCompactData::RegionData::dc_one = 0x1U &lt;&lt; dc_shift;
 122 
 123 const ParallelCompactData::RegionData::region_sz_t
 124 ParallelCompactData::RegionData::los_mask = ~dc_mask;
 125 
 126 const ParallelCompactData::RegionData::region_sz_t
 127 ParallelCompactData::RegionData::dc_claimed = 0x8U &lt;&lt; dc_shift;
 128 
 129 const ParallelCompactData::RegionData::region_sz_t
 130 ParallelCompactData::RegionData::dc_completed = 0xcU &lt;&lt; dc_shift;
 131 
 132 SpaceInfo PSParallelCompact::_space_info[PSParallelCompact::last_space_id];
 133 
 134 SpanSubjectToDiscoveryClosure PSParallelCompact::_span_based_discoverer;
 135 ReferenceProcessor* PSParallelCompact::_ref_processor = NULL;
 136 
 137 double PSParallelCompact::_dwl_mean;
 138 double PSParallelCompact::_dwl_std_dev;
 139 double PSParallelCompact::_dwl_first_term;
 140 double PSParallelCompact::_dwl_adjustment;
 141 #ifdef  ASSERT
 142 bool   PSParallelCompact::_dwl_initialized = false;
 143 #endif  // #ifdef ASSERT
 144 
 145 void SplitInfo::record(size_t src_region_idx, size_t partial_obj_size,
 146                        HeapWord* destination)
 147 {
 148   assert(src_region_idx != 0, "invalid src_region_idx");
 149   assert(partial_obj_size != 0, "invalid partial_obj_size argument");
 150   assert(destination != NULL, "invalid destination argument");
 151 
 152   _src_region_idx = src_region_idx;
 153   _partial_obj_size = partial_obj_size;
 154   _destination = destination;
 155 
 156   // These fields may not be updated below, so make sure they're clear.
 157   assert(_dest_region_addr == NULL, "should have been cleared");
 158   assert(_first_src_addr == NULL, "should have been cleared");
 159 
 160   // Determine the number of destination regions for the partial object.
 161   HeapWord* const last_word = destination + partial_obj_size - 1;
 162   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
 163   HeapWord* const beg_region_addr = sd.region_align_down(destination);
 164   HeapWord* const end_region_addr = sd.region_align_down(last_word);
 165 
 166   if (beg_region_addr == end_region_addr) {
 167     // One destination region.
 168     _destination_count = 1;
 169     if (end_region_addr == destination) {
 170       // The destination falls on a region boundary, thus the first word of the
 171       // partial object will be the first word copied to the destination region.
 172       _dest_region_addr = end_region_addr;
 173       _first_src_addr = sd.region_to_addr(src_region_idx);
 174     }
 175   } else {
 176     // Two destination regions.  When copied, the partial object will cross a
 177     // destination region boundary, so a word somewhere within the partial
 178     // object will be the first word copied to the second destination region.
 179     _destination_count = 2;
 180     _dest_region_addr = end_region_addr;
 181     const size_t ofs = pointer_delta(end_region_addr, destination);
 182     assert(ofs &lt; _partial_obj_size, "sanity");
 183     _first_src_addr = sd.region_to_addr(src_region_idx) + ofs;
 184   }
 185 }
 186 
 187 void SplitInfo::clear()
 188 {
 189   _src_region_idx = 0;
 190   _partial_obj_size = 0;
 191   _destination = NULL;
 192   _destination_count = 0;
 193   _dest_region_addr = NULL;
 194   _first_src_addr = NULL;
 195   assert(!is_valid(), "sanity");
 196 }
 197 
 198 #ifdef  ASSERT
 199 void SplitInfo::verify_clear()
 200 {
 201   assert(_src_region_idx == 0, "not clear");
 202   assert(_partial_obj_size == 0, "not clear");
 203   assert(_destination == NULL, "not clear");
 204   assert(_destination_count == 0, "not clear");
 205   assert(_dest_region_addr == NULL, "not clear");
 206   assert(_first_src_addr == NULL, "not clear");
 207 }
 208 #endif  // #ifdef ASSERT
 209 
 210 
 211 void PSParallelCompact::print_on_error(outputStream* st) {
 212   _mark_bitmap.print_on_error(st);
 213 }
 214 
 215 #ifndef PRODUCT
 216 const char* PSParallelCompact::space_names[] = {
 217   "old ", "eden", "from", "to  "
 218 };
 219 
 220 void PSParallelCompact::print_region_ranges() {
 221   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 222     return;
 223   }
 224   Log(gc, compaction) log;
 225   ResourceMark rm;
 226   LogStream ls(log.trace());
 227   Universe::print_on(&amp;ls);
 228   log.trace("space  bottom     top        end        new_top");
 229   log.trace("------ ---------- ---------- ---------- ----------");
 230 
 231   for (unsigned int id = 0; id &lt; last_space_id; ++id) {
 232     const MutableSpace* space = _space_info[id].space();
 233     log.trace("%u %s "
 234               SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10) " "
 235               SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10) " ",
 236               id, space_names[id],
 237               summary_data().addr_to_region_idx(space-&gt;bottom()),
 238               summary_data().addr_to_region_idx(space-&gt;top()),
 239               summary_data().addr_to_region_idx(space-&gt;end()),
 240               summary_data().addr_to_region_idx(_space_info[id].new_top()));
 241   }
 242 }
 243 
 244 void
 245 print_generic_summary_region(size_t i, const ParallelCompactData::RegionData* c)
 246 {
 247 #define REGION_IDX_FORMAT        SIZE_FORMAT_W(7)
 248 #define REGION_DATA_FORMAT       SIZE_FORMAT_W(5)
 249 
 250   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
 251   size_t dci = c-&gt;destination() ? sd.addr_to_region_idx(c-&gt;destination()) : 0;
 252   log_develop_trace(gc, compaction)(
 253       REGION_IDX_FORMAT " " PTR_FORMAT " "
 254       REGION_IDX_FORMAT " " PTR_FORMAT " "
 255       REGION_DATA_FORMAT " " REGION_DATA_FORMAT " "
 256       REGION_DATA_FORMAT " " REGION_IDX_FORMAT " %d",
 257       i, p2i(c-&gt;data_location()), dci, p2i(c-&gt;destination()),
 258       c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 259       c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 260 
 261 #undef  REGION_IDX_FORMAT
 262 #undef  REGION_DATA_FORMAT
 263 }
 264 
 265 void
 266 print_generic_summary_data(ParallelCompactData&amp; summary_data,
 267                            HeapWord* const beg_addr,
 268                            HeapWord* const end_addr)
 269 {
 270   size_t total_words = 0;
 271   size_t i = summary_data.addr_to_region_idx(beg_addr);
 272   const size_t last = summary_data.addr_to_region_idx(end_addr);
 273   HeapWord* pdest = 0;
 274 
 275   while (i &lt; last) {
 276     ParallelCompactData::RegionData* c = summary_data.region(i);
 277     if (c-&gt;data_size() != 0 || c-&gt;destination() != pdest) {
 278       print_generic_summary_region(i, c);
 279       total_words += c-&gt;data_size();
 280       pdest = c-&gt;destination();
 281     }
 282     ++i;
 283   }
 284 
 285   log_develop_trace(gc, compaction)("summary_data_bytes=" SIZE_FORMAT, total_words * HeapWordSize);
 286 }
 287 
 288 void
 289 PSParallelCompact::print_generic_summary_data(ParallelCompactData&amp; summary_data,
 290                                               HeapWord* const beg_addr,
 291                                               HeapWord* const end_addr) {
 292   ::print_generic_summary_data(summary_data,beg_addr, end_addr);
 293 }
 294 
 295 void
 296 print_generic_summary_data(ParallelCompactData&amp; summary_data,
 297                            SpaceInfo* space_info)
 298 {
 299   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 300     return;
 301   }
 302 
 303   for (unsigned int id = 0; id &lt; PSParallelCompact::last_space_id; ++id) {
 304     const MutableSpace* space = space_info[id].space();
 305     print_generic_summary_data(summary_data, space-&gt;bottom(),
 306                                MAX2(space-&gt;top(), space_info[id].new_top()));
 307   }
 308 }
 309 
 310 void
 311 print_initial_summary_data(ParallelCompactData&amp; summary_data,
 312                            const MutableSpace* space) {
 313   if (space-&gt;top() == space-&gt;bottom()) {
 314     return;
 315   }
 316 
 317   const size_t region_size = ParallelCompactData::RegionSize;
 318   typedef ParallelCompactData::RegionData RegionData;
 319   HeapWord* const top_aligned_up = summary_data.region_align_up(space-&gt;top());
 320   const size_t end_region = summary_data.addr_to_region_idx(top_aligned_up);
 321   const RegionData* c = summary_data.region(end_region - 1);
 322   HeapWord* end_addr = c-&gt;destination() + c-&gt;data_size();
 323   const size_t live_in_space = pointer_delta(end_addr, space-&gt;bottom());
 324 
 325   // Print (and count) the full regions at the beginning of the space.
 326   size_t full_region_count = 0;
 327   size_t i = summary_data.addr_to_region_idx(space-&gt;bottom());
 328   while (i &lt; end_region &amp;&amp; summary_data.region(i)-&gt;data_size() == region_size) {
 329     ParallelCompactData::RegionData* c = summary_data.region(i);
 330     log_develop_trace(gc, compaction)(
 331         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d",
 332         i, p2i(c-&gt;destination()),
 333         c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 334         c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 335     ++full_region_count;
 336     ++i;
 337   }
 338 
 339   size_t live_to_right = live_in_space - full_region_count * region_size;
 340 
 341   double max_reclaimed_ratio = 0.0;
 342   size_t max_reclaimed_ratio_region = 0;
 343   size_t max_dead_to_right = 0;
 344   size_t max_live_to_right = 0;
 345 
 346   // Print the 'reclaimed ratio' for regions while there is something live in
 347   // the region or to the right of it.  The remaining regions are empty (and
 348   // uninteresting), and computing the ratio will result in division by 0.
 349   while (i &lt; end_region &amp;&amp; live_to_right &gt; 0) {
 350     c = summary_data.region(i);
 351     HeapWord* const region_addr = summary_data.region_to_addr(i);
 352     const size_t used_to_right = pointer_delta(space-&gt;top(), region_addr);
 353     const size_t dead_to_right = used_to_right - live_to_right;
 354     const double reclaimed_ratio = double(dead_to_right) / live_to_right;
 355 
 356     if (reclaimed_ratio &gt; max_reclaimed_ratio) {
 357             max_reclaimed_ratio = reclaimed_ratio;
 358             max_reclaimed_ratio_region = i;
 359             max_dead_to_right = dead_to_right;
 360             max_live_to_right = live_to_right;
 361     }
 362 
 363     ParallelCompactData::RegionData* c = summary_data.region(i);
 364     log_develop_trace(gc, compaction)(
 365         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d"
 366         "%12.10f " SIZE_FORMAT_W(10) " " SIZE_FORMAT_W(10),
 367         i, p2i(c-&gt;destination()),
 368         c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 369         c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count(),
 370         reclaimed_ratio, dead_to_right, live_to_right);
 371 
 372 
 373     live_to_right -= c-&gt;data_size();
 374     ++i;
 375   }
 376 
 377   // Any remaining regions are empty.  Print one more if there is one.
 378   if (i &lt; end_region) {
 379     ParallelCompactData::RegionData* c = summary_data.region(i);
 380     log_develop_trace(gc, compaction)(
 381         SIZE_FORMAT_W(5) " " PTR_FORMAT " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " " SIZE_FORMAT_W(5) " %d",
 382          i, p2i(c-&gt;destination()),
 383          c-&gt;partial_obj_size(), c-&gt;live_obj_size(),
 384          c-&gt;data_size(), c-&gt;source_region(), c-&gt;destination_count());
 385   }
 386 
 387   log_develop_trace(gc, compaction)("max:  " SIZE_FORMAT_W(4) " d2r=" SIZE_FORMAT_W(10) " l2r=" SIZE_FORMAT_W(10) " max_ratio=%14.12f",
 388                                     max_reclaimed_ratio_region, max_dead_to_right, max_live_to_right, max_reclaimed_ratio);
 389 }
 390 
 391 void
 392 print_initial_summary_data(ParallelCompactData&amp; summary_data,
 393                            SpaceInfo* space_info) {
 394   if (!log_develop_is_enabled(Trace, gc, compaction)) {
 395     return;
 396   }
 397 
 398   unsigned int id = PSParallelCompact::old_space_id;
 399   const MutableSpace* space;
 400   do {
 401     space = space_info[id].space();
 402     print_initial_summary_data(summary_data, space);
 403   } while (++id &lt; PSParallelCompact::eden_space_id);
 404 
 405   do {
 406     space = space_info[id].space();
 407     print_generic_summary_data(summary_data, space-&gt;bottom(), space-&gt;top());
 408   } while (++id &lt; PSParallelCompact::last_space_id);
 409 }
 410 #endif  // #ifndef PRODUCT
 411 
 412 #ifdef  ASSERT
 413 size_t add_obj_count;
 414 size_t add_obj_size;
 415 size_t mark_bitmap_count;
 416 size_t mark_bitmap_size;
 417 #endif  // #ifdef ASSERT
 418 
 419 ParallelCompactData::ParallelCompactData() :
 420   _region_start(NULL),
 421   DEBUG_ONLY(_region_end(NULL) COMMA)
 422   _region_vspace(NULL),
 423   _reserved_byte_size(0),
 424   _region_data(NULL),
 425   _region_count(0),
 426   _block_vspace(NULL),
 427   _block_data(NULL),
 428   _block_count(0) {}
 429 
 430 bool ParallelCompactData::initialize(MemRegion covered_region)
 431 {
 432   _region_start = covered_region.start();
 433   const size_t region_size = covered_region.word_size();
 434   DEBUG_ONLY(_region_end = _region_start + region_size;)
 435 
 436   assert(region_align_down(_region_start) == _region_start,
 437          "region start not aligned");
 438   assert((region_size &amp; RegionSizeOffsetMask) == 0,
 439          "region size not a multiple of RegionSize");
 440 
 441   bool result = initialize_region_data(region_size) &amp;&amp; initialize_block_data();
 442   return result;
 443 }
 444 
 445 PSVirtualSpace*
 446 ParallelCompactData::create_vspace(size_t count, size_t element_size)
 447 {
 448   const size_t raw_bytes = count * element_size;
 449   const size_t page_sz = os::page_size_for_region_aligned(raw_bytes, 10);
 450   const size_t granularity = os::vm_allocation_granularity();
 451   _reserved_byte_size = align_up(raw_bytes, MAX2(page_sz, granularity));
 452 
 453   const size_t rs_align = page_sz == (size_t) os::vm_page_size() ? 0 :
 454     MAX2(page_sz, granularity);
 455   ReservedSpace rs(_reserved_byte_size, rs_align, rs_align &gt; 0);
 456   os::trace_page_sizes("Parallel Compact Data", raw_bytes, raw_bytes, page_sz, rs.base(),
 457                        rs.size());
 458 
 459   MemTracker::record_virtual_memory_type((address)rs.base(), mtGC);
 460 
 461   PSVirtualSpace* vspace = new PSVirtualSpace(rs, page_sz);
 462   if (vspace != 0) {
 463     if (vspace-&gt;expand_by(_reserved_byte_size)) {
 464       return vspace;
 465     }
 466     delete vspace;
 467     // Release memory reserved in the space.
 468     rs.release();
 469   }
 470 
 471   return 0;
 472 }
 473 
 474 bool ParallelCompactData::initialize_region_data(size_t region_size)
 475 {
 476   const size_t count = (region_size + RegionSizeOffsetMask) &gt;&gt; Log2RegionSize;
 477   _region_vspace = create_vspace(count, sizeof(RegionData));
 478   if (_region_vspace != 0) {
 479     _region_data = (RegionData*)_region_vspace-&gt;reserved_low_addr();
 480     _region_count = count;
 481     return true;
 482   }
 483   return false;
 484 }
 485 
 486 bool ParallelCompactData::initialize_block_data()
 487 {
 488   assert(_region_count != 0, "region data must be initialized first");
 489   const size_t count = _region_count &lt;&lt; Log2BlocksPerRegion;
 490   _block_vspace = create_vspace(count, sizeof(BlockData));
 491   if (_block_vspace != 0) {
 492     _block_data = (BlockData*)_block_vspace-&gt;reserved_low_addr();
 493     _block_count = count;
 494     return true;
 495   }
 496   return false;
 497 }
 498 
 499 void ParallelCompactData::clear()
 500 {
 501   memset(_region_data, 0, _region_vspace-&gt;committed_size());
 502   memset(_block_data, 0, _block_vspace-&gt;committed_size());
 503 }
 504 
 505 void ParallelCompactData::clear_range(size_t beg_region, size_t end_region) {
 506   assert(beg_region &lt;= _region_count, "beg_region out of range");
 507   assert(end_region &lt;= _region_count, "end_region out of range");
 508   assert(RegionSize % BlockSize == 0, "RegionSize not a multiple of BlockSize");
 509 
 510   const size_t region_cnt = end_region - beg_region;
 511   memset(_region_data + beg_region, 0, region_cnt * sizeof(RegionData));
 512 
 513   const size_t beg_block = beg_region * BlocksPerRegion;
 514   const size_t block_cnt = region_cnt * BlocksPerRegion;
 515   memset(_block_data + beg_block, 0, block_cnt * sizeof(BlockData));
 516 }
 517 
 518 HeapWord* ParallelCompactData::partial_obj_end(size_t region_idx) const
 519 {
 520   const RegionData* cur_cp = region(region_idx);
 521   const RegionData* const end_cp = region(region_count() - 1);
 522 
 523   HeapWord* result = region_to_addr(region_idx);
 524   if (cur_cp &lt; end_cp) {
 525     do {
 526       result += cur_cp-&gt;partial_obj_size();
 527     } while (cur_cp-&gt;partial_obj_size() == RegionSize &amp;&amp; ++cur_cp &lt; end_cp);
 528   }
 529   return result;
 530 }
 531 
 532 void ParallelCompactData::add_obj(HeapWord* addr, size_t len)
 533 {
 534   const size_t obj_ofs = pointer_delta(addr, _region_start);
 535   const size_t beg_region = obj_ofs &gt;&gt; Log2RegionSize;
 536   const size_t end_region = (obj_ofs + len - 1) &gt;&gt; Log2RegionSize;
 537 
 538   DEBUG_ONLY(Atomic::inc(&amp;add_obj_count);)
 539   DEBUG_ONLY(Atomic::add(&amp;add_obj_size, len);)
 540 
 541   if (beg_region == end_region) {
 542     // All in one region.
 543     _region_data[beg_region].add_live_obj(len);
 544     return;
 545   }
 546 
 547   // First region.
 548   const size_t beg_ofs = region_offset(addr);
 549   _region_data[beg_region].add_live_obj(RegionSize - beg_ofs);
 550 
 551   Klass* klass = ((oop)addr)-&gt;klass();
 552   // Middle regions--completely spanned by this object.
 553   for (size_t region = beg_region + 1; region &lt; end_region; ++region) {
 554     _region_data[region].set_partial_obj_size(RegionSize);
 555     _region_data[region].set_partial_obj_addr(addr);
 556   }
 557 
 558   // Last region.
 559   const size_t end_ofs = region_offset(addr + len - 1);
 560   _region_data[end_region].set_partial_obj_size(end_ofs + 1);
 561   _region_data[end_region].set_partial_obj_addr(addr);
 562 }
 563 
 564 void
 565 ParallelCompactData::summarize_dense_prefix(HeapWord* beg, HeapWord* end)
 566 {
 567   assert(region_offset(beg) == 0, "not RegionSize aligned");
 568   assert(region_offset(end) == 0, "not RegionSize aligned");
 569 
 570   size_t cur_region = addr_to_region_idx(beg);
 571   const size_t end_region = addr_to_region_idx(end);
 572   HeapWord* addr = beg;
 573   while (cur_region &lt; end_region) {
 574     _region_data[cur_region].set_destination(addr);
 575     _region_data[cur_region].set_destination_count(0);
 576     _region_data[cur_region].set_source_region(cur_region);
 577     _region_data[cur_region].set_data_location(addr);
 578 
 579     // Update live_obj_size so the region appears completely full.
 580     size_t live_size = RegionSize - _region_data[cur_region].partial_obj_size();
 581     _region_data[cur_region].set_live_obj_size(live_size);
 582 
 583     ++cur_region;
 584     addr += RegionSize;
 585   }
 586 }
 587 
 588 // Find the point at which a space can be split and, if necessary, record the
 589 // split point.
 590 //
 591 // If the current src region (which overflowed the destination space) doesn't
 592 // have a partial object, the split point is at the beginning of the current src
 593 // region (an "easy" split, no extra bookkeeping required).
 594 //
 595 // If the current src region has a partial object, the split point is in the
 596 // region where that partial object starts (call it the split_region).  If
 597 // split_region has a partial object, then the split point is just after that
 598 // partial object (a "hard" split where we have to record the split data and
 599 // zero the partial_obj_size field).  With a "hard" split, we know that the
 600 // partial_obj ends within split_region because the partial object that caused
 601 // the overflow starts in split_region.  If split_region doesn't have a partial
 602 // obj, then the split is at the beginning of split_region (another "easy"
 603 // split).
 604 HeapWord*
 605 ParallelCompactData::summarize_split_space(size_t src_region,
 606                                            SplitInfo&amp; split_info,
 607                                            HeapWord* destination,
 608                                            HeapWord* target_end,
 609                                            HeapWord** target_next)
 610 {
 611   assert(destination &lt;= target_end, "sanity");
 612   assert(destination + _region_data[src_region].data_size() &gt; target_end,
 613     "region should not fit into target space");
 614   assert(is_region_aligned(target_end), "sanity");
 615 
 616   size_t split_region = src_region;
 617   HeapWord* split_destination = destination;
 618   size_t partial_obj_size = _region_data[src_region].partial_obj_size();
 619 
 620   if (destination + partial_obj_size &gt; target_end) {
 621     // The split point is just after the partial object (if any) in the
 622     // src_region that contains the start of the object that overflowed the
 623     // destination space.
 624     //
 625     // Find the start of the "overflow" object and set split_region to the
 626     // region containing it.
 627     HeapWord* const overflow_obj = _region_data[src_region].partial_obj_addr();
 628     split_region = addr_to_region_idx(overflow_obj);
 629 
 630     // Clear the source_region field of all destination regions whose first word
 631     // came from data after the split point (a non-null source_region field
 632     // implies a region must be filled).
 633     //
 634     // An alternative to the simple loop below:  clear during post_compact(),
 635     // which uses memcpy instead of individual stores, and is easy to
 636     // parallelize.  (The downside is that it clears the entire RegionData
 637     // object as opposed to just one field.)
 638     //
 639     // post_compact() would have to clear the summary data up to the highest
 640     // address that was written during the summary phase, which would be
 641     //
 642     //         max(top, max(new_top, clear_top))
 643     //
 644     // where clear_top is a new field in SpaceInfo.  Would have to set clear_top
 645     // to target_end.
 646     const RegionData* const sr = region(split_region);
 647     const size_t beg_idx =
 648       addr_to_region_idx(region_align_up(sr-&gt;destination() +
 649                                          sr-&gt;partial_obj_size()));
 650     const size_t end_idx = addr_to_region_idx(target_end);
 651 
 652     log_develop_trace(gc, compaction)("split:  clearing source_region field in [" SIZE_FORMAT ", " SIZE_FORMAT ")", beg_idx, end_idx);
 653     for (size_t idx = beg_idx; idx &lt; end_idx; ++idx) {
 654       _region_data[idx].set_source_region(0);
 655     }
 656 
 657     // Set split_destination and partial_obj_size to reflect the split region.
 658     split_destination = sr-&gt;destination();
 659     partial_obj_size = sr-&gt;partial_obj_size();
 660   }
 661 
 662   // The split is recorded only if a partial object extends onto the region.
 663   if (partial_obj_size != 0) {
 664     _region_data[split_region].set_partial_obj_size(0);
 665     split_info.record(split_region, partial_obj_size, split_destination);
 666   }
 667 
 668   // Setup the continuation addresses.
 669   *target_next = split_destination + partial_obj_size;
 670   HeapWord* const source_next = region_to_addr(split_region) + partial_obj_size;
 671 
 672   if (log_develop_is_enabled(Trace, gc, compaction)) {
 673     const char * split_type = partial_obj_size == 0 ? "easy" : "hard";
 674     log_develop_trace(gc, compaction)("%s split:  src=" PTR_FORMAT " src_c=" SIZE_FORMAT " pos=" SIZE_FORMAT,
 675                                       split_type, p2i(source_next), split_region, partial_obj_size);
 676     log_develop_trace(gc, compaction)("%s split:  dst=" PTR_FORMAT " dst_c=" SIZE_FORMAT " tn=" PTR_FORMAT,
 677                                       split_type, p2i(split_destination),
 678                                       addr_to_region_idx(split_destination),
 679                                       p2i(*target_next));
 680 
 681     if (partial_obj_size != 0) {
 682       HeapWord* const po_beg = split_info.destination();
 683       HeapWord* const po_end = po_beg + split_info.partial_obj_size();
 684       log_develop_trace(gc, compaction)("%s split:  po_beg=" PTR_FORMAT " " SIZE_FORMAT " po_end=" PTR_FORMAT " " SIZE_FORMAT,
 685                                         split_type,
 686                                         p2i(po_beg), addr_to_region_idx(po_beg),
 687                                         p2i(po_end), addr_to_region_idx(po_end));
 688     }
 689   }
 690 
 691   return source_next;
 692 }
 693 
 694 bool ParallelCompactData::summarize(SplitInfo&amp; split_info,
 695                                     HeapWord* source_beg, HeapWord* source_end,
 696                                     HeapWord** source_next,
 697                                     HeapWord* target_beg, HeapWord* target_end,
 698                                     HeapWord** target_next)
 699 {
 700   HeapWord* const source_next_val = source_next == NULL ? NULL : *source_next;
 701   log_develop_trace(gc, compaction)(
 702       "sb=" PTR_FORMAT " se=" PTR_FORMAT " sn=" PTR_FORMAT
 703       "tb=" PTR_FORMAT " te=" PTR_FORMAT " tn=" PTR_FORMAT,
 704       p2i(source_beg), p2i(source_end), p2i(source_next_val),
 705       p2i(target_beg), p2i(target_end), p2i(*target_next));
 706 
 707   size_t cur_region = addr_to_region_idx(source_beg);
 708   const size_t end_region = addr_to_region_idx(region_align_up(source_end));
 709 
 710   HeapWord *dest_addr = target_beg;
 711   while (cur_region &lt; end_region) {
 712     // The destination must be set even if the region has no data.
 713     _region_data[cur_region].set_destination(dest_addr);
 714 
 715     size_t words = _region_data[cur_region].data_size();
 716     if (words &gt; 0) {
 717       // If cur_region does not fit entirely into the target space, find a point
 718       // at which the source space can be 'split' so that part is copied to the
 719       // target space and the rest is copied elsewhere.
 720       if (dest_addr + words &gt; target_end) {
 721         assert(source_next != NULL, "source_next is NULL when splitting");
 722         *source_next = summarize_split_space(cur_region, split_info, dest_addr,
 723                                              target_end, target_next);
 724         return false;
 725       }
 726 
 727       // Compute the destination_count for cur_region, and if necessary, update
 728       // source_region for a destination region.  The source_region field is
 729       // updated if cur_region is the first (left-most) region to be copied to a
 730       // destination region.
 731       //
 732       // The destination_count calculation is a bit subtle.  A region that has
 733       // data that compacts into itself does not count itself as a destination.
 734       // This maintains the invariant that a zero count means the region is
 735       // available and can be claimed and then filled.
 736       uint destination_count = 0;
 737       if (split_info.is_split(cur_region)) {
 738         // The current region has been split:  the partial object will be copied
 739         // to one destination space and the remaining data will be copied to
 740         // another destination space.  Adjust the initial destination_count and,
 741         // if necessary, set the source_region field if the partial object will
 742         // cross a destination region boundary.
 743         destination_count = split_info.destination_count();
 744         if (destination_count == 2) {
 745           size_t dest_idx = addr_to_region_idx(split_info.dest_region_addr());
 746           _region_data[dest_idx].set_source_region(cur_region);
 747         }
 748       }
 749 
 750       HeapWord* const last_addr = dest_addr + words - 1;
 751       const size_t dest_region_1 = addr_to_region_idx(dest_addr);
 752       const size_t dest_region_2 = addr_to_region_idx(last_addr);
 753 
 754       // Initially assume that the destination regions will be the same and
 755       // adjust the value below if necessary.  Under this assumption, if
 756       // cur_region == dest_region_2, then cur_region will be compacted
 757       // completely into itself.
 758       destination_count += cur_region == dest_region_2 ? 0 : 1;
 759       if (dest_region_1 != dest_region_2) {
 760         // Destination regions differ; adjust destination_count.
 761         destination_count += 1;
 762         // Data from cur_region will be copied to the start of dest_region_2.
 763         _region_data[dest_region_2].set_source_region(cur_region);
 764       } else if (region_offset(dest_addr) == 0) {
 765         // Data from cur_region will be copied to the start of the destination
 766         // region.
 767         _region_data[dest_region_1].set_source_region(cur_region);
 768       }
 769 
 770       _region_data[cur_region].set_destination_count(destination_count);
 771       _region_data[cur_region].set_data_location(region_to_addr(cur_region));
 772       dest_addr += words;
 773     }
 774 
 775     ++cur_region;
 776   }
 777 
 778   *target_next = dest_addr;
 779   return true;
 780 }
 781 
 782 HeapWord* ParallelCompactData::calc_new_pointer(HeapWord* addr, ParCompactionManager* cm) {
 783   assert(addr != NULL, "Should detect NULL oop earlier");
 784   assert(ParallelScavengeHeap::heap()-&gt;is_in(addr), "not in heap");
 785   assert(PSParallelCompact::mark_bitmap()-&gt;is_marked(addr), "not marked");
 786 
 787   // Region covering the object.
 788   RegionData* const region_ptr = addr_to_region_ptr(addr);
 789   HeapWord* result = region_ptr-&gt;destination();
 790 
 791   // If the entire Region is live, the new location is region-&gt;destination + the
 792   // offset of the object within in the Region.
 793 
 794   // Run some performance tests to determine if this special case pays off.  It
 795   // is worth it for pointers into the dense prefix.  If the optimization to
 796   // avoid pointer updates in regions that only point to the dense prefix is
 797   // ever implemented, this should be revisited.
 798   if (region_ptr-&gt;data_size() == RegionSize) {
 799     result += region_offset(addr);
 800     return result;
 801   }
 802 
 803   // Otherwise, the new location is region-&gt;destination + block offset + the
 804   // number of live words in the Block that are (a) to the left of addr and (b)
 805   // due to objects that start in the Block.
 806 
 807   // Fill in the block table if necessary.  This is unsynchronized, so multiple
 808   // threads may fill the block table for a region (harmless, since it is
 809   // idempotent).
 810   if (!region_ptr-&gt;blocks_filled()) {
 811     PSParallelCompact::fill_blocks(addr_to_region_idx(addr));
 812     region_ptr-&gt;set_blocks_filled();
 813   }
 814 
 815   HeapWord* const search_start = block_align_down(addr);
 816   const size_t block_offset = addr_to_block_ptr(addr)-&gt;offset();
 817 
 818   const ParMarkBitMap* bitmap = PSParallelCompact::mark_bitmap();
 819   const size_t live = bitmap-&gt;live_words_in_range(cm, search_start, oop(addr));
 820   result += block_offset + live;
 821   DEBUG_ONLY(PSParallelCompact::check_new_location(addr, result));
 822   return result;
 823 }
 824 
 825 #ifdef ASSERT
 826 void ParallelCompactData::verify_clear(const PSVirtualSpace* vspace)
 827 {
 828   const size_t* const beg = (const size_t*)vspace-&gt;committed_low_addr();
 829   const size_t* const end = (const size_t*)vspace-&gt;committed_high_addr();
 830   for (const size_t* p = beg; p &lt; end; ++p) {
 831     assert(*p == 0, "not zero");
 832   }
 833 }
 834 
 835 void ParallelCompactData::verify_clear()
 836 {
 837   verify_clear(_region_vspace);
 838   verify_clear(_block_vspace);
 839 }
 840 #endif  // #ifdef ASSERT
 841 
 842 STWGCTimer          PSParallelCompact::_gc_timer;
 843 ParallelOldTracer   PSParallelCompact::_gc_tracer;
 844 elapsedTimer        PSParallelCompact::_accumulated_time;
 845 unsigned int        PSParallelCompact::_total_invocations = 0;
 846 unsigned int        PSParallelCompact::_maximum_compaction_gc_num = 0;
 847 CollectorCounters*  PSParallelCompact::_counters = NULL;
 848 ParMarkBitMap       PSParallelCompact::_mark_bitmap;
 849 ParallelCompactData PSParallelCompact::_summary_data;
 850 
 851 PSParallelCompact::IsAliveClosure PSParallelCompact::_is_alive_closure;
 852 
 853 bool PSParallelCompact::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()-&gt;is_marked(p); }
 854 
 855 class PCReferenceProcessor: public ReferenceProcessor {
 856 public:
 857   PCReferenceProcessor(
 858     BoolObjectClosure* is_subject_to_discovery,
 859     BoolObjectClosure* is_alive_non_header) :
 860       ReferenceProcessor(is_subject_to_discovery,
 861       ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1), // mt processing
 862       ParallelGCThreads,   // mt processing degree
 863       true,                // mt discovery
 864       ParallelGCThreads,   // mt discovery degree
 865       true,                // atomic_discovery
 866       is_alive_non_header) {
 867   }
 868 
 869   template&lt;typename T&gt; bool discover(oop obj, ReferenceType type) {
 870     T* referent_addr = (T*) java_lang_ref_Reference::referent_addr_raw(obj);
 871     T heap_oop = RawAccess&lt;&gt;::oop_load(referent_addr);
 872     oop referent = CompressedOops::decode_not_null(heap_oop);
 873     return PSParallelCompact::mark_bitmap()-&gt;is_unmarked(referent)
 874         &amp;&amp; ReferenceProcessor::discover_reference(obj, type);
 875   }
 876   virtual bool discover_reference(oop obj, ReferenceType type) {
 877     if (UseCompressedOops) {
 878       return discover&lt;narrowOop&gt;(obj, type);
 879     } else {
 880       return discover&lt;oop&gt;(obj, type);
 881     }
 882   }
 883 };
 884 
 885 void PSParallelCompact::post_initialize() {
 886   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 887   _span_based_discoverer.set_span(heap-&gt;reserved_region());
 888   _ref_processor =
 889     new PCReferenceProcessor(&amp;_span_based_discoverer,
 890                              &amp;_is_alive_closure); // non-header is alive closure
 891 
 892   _counters = new CollectorCounters("Parallel full collection pauses", 1);
 893 
 894   // Initialize static fields in ParCompactionManager.
 895   ParCompactionManager::initialize(mark_bitmap());
 896 }
 897 
 898 bool PSParallelCompact::initialize() {
 899   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 900   MemRegion mr = heap-&gt;reserved_region();
 901 
 902   // Was the old gen get allocated successfully?
 903   if (!heap-&gt;old_gen()-&gt;is_allocated()) {
 904     return false;
 905   }
 906 
 907   initialize_space_info();
 908   initialize_dead_wood_limiter();
 909 
 910   if (!_mark_bitmap.initialize(mr)) {
 911     vm_shutdown_during_initialization(
 912       err_msg("Unable to allocate " SIZE_FORMAT "KB bitmaps for parallel "
 913       "garbage collection for the requested " SIZE_FORMAT "KB heap.",
 914       _mark_bitmap.reserved_byte_size()/K, mr.byte_size()/K));
 915     return false;
 916   }
 917 
 918   if (!_summary_data.initialize(mr)) {
 919     vm_shutdown_during_initialization(
 920       err_msg("Unable to allocate " SIZE_FORMAT "KB card tables for parallel "
 921       "garbage collection for the requested " SIZE_FORMAT "KB heap.",
 922       _summary_data.reserved_byte_size()/K, mr.byte_size()/K));
 923     return false;
 924   }
 925 
 926   return true;
 927 }
 928 
 929 void PSParallelCompact::initialize_space_info()
 930 {
 931   memset(&amp;_space_info, 0, sizeof(_space_info));
 932 
 933   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 934   PSYoungGen* young_gen = heap-&gt;young_gen();
 935 
 936   _space_info[old_space_id].set_space(heap-&gt;old_gen()-&gt;object_space());
 937   _space_info[eden_space_id].set_space(young_gen-&gt;eden_space());
 938   _space_info[from_space_id].set_space(young_gen-&gt;from_space());
 939   _space_info[to_space_id].set_space(young_gen-&gt;to_space());
 940 
 941   _space_info[old_space_id].set_start_array(heap-&gt;old_gen()-&gt;start_array());
 942 }
 943 
 944 void PSParallelCompact::initialize_dead_wood_limiter()
 945 {
 946   const size_t max = 100;
 947   _dwl_mean = double(MIN2(ParallelOldDeadWoodLimiterMean, max)) / 100.0;
 948   _dwl_std_dev = double(MIN2(ParallelOldDeadWoodLimiterStdDev, max)) / 100.0;
 949   _dwl_first_term = 1.0 / (sqrt(2.0 * M_PI) * _dwl_std_dev);
 950   DEBUG_ONLY(_dwl_initialized = true;)
 951   _dwl_adjustment = normal_distribution(1.0);
 952 }
 953 
 954 void
 955 PSParallelCompact::clear_data_covering_space(SpaceId id)
 956 {
 957   // At this point, top is the value before GC, new_top() is the value that will
 958   // be set at the end of GC.  The marking bitmap is cleared to top; nothing
 959   // should be marked above top.  The summary data is cleared to the larger of
 960   // top &amp; new_top.
 961   MutableSpace* const space = _space_info[id].space();
 962   HeapWord* const bot = space-&gt;bottom();
 963   HeapWord* const top = space-&gt;top();
 964   HeapWord* const max_top = MAX2(top, _space_info[id].new_top());
 965 
 966   const idx_t beg_bit = _mark_bitmap.addr_to_bit(bot);
 967   const idx_t end_bit = _mark_bitmap.align_range_end(_mark_bitmap.addr_to_bit(top));
 968   _mark_bitmap.clear_range(beg_bit, end_bit);
 969 
 970   const size_t beg_region = _summary_data.addr_to_region_idx(bot);
 971   const size_t end_region =
 972     _summary_data.addr_to_region_idx(_summary_data.region_align_up(max_top));
 973   _summary_data.clear_range(beg_region, end_region);
 974 
 975   // Clear the data used to 'split' regions.
 976   SplitInfo&amp; split_info = _space_info[id].split_info();
 977   if (split_info.is_valid()) {
 978     split_info.clear();
 979   }
 980   DEBUG_ONLY(split_info.verify_clear();)
 981 }
 982 
 983 void PSParallelCompact::pre_compact()
 984 {
 985   // Update the from &amp; to space pointers in space_info, since they are swapped
 986   // at each young gen gc.  Do the update unconditionally (even though a
 987   // promotion failure does not swap spaces) because an unknown number of young
 988   // collections will have swapped the spaces an unknown number of times.
 989   GCTraceTime(Debug, gc, phases) tm("Pre Compact", &amp;_gc_timer);
 990   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
 991   _space_info[from_space_id].set_space(heap-&gt;young_gen()-&gt;from_space());
 992   _space_info[to_space_id].set_space(heap-&gt;young_gen()-&gt;to_space());
 993 
 994   DEBUG_ONLY(add_obj_count = add_obj_size = 0;)
 995   DEBUG_ONLY(mark_bitmap_count = mark_bitmap_size = 0;)
 996 
 997   // Increment the invocation count
 998   heap-&gt;increment_total_collections(true);
 999 
1000   // We need to track unique mark sweep invocations as well.
1001   _total_invocations++;
1002 
1003   heap-&gt;print_heap_before_gc();
1004   heap-&gt;trace_heap_before_gc(&amp;_gc_tracer);
1005 
1006   // Fill in TLABs
1007   heap-&gt;ensure_parsability(true);  // retire TLABs
1008 
1009   if (VerifyBeforeGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
1010     Universe::verify("Before GC");
1011   }
1012 
1013   // Verify object start arrays
1014   if (VerifyObjectStartArray &amp;&amp;
1015       VerifyBeforeGC) {
1016     heap-&gt;old_gen()-&gt;verify_object_start_array();
1017   }
1018 
1019   DEBUG_ONLY(mark_bitmap()-&gt;verify_clear();)
1020   DEBUG_ONLY(summary_data().verify_clear();)
1021 
1022   ParCompactionManager::reset_all_bitmap_query_caches();
1023 }
1024 
1025 void PSParallelCompact::post_compact()
1026 {
1027   GCTraceTime(Info, gc, phases) tm("Post Compact", &amp;_gc_timer);
1028   ParCompactionManager::remove_all_shadow_regions();
1029 
1030   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
1031     // Clear the marking bitmap, summary data and split info.
1032     clear_data_covering_space(SpaceId(id));
1033     // Update top().  Must be done after clearing the bitmap and summary data.
1034     _space_info[id].publish_new_top();
1035   }
1036 
1037   MutableSpace* const eden_space = _space_info[eden_space_id].space();
1038   MutableSpace* const from_space = _space_info[from_space_id].space();
1039   MutableSpace* const to_space   = _space_info[to_space_id].space();
1040 
1041   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1042   bool eden_empty = eden_space-&gt;is_empty();
1043 
1044   // Update heap occupancy information which is used as input to the soft ref
1045   // clearing policy at the next gc.
1046   Universe::update_heap_info_at_gc();
1047 
1048   bool young_gen_empty = eden_empty &amp;&amp; from_space-&gt;is_empty() &amp;&amp;
1049     to_space-&gt;is_empty();
1050 
1051   PSCardTable* ct = heap-&gt;card_table();
1052   MemRegion old_mr = heap-&gt;old_gen()-&gt;reserved();
1053   if (young_gen_empty) {
1054     ct-&gt;clear(MemRegion(old_mr.start(), old_mr.end()));
1055   } else {
1056     ct-&gt;invalidate(MemRegion(old_mr.start(), old_mr.end()));
1057   }
1058 
1059   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1060   ClassLoaderDataGraph::purge(/*at_safepoint*/true);
<a name="1" id="anc1"></a><span class="changed">1061   MetaspaceUtils::verify_metrics();</span>
1062 
1063   heap-&gt;prune_scavengable_nmethods();
1064 
1065 #if COMPILER2_OR_JVMCI
1066   DerivedPointerTable::update_pointers();
1067 #endif
1068 
1069   if (ZapUnusedHeapArea) {
1070     heap-&gt;gen_mangle_unused_area();
1071   }
1072 
1073   // Signal that we have completed a visit to all live objects.
1074   Universe::heap()-&gt;record_whole_heap_examined_timestamp();
1075 }
1076 
1077 HeapWord*
1078 PSParallelCompact::compute_dense_prefix_via_density(const SpaceId id,
1079                                                     bool maximum_compaction)
1080 {
1081   const size_t region_size = ParallelCompactData::RegionSize;
1082   const ParallelCompactData&amp; sd = summary_data();
1083 
1084   const MutableSpace* const space = _space_info[id].space();
1085   HeapWord* const top_aligned_up = sd.region_align_up(space-&gt;top());
1086   const RegionData* const beg_cp = sd.addr_to_region_ptr(space-&gt;bottom());
1087   const RegionData* const end_cp = sd.addr_to_region_ptr(top_aligned_up);
1088 
1089   // Skip full regions at the beginning of the space--they are necessarily part
1090   // of the dense prefix.
1091   size_t full_count = 0;
1092   const RegionData* cp;
1093   for (cp = beg_cp; cp &lt; end_cp &amp;&amp; cp-&gt;data_size() == region_size; ++cp) {
1094     ++full_count;
1095   }
1096 
1097   assert(total_invocations() &gt;= _maximum_compaction_gc_num, "sanity");
1098   const size_t gcs_since_max = total_invocations() - _maximum_compaction_gc_num;
1099   const bool interval_ended = gcs_since_max &gt; HeapMaximumCompactionInterval;
1100   if (maximum_compaction || cp == end_cp || interval_ended) {
1101     _maximum_compaction_gc_num = total_invocations();
1102     return sd.region_to_addr(cp);
1103   }
1104 
1105   HeapWord* const new_top = _space_info[id].new_top();
1106   const size_t space_live = pointer_delta(new_top, space-&gt;bottom());
1107   const size_t space_used = space-&gt;used_in_words();
1108   const size_t space_capacity = space-&gt;capacity_in_words();
1109 
1110   const double cur_density = double(space_live) / space_capacity;
1111   const double deadwood_density =
1112     (1.0 - cur_density) * (1.0 - cur_density) * cur_density * cur_density;
1113   const size_t deadwood_goal = size_t(space_capacity * deadwood_density);
1114 
1115   log_develop_debug(gc, compaction)(
1116       "cur_dens=%5.3f dw_dens=%5.3f dw_goal=" SIZE_FORMAT,
1117       cur_density, deadwood_density, deadwood_goal);
1118   log_develop_debug(gc, compaction)(
1119       "space_live=" SIZE_FORMAT " space_used=" SIZE_FORMAT " "
1120       "space_cap=" SIZE_FORMAT,
1121       space_live, space_used,
1122       space_capacity);
1123 
1124   // XXX - Use binary search?
1125   HeapWord* dense_prefix = sd.region_to_addr(cp);
1126   const RegionData* full_cp = cp;
1127   const RegionData* const top_cp = sd.addr_to_region_ptr(space-&gt;top() - 1);
1128   while (cp &lt; end_cp) {
1129     HeapWord* region_destination = cp-&gt;destination();
1130     const size_t cur_deadwood = pointer_delta(dense_prefix, region_destination);
1131 
1132     log_develop_trace(gc, compaction)(
1133         "c#=" SIZE_FORMAT_W(4) " dst=" PTR_FORMAT " "
1134         "dp=" PTR_FORMAT " cdw=" SIZE_FORMAT_W(8),
1135         sd.region(cp), p2i(region_destination),
1136         p2i(dense_prefix), cur_deadwood);
1137 
1138     if (cur_deadwood &gt;= deadwood_goal) {
1139       // Found the region that has the correct amount of deadwood to the left.
1140       // This typically occurs after crossing a fairly sparse set of regions, so
1141       // iterate backwards over those sparse regions, looking for the region
1142       // that has the lowest density of live objects 'to the right.'
1143       size_t space_to_left = sd.region(cp) * region_size;
1144       size_t live_to_left = space_to_left - cur_deadwood;
1145       size_t space_to_right = space_capacity - space_to_left;
1146       size_t live_to_right = space_live - live_to_left;
1147       double density_to_right = double(live_to_right) / space_to_right;
1148       while (cp &gt; full_cp) {
1149         --cp;
1150         const size_t prev_region_live_to_right = live_to_right -
1151           cp-&gt;data_size();
1152         const size_t prev_region_space_to_right = space_to_right + region_size;
1153         double prev_region_density_to_right =
1154           double(prev_region_live_to_right) / prev_region_space_to_right;
1155         if (density_to_right &lt;= prev_region_density_to_right) {
1156           return dense_prefix;
1157         }
1158 
1159         log_develop_trace(gc, compaction)(
1160             "backing up from c=" SIZE_FORMAT_W(4) " d2r=%10.8f "
1161             "pc_d2r=%10.8f",
1162             sd.region(cp), density_to_right,
1163             prev_region_density_to_right);
1164 
1165         dense_prefix -= region_size;
1166         live_to_right = prev_region_live_to_right;
1167         space_to_right = prev_region_space_to_right;
1168         density_to_right = prev_region_density_to_right;
1169       }
1170       return dense_prefix;
1171     }
1172 
1173     dense_prefix += region_size;
1174     ++cp;
1175   }
1176 
1177   return dense_prefix;
1178 }
1179 
1180 #ifndef PRODUCT
1181 void PSParallelCompact::print_dense_prefix_stats(const char* const algorithm,
1182                                                  const SpaceId id,
1183                                                  const bool maximum_compaction,
1184                                                  HeapWord* const addr)
1185 {
1186   const size_t region_idx = summary_data().addr_to_region_idx(addr);
1187   RegionData* const cp = summary_data().region(region_idx);
1188   const MutableSpace* const space = _space_info[id].space();
1189   HeapWord* const new_top = _space_info[id].new_top();
1190 
1191   const size_t space_live = pointer_delta(new_top, space-&gt;bottom());
1192   const size_t dead_to_left = pointer_delta(addr, cp-&gt;destination());
1193   const size_t space_cap = space-&gt;capacity_in_words();
1194   const double dead_to_left_pct = double(dead_to_left) / space_cap;
1195   const size_t live_to_right = new_top - cp-&gt;destination();
1196   const size_t dead_to_right = space-&gt;top() - addr - live_to_right;
1197 
1198   log_develop_debug(gc, compaction)(
1199       "%s=" PTR_FORMAT " dpc=" SIZE_FORMAT_W(5) " "
1200       "spl=" SIZE_FORMAT " "
1201       "d2l=" SIZE_FORMAT " d2l%%=%6.4f "
1202       "d2r=" SIZE_FORMAT " l2r=" SIZE_FORMAT " "
1203       "ratio=%10.8f",
1204       algorithm, p2i(addr), region_idx,
1205       space_live,
1206       dead_to_left, dead_to_left_pct,
1207       dead_to_right, live_to_right,
1208       double(dead_to_right) / live_to_right);
1209 }
1210 #endif  // #ifndef PRODUCT
1211 
1212 // Return a fraction indicating how much of the generation can be treated as
1213 // "dead wood" (i.e., not reclaimed).  The function uses a normal distribution
1214 // based on the density of live objects in the generation to determine a limit,
1215 // which is then adjusted so the return value is min_percent when the density is
1216 // 1.
1217 //
1218 // The following table shows some return values for a different values of the
1219 // standard deviation (ParallelOldDeadWoodLimiterStdDev); the mean is 0.5 and
1220 // min_percent is 1.
1221 //
1222 //                          fraction allowed as dead wood
1223 //         -----------------------------------------------------------------
1224 // density std_dev=70 std_dev=75 std_dev=80 std_dev=85 std_dev=90 std_dev=95
1225 // ------- ---------- ---------- ---------- ---------- ---------- ----------
1226 // 0.00000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000
1227 // 0.05000 0.03193096 0.02836880 0.02550828 0.02319280 0.02130337 0.01974941
1228 // 0.10000 0.05247504 0.04547452 0.03988045 0.03537016 0.03170171 0.02869272
1229 // 0.15000 0.07135702 0.06111390 0.05296419 0.04641639 0.04110601 0.03676066
1230 // 0.20000 0.08831616 0.07509618 0.06461766 0.05622444 0.04943437 0.04388975
1231 // 0.25000 0.10311208 0.08724696 0.07471205 0.06469760 0.05661313 0.05002313
1232 // 0.30000 0.11553050 0.09741183 0.08313394 0.07175114 0.06257797 0.05511132
1233 // 0.35000 0.12538832 0.10545958 0.08978741 0.07731366 0.06727491 0.05911289
1234 // 0.40000 0.13253818 0.11128511 0.09459590 0.08132834 0.07066107 0.06199500
1235 // 0.45000 0.13687208 0.11481163 0.09750361 0.08375387 0.07270534 0.06373386
1236 // 0.50000 0.13832410 0.11599237 0.09847664 0.08456518 0.07338887 0.06431510
1237 // 0.55000 0.13687208 0.11481163 0.09750361 0.08375387 0.07270534 0.06373386
1238 // 0.60000 0.13253818 0.11128511 0.09459590 0.08132834 0.07066107 0.06199500
1239 // 0.65000 0.12538832 0.10545958 0.08978741 0.07731366 0.06727491 0.05911289
1240 // 0.70000 0.11553050 0.09741183 0.08313394 0.07175114 0.06257797 0.05511132
1241 // 0.75000 0.10311208 0.08724696 0.07471205 0.06469760 0.05661313 0.05002313
1242 // 0.80000 0.08831616 0.07509618 0.06461766 0.05622444 0.04943437 0.04388975
1243 // 0.85000 0.07135702 0.06111390 0.05296419 0.04641639 0.04110601 0.03676066
1244 // 0.90000 0.05247504 0.04547452 0.03988045 0.03537016 0.03170171 0.02869272
1245 // 0.95000 0.03193096 0.02836880 0.02550828 0.02319280 0.02130337 0.01974941
1246 // 1.00000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000 0.01000000
1247 
1248 double PSParallelCompact::dead_wood_limiter(double density, size_t min_percent)
1249 {
1250   assert(_dwl_initialized, "uninitialized");
1251 
1252   // The raw limit is the value of the normal distribution at x = density.
1253   const double raw_limit = normal_distribution(density);
1254 
1255   // Adjust the raw limit so it becomes the minimum when the density is 1.
1256   //
1257   // First subtract the adjustment value (which is simply the precomputed value
1258   // normal_distribution(1.0)); this yields a value of 0 when the density is 1.
1259   // Then add the minimum value, so the minimum is returned when the density is
1260   // 1.  Finally, prevent negative values, which occur when the mean is not 0.5.
1261   const double min = double(min_percent) / 100.0;
1262   const double limit = raw_limit - _dwl_adjustment + min;
1263   return MAX2(limit, 0.0);
1264 }
1265 
1266 ParallelCompactData::RegionData*
1267 PSParallelCompact::first_dead_space_region(const RegionData* beg,
1268                                            const RegionData* end)
1269 {
1270   const size_t region_size = ParallelCompactData::RegionSize;
1271   ParallelCompactData&amp; sd = summary_data();
1272   size_t left = sd.region(beg);
1273   size_t right = end &gt; beg ? sd.region(end) - 1 : left;
1274 
1275   // Binary search.
1276   while (left &lt; right) {
1277     // Equivalent to (left + right) / 2, but does not overflow.
1278     const size_t middle = left + (right - left) / 2;
1279     RegionData* const middle_ptr = sd.region(middle);
1280     HeapWord* const dest = middle_ptr-&gt;destination();
1281     HeapWord* const addr = sd.region_to_addr(middle);
1282     assert(dest != NULL, "sanity");
1283     assert(dest &lt;= addr, "must move left");
1284 
1285     if (middle &gt; left &amp;&amp; dest &lt; addr) {
1286       right = middle - 1;
1287     } else if (middle &lt; right &amp;&amp; middle_ptr-&gt;data_size() == region_size) {
1288       left = middle + 1;
1289     } else {
1290       return middle_ptr;
1291     }
1292   }
1293   return sd.region(left);
1294 }
1295 
1296 ParallelCompactData::RegionData*
1297 PSParallelCompact::dead_wood_limit_region(const RegionData* beg,
1298                                           const RegionData* end,
1299                                           size_t dead_words)
1300 {
1301   ParallelCompactData&amp; sd = summary_data();
1302   size_t left = sd.region(beg);
1303   size_t right = end &gt; beg ? sd.region(end) - 1 : left;
1304 
1305   // Binary search.
1306   while (left &lt; right) {
1307     // Equivalent to (left + right) / 2, but does not overflow.
1308     const size_t middle = left + (right - left) / 2;
1309     RegionData* const middle_ptr = sd.region(middle);
1310     HeapWord* const dest = middle_ptr-&gt;destination();
1311     HeapWord* const addr = sd.region_to_addr(middle);
1312     assert(dest != NULL, "sanity");
1313     assert(dest &lt;= addr, "must move left");
1314 
1315     const size_t dead_to_left = pointer_delta(addr, dest);
1316     if (middle &gt; left &amp;&amp; dead_to_left &gt; dead_words) {
1317       right = middle - 1;
1318     } else if (middle &lt; right &amp;&amp; dead_to_left &lt; dead_words) {
1319       left = middle + 1;
1320     } else {
1321       return middle_ptr;
1322     }
1323   }
1324   return sd.region(left);
1325 }
1326 
1327 // The result is valid during the summary phase, after the initial summarization
1328 // of each space into itself, and before final summarization.
1329 inline double
1330 PSParallelCompact::reclaimed_ratio(const RegionData* const cp,
1331                                    HeapWord* const bottom,
1332                                    HeapWord* const top,
1333                                    HeapWord* const new_top)
1334 {
1335   ParallelCompactData&amp; sd = summary_data();
1336 
1337   assert(cp != NULL, "sanity");
1338   assert(bottom != NULL, "sanity");
1339   assert(top != NULL, "sanity");
1340   assert(new_top != NULL, "sanity");
1341   assert(top &gt;= new_top, "summary data problem?");
1342   assert(new_top &gt; bottom, "space is empty; should not be here");
1343   assert(new_top &gt;= cp-&gt;destination(), "sanity");
1344   assert(top &gt;= sd.region_to_addr(cp), "sanity");
1345 
1346   HeapWord* const destination = cp-&gt;destination();
1347   const size_t dense_prefix_live  = pointer_delta(destination, bottom);
1348   const size_t compacted_region_live = pointer_delta(new_top, destination);
1349   const size_t compacted_region_used = pointer_delta(top,
1350                                                      sd.region_to_addr(cp));
1351   const size_t reclaimable = compacted_region_used - compacted_region_live;
1352 
1353   const double divisor = dense_prefix_live + 1.25 * compacted_region_live;
1354   return double(reclaimable) / divisor;
1355 }
1356 
1357 // Return the address of the end of the dense prefix, a.k.a. the start of the
1358 // compacted region.  The address is always on a region boundary.
1359 //
1360 // Completely full regions at the left are skipped, since no compaction can
1361 // occur in those regions.  Then the maximum amount of dead wood to allow is
1362 // computed, based on the density (amount live / capacity) of the generation;
1363 // the region with approximately that amount of dead space to the left is
1364 // identified as the limit region.  Regions between the last completely full
1365 // region and the limit region are scanned and the one that has the best
1366 // (maximum) reclaimed_ratio() is selected.
1367 HeapWord*
1368 PSParallelCompact::compute_dense_prefix(const SpaceId id,
1369                                         bool maximum_compaction)
1370 {
1371   const size_t region_size = ParallelCompactData::RegionSize;
1372   const ParallelCompactData&amp; sd = summary_data();
1373 
1374   const MutableSpace* const space = _space_info[id].space();
1375   HeapWord* const top = space-&gt;top();
1376   HeapWord* const top_aligned_up = sd.region_align_up(top);
1377   HeapWord* const new_top = _space_info[id].new_top();
1378   HeapWord* const new_top_aligned_up = sd.region_align_up(new_top);
1379   HeapWord* const bottom = space-&gt;bottom();
1380   const RegionData* const beg_cp = sd.addr_to_region_ptr(bottom);
1381   const RegionData* const top_cp = sd.addr_to_region_ptr(top_aligned_up);
1382   const RegionData* const new_top_cp =
1383     sd.addr_to_region_ptr(new_top_aligned_up);
1384 
1385   // Skip full regions at the beginning of the space--they are necessarily part
1386   // of the dense prefix.
1387   const RegionData* const full_cp = first_dead_space_region(beg_cp, new_top_cp);
1388   assert(full_cp-&gt;destination() == sd.region_to_addr(full_cp) ||
1389          space-&gt;is_empty(), "no dead space allowed to the left");
1390   assert(full_cp-&gt;data_size() &lt; region_size || full_cp == new_top_cp - 1,
1391          "region must have dead space");
1392 
1393   // The gc number is saved whenever a maximum compaction is done, and used to
1394   // determine when the maximum compaction interval has expired.  This avoids
1395   // successive max compactions for different reasons.
1396   assert(total_invocations() &gt;= _maximum_compaction_gc_num, "sanity");
1397   const size_t gcs_since_max = total_invocations() - _maximum_compaction_gc_num;
1398   const bool interval_ended = gcs_since_max &gt; HeapMaximumCompactionInterval ||
1399     total_invocations() == HeapFirstMaximumCompactionCount;
1400   if (maximum_compaction || full_cp == top_cp || interval_ended) {
1401     _maximum_compaction_gc_num = total_invocations();
1402     return sd.region_to_addr(full_cp);
1403   }
1404 
1405   const size_t space_live = pointer_delta(new_top, bottom);
1406   const size_t space_used = space-&gt;used_in_words();
1407   const size_t space_capacity = space-&gt;capacity_in_words();
1408 
1409   const double density = double(space_live) / double(space_capacity);
1410   const size_t min_percent_free = MarkSweepDeadRatio;
1411   const double limiter = dead_wood_limiter(density, min_percent_free);
1412   const size_t dead_wood_max = space_used - space_live;
1413   const size_t dead_wood_limit = MIN2(size_t(space_capacity * limiter),
1414                                       dead_wood_max);
1415 
1416   log_develop_debug(gc, compaction)(
1417       "space_live=" SIZE_FORMAT " space_used=" SIZE_FORMAT " "
1418       "space_cap=" SIZE_FORMAT,
1419       space_live, space_used,
1420       space_capacity);
1421   log_develop_debug(gc, compaction)(
1422       "dead_wood_limiter(%6.4f, " SIZE_FORMAT ")=%6.4f "
1423       "dead_wood_max=" SIZE_FORMAT " dead_wood_limit=" SIZE_FORMAT,
1424       density, min_percent_free, limiter,
1425       dead_wood_max, dead_wood_limit);
1426 
1427   // Locate the region with the desired amount of dead space to the left.
1428   const RegionData* const limit_cp =
1429     dead_wood_limit_region(full_cp, top_cp, dead_wood_limit);
1430 
1431   // Scan from the first region with dead space to the limit region and find the
1432   // one with the best (largest) reclaimed ratio.
1433   double best_ratio = 0.0;
1434   const RegionData* best_cp = full_cp;
1435   for (const RegionData* cp = full_cp; cp &lt; limit_cp; ++cp) {
1436     double tmp_ratio = reclaimed_ratio(cp, bottom, top, new_top);
1437     if (tmp_ratio &gt; best_ratio) {
1438       best_cp = cp;
1439       best_ratio = tmp_ratio;
1440     }
1441   }
1442 
1443   return sd.region_to_addr(best_cp);
1444 }
1445 
1446 void PSParallelCompact::summarize_spaces_quick()
1447 {
1448   for (unsigned int i = 0; i &lt; last_space_id; ++i) {
1449     const MutableSpace* space = _space_info[i].space();
1450     HeapWord** nta = _space_info[i].new_top_addr();
1451     bool result = _summary_data.summarize(_space_info[i].split_info(),
1452                                           space-&gt;bottom(), space-&gt;top(), NULL,
1453                                           space-&gt;bottom(), space-&gt;end(), nta);
1454     assert(result, "space must fit into itself");
1455     _space_info[i].set_dense_prefix(space-&gt;bottom());
1456   }
1457 }
1458 
1459 void PSParallelCompact::fill_dense_prefix_end(SpaceId id)
1460 {
1461   HeapWord* const dense_prefix_end = dense_prefix(id);
1462   const RegionData* region = _summary_data.addr_to_region_ptr(dense_prefix_end);
1463   const idx_t dense_prefix_bit = _mark_bitmap.addr_to_bit(dense_prefix_end);
1464   if (dead_space_crosses_boundary(region, dense_prefix_bit)) {
1465     // Only enough dead space is filled so that any remaining dead space to the
1466     // left is larger than the minimum filler object.  (The remainder is filled
1467     // during the copy/update phase.)
1468     //
1469     // The size of the dead space to the right of the boundary is not a
1470     // concern, since compaction will be able to use whatever space is
1471     // available.
1472     //
1473     // Here '||' is the boundary, 'x' represents a don't care bit and a box
1474     // surrounds the space to be filled with an object.
1475     //
1476     // In the 32-bit VM, each bit represents two 32-bit words:
1477     //                              +---+
1478     // a) beg_bits:  ...  x   x   x | 0 | ||   0   x  x  ...
1479     //    end_bits:  ...  x   x   x | 0 | ||   0   x  x  ...
1480     //                              +---+
1481     //
1482     // In the 64-bit VM, each bit represents one 64-bit word:
1483     //                              +------------+
1484     // b) beg_bits:  ...  x   x   x | 0   ||   0 | x  x  ...
1485     //    end_bits:  ...  x   x   1 | 0   ||   0 | x  x  ...
1486     //                              +------------+
1487     //                          +-------+
1488     // c) beg_bits:  ...  x   x | 0   0 | ||   0   x  x  ...
1489     //    end_bits:  ...  x   1 | 0   0 | ||   0   x  x  ...
1490     //                          +-------+
1491     //                      +-----------+
1492     // d) beg_bits:  ...  x | 0   0   0 | ||   0   x  x  ...
1493     //    end_bits:  ...  1 | 0   0   0 | ||   0   x  x  ...
1494     //                      +-----------+
1495     //                          +-------+
1496     // e) beg_bits:  ...  0   0 | 0   0 | ||   0   x  x  ...
1497     //    end_bits:  ...  0   0 | 0   0 | ||   0   x  x  ...
1498     //                          +-------+
1499 
1500     // Initially assume case a, c or e will apply.
1501     size_t obj_len = CollectedHeap::min_fill_size();
1502     HeapWord* obj_beg = dense_prefix_end - obj_len;
1503 
1504 #ifdef  _LP64
1505     if (MinObjAlignment &gt; 1) { // object alignment &gt; heap word size
1506       // Cases a, c or e.
1507     } else if (_mark_bitmap.is_obj_end(dense_prefix_bit - 2)) {
1508       // Case b above.
1509       obj_beg = dense_prefix_end - 1;
1510     } else if (!_mark_bitmap.is_obj_end(dense_prefix_bit - 3) &amp;&amp;
1511                _mark_bitmap.is_obj_end(dense_prefix_bit - 4)) {
1512       // Case d above.
1513       obj_beg = dense_prefix_end - 3;
1514       obj_len = 3;
1515     }
1516 #endif  // #ifdef _LP64
1517 
1518     CollectedHeap::fill_with_object(obj_beg, obj_len);
1519     _mark_bitmap.mark_obj(obj_beg, obj_len);
1520     _summary_data.add_obj(obj_beg, obj_len);
1521     assert(start_array(id) != NULL, "sanity");
1522     start_array(id)-&gt;allocate_block(obj_beg);
1523   }
1524 }
1525 
1526 void
1527 PSParallelCompact::summarize_space(SpaceId id, bool maximum_compaction)
1528 {
1529   assert(id &lt; last_space_id, "id out of range");
1530   assert(_space_info[id].dense_prefix() == _space_info[id].space()-&gt;bottom(),
1531          "should have been reset in summarize_spaces_quick()");
1532 
1533   const MutableSpace* space = _space_info[id].space();
1534   if (_space_info[id].new_top() != space-&gt;bottom()) {
1535     HeapWord* dense_prefix_end = compute_dense_prefix(id, maximum_compaction);
1536     _space_info[id].set_dense_prefix(dense_prefix_end);
1537 
1538 #ifndef PRODUCT
1539     if (log_is_enabled(Debug, gc, compaction)) {
1540       print_dense_prefix_stats("ratio", id, maximum_compaction,
1541                                dense_prefix_end);
1542       HeapWord* addr = compute_dense_prefix_via_density(id, maximum_compaction);
1543       print_dense_prefix_stats("density", id, maximum_compaction, addr);
1544     }
1545 #endif  // #ifndef PRODUCT
1546 
1547     // Recompute the summary data, taking into account the dense prefix.  If
1548     // every last byte will be reclaimed, then the existing summary data which
1549     // compacts everything can be left in place.
1550     if (!maximum_compaction &amp;&amp; dense_prefix_end != space-&gt;bottom()) {
1551       // If dead space crosses the dense prefix boundary, it is (at least
1552       // partially) filled with a dummy object, marked live and added to the
1553       // summary data.  This simplifies the copy/update phase and must be done
1554       // before the final locations of objects are determined, to prevent
1555       // leaving a fragment of dead space that is too small to fill.
1556       fill_dense_prefix_end(id);
1557 
1558       // Compute the destination of each Region, and thus each object.
1559       _summary_data.summarize_dense_prefix(space-&gt;bottom(), dense_prefix_end);
1560       _summary_data.summarize(_space_info[id].split_info(),
1561                               dense_prefix_end, space-&gt;top(), NULL,
1562                               dense_prefix_end, space-&gt;end(),
1563                               _space_info[id].new_top_addr());
1564     }
1565   }
1566 
1567   if (log_develop_is_enabled(Trace, gc, compaction)) {
1568     const size_t region_size = ParallelCompactData::RegionSize;
1569     HeapWord* const dense_prefix_end = _space_info[id].dense_prefix();
1570     const size_t dp_region = _summary_data.addr_to_region_idx(dense_prefix_end);
1571     const size_t dp_words = pointer_delta(dense_prefix_end, space-&gt;bottom());
1572     HeapWord* const new_top = _space_info[id].new_top();
1573     const HeapWord* nt_aligned_up = _summary_data.region_align_up(new_top);
1574     const size_t cr_words = pointer_delta(nt_aligned_up, dense_prefix_end);
1575     log_develop_trace(gc, compaction)(
1576         "id=%d cap=" SIZE_FORMAT " dp=" PTR_FORMAT " "
1577         "dp_region=" SIZE_FORMAT " " "dp_count=" SIZE_FORMAT " "
1578         "cr_count=" SIZE_FORMAT " " "nt=" PTR_FORMAT,
1579         id, space-&gt;capacity_in_words(), p2i(dense_prefix_end),
1580         dp_region, dp_words / region_size,
1581         cr_words / region_size, p2i(new_top));
1582   }
1583 }
1584 
1585 #ifndef PRODUCT
1586 void PSParallelCompact::summary_phase_msg(SpaceId dst_space_id,
1587                                           HeapWord* dst_beg, HeapWord* dst_end,
1588                                           SpaceId src_space_id,
1589                                           HeapWord* src_beg, HeapWord* src_end)
1590 {
1591   log_develop_trace(gc, compaction)(
1592       "Summarizing %d [%s] into %d [%s]:  "
1593       "src=" PTR_FORMAT "-" PTR_FORMAT " "
1594       SIZE_FORMAT "-" SIZE_FORMAT " "
1595       "dst=" PTR_FORMAT "-" PTR_FORMAT " "
1596       SIZE_FORMAT "-" SIZE_FORMAT,
1597       src_space_id, space_names[src_space_id],
1598       dst_space_id, space_names[dst_space_id],
1599       p2i(src_beg), p2i(src_end),
1600       _summary_data.addr_to_region_idx(src_beg),
1601       _summary_data.addr_to_region_idx(src_end),
1602       p2i(dst_beg), p2i(dst_end),
1603       _summary_data.addr_to_region_idx(dst_beg),
1604       _summary_data.addr_to_region_idx(dst_end));
1605 }
1606 #endif  // #ifndef PRODUCT
1607 
1608 void PSParallelCompact::summary_phase(ParCompactionManager* cm,
1609                                       bool maximum_compaction)
1610 {
1611   GCTraceTime(Info, gc, phases) tm("Summary Phase", &amp;_gc_timer);
1612 
1613 #ifdef ASSERT
1614   log_develop_debug(gc, marking)(
1615       "add_obj_count=" SIZE_FORMAT " "
1616       "add_obj_bytes=" SIZE_FORMAT,
1617       add_obj_count,
1618       add_obj_size * HeapWordSize);
1619   log_develop_debug(gc, marking)(
1620       "mark_bitmap_count=" SIZE_FORMAT " "
1621       "mark_bitmap_bytes=" SIZE_FORMAT,
1622       mark_bitmap_count,
1623       mark_bitmap_size * HeapWordSize);
1624 #endif // ASSERT
1625 
1626   // Quick summarization of each space into itself, to see how much is live.
1627   summarize_spaces_quick();
1628 
1629   log_develop_trace(gc, compaction)("summary phase:  after summarizing each space to self");
1630   NOT_PRODUCT(print_region_ranges());
1631   NOT_PRODUCT(print_initial_summary_data(_summary_data, _space_info));
1632 
1633   // The amount of live data that will end up in old space (assuming it fits).
1634   size_t old_space_total_live = 0;
1635   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
1636     old_space_total_live += pointer_delta(_space_info[id].new_top(),
1637                                           _space_info[id].space()-&gt;bottom());
1638   }
1639 
1640   MutableSpace* const old_space = _space_info[old_space_id].space();
1641   const size_t old_capacity = old_space-&gt;capacity_in_words();
1642   if (old_space_total_live &gt; old_capacity) {
1643     // XXX - should also try to expand
1644     maximum_compaction = true;
1645   }
1646 
1647   // Old generations.
1648   summarize_space(old_space_id, maximum_compaction);
1649 
1650   // Summarize the remaining spaces in the young gen.  The initial target space
1651   // is the old gen.  If a space does not fit entirely into the target, then the
1652   // remainder is compacted into the space itself and that space becomes the new
1653   // target.
1654   SpaceId dst_space_id = old_space_id;
1655   HeapWord* dst_space_end = old_space-&gt;end();
1656   HeapWord** new_top_addr = _space_info[dst_space_id].new_top_addr();
1657   for (unsigned int id = eden_space_id; id &lt; last_space_id; ++id) {
1658     const MutableSpace* space = _space_info[id].space();
1659     const size_t live = pointer_delta(_space_info[id].new_top(),
1660                                       space-&gt;bottom());
1661     const size_t available = pointer_delta(dst_space_end, *new_top_addr);
1662 
1663     NOT_PRODUCT(summary_phase_msg(dst_space_id, *new_top_addr, dst_space_end,
1664                                   SpaceId(id), space-&gt;bottom(), space-&gt;top());)
1665     if (live &gt; 0 &amp;&amp; live &lt;= available) {
1666       // All the live data will fit.
1667       bool done = _summary_data.summarize(_space_info[id].split_info(),
1668                                           space-&gt;bottom(), space-&gt;top(),
1669                                           NULL,
1670                                           *new_top_addr, dst_space_end,
1671                                           new_top_addr);
1672       assert(done, "space must fit into old gen");
1673 
1674       // Reset the new_top value for the space.
1675       _space_info[id].set_new_top(space-&gt;bottom());
1676     } else if (live &gt; 0) {
1677       // Attempt to fit part of the source space into the target space.
1678       HeapWord* next_src_addr = NULL;
1679       bool done = _summary_data.summarize(_space_info[id].split_info(),
1680                                           space-&gt;bottom(), space-&gt;top(),
1681                                           &amp;next_src_addr,
1682                                           *new_top_addr, dst_space_end,
1683                                           new_top_addr);
1684       assert(!done, "space should not fit into old gen");
1685       assert(next_src_addr != NULL, "sanity");
1686 
1687       // The source space becomes the new target, so the remainder is compacted
1688       // within the space itself.
1689       dst_space_id = SpaceId(id);
1690       dst_space_end = space-&gt;end();
1691       new_top_addr = _space_info[id].new_top_addr();
1692       NOT_PRODUCT(summary_phase_msg(dst_space_id,
1693                                     space-&gt;bottom(), dst_space_end,
1694                                     SpaceId(id), next_src_addr, space-&gt;top());)
1695       done = _summary_data.summarize(_space_info[id].split_info(),
1696                                      next_src_addr, space-&gt;top(),
1697                                      NULL,
1698                                      space-&gt;bottom(), dst_space_end,
1699                                      new_top_addr);
1700       assert(done, "space must fit when compacted into itself");
1701       assert(*new_top_addr &lt;= space-&gt;top(), "usage should not grow");
1702     }
1703   }
1704 
1705   log_develop_trace(gc, compaction)("Summary_phase:  after final summarization");
1706   NOT_PRODUCT(print_region_ranges());
1707   NOT_PRODUCT(print_initial_summary_data(_summary_data, _space_info));
1708 }
1709 
1710 // This method should contain all heap-specific policy for invoking a full
1711 // collection.  invoke_no_policy() will only attempt to compact the heap; it
1712 // will do nothing further.  If we need to bail out for policy reasons, scavenge
1713 // before full gc, or any other specialized behavior, it needs to be added here.
1714 //
1715 // Note that this method should only be called from the vm_thread while at a
1716 // safepoint.
1717 //
1718 // Note that the all_soft_refs_clear flag in the soft ref policy
1719 // may be true because this method can be called without intervening
1720 // activity.  For example when the heap space is tight and full measure
1721 // are being taken to free space.
1722 void PSParallelCompact::invoke(bool maximum_heap_compaction) {
1723   assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
1724   assert(Thread::current() == (Thread*)VMThread::vm_thread(),
1725          "should be in vm thread");
1726 
1727   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1728   GCCause::Cause gc_cause = heap-&gt;gc_cause();
1729   assert(!heap-&gt;is_gc_active(), "not reentrant");
1730 
1731   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
1732   IsGCActiveMark mark;
1733 
1734   if (ScavengeBeforeFullGC) {
1735     PSScavenge::invoke_no_policy();
1736   }
1737 
1738   const bool clear_all_soft_refs =
1739     heap-&gt;soft_ref_policy()-&gt;should_clear_all_soft_refs();
1740 
1741   PSParallelCompact::invoke_no_policy(clear_all_soft_refs ||
1742                                       maximum_heap_compaction);
1743 }
1744 
1745 // This method contains no policy. You should probably
1746 // be calling invoke() instead.
1747 bool PSParallelCompact::invoke_no_policy(bool maximum_heap_compaction) {
1748   assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");
1749   assert(ref_processor() != NULL, "Sanity");
1750 
1751   if (GCLocker::check_active_before_gc()) {
1752     return false;
1753   }
1754 
1755   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
1756 
1757   GCIdMark gc_id_mark;
1758   _gc_timer.register_gc_start();
1759   _gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer.gc_start());
1760 
1761   TimeStamp marking_start;
1762   TimeStamp compaction_start;
1763   TimeStamp collection_exit;
1764 
1765   GCCause::Cause gc_cause = heap-&gt;gc_cause();
1766   PSYoungGen* young_gen = heap-&gt;young_gen();
1767   PSOldGen* old_gen = heap-&gt;old_gen();
1768   PSAdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
1769 
1770   // The scope of casr should end after code that can change
1771   // SoftRefPolicy::_should_clear_all_soft_refs.
1772   ClearedAllSoftRefs casr(maximum_heap_compaction,
1773                           heap-&gt;soft_ref_policy());
1774 
1775   if (ZapUnusedHeapArea) {
1776     // Save information needed to minimize mangling
1777     heap-&gt;record_gen_tops_before_GC();
1778   }
1779 
1780   // Make sure data structures are sane, make the heap parsable, and do other
1781   // miscellaneous bookkeeping.
1782   pre_compact();
1783 
1784   const PreGenGCValues pre_gc_values = heap-&gt;get_pre_gc_values();
1785 
1786   // Get the compaction manager reserved for the VM thread.
1787   ParCompactionManager* const vmthread_cm =
1788     ParCompactionManager::manager_array(ParallelScavengeHeap::heap()-&gt;workers().total_workers());
1789 
1790   {
1791     ResourceMark rm;
1792 
1793     const uint active_workers =
1794       WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()-&gt;workers().total_workers(),
1795                                         ParallelScavengeHeap::heap()-&gt;workers().active_workers(),
1796                                         Threads::number_of_non_daemon_threads());
1797     ParallelScavengeHeap::heap()-&gt;workers().update_active_workers(active_workers);
1798 
1799     GCTraceCPUTime tcpu;
1800     GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause, true);
1801 
1802     heap-&gt;pre_full_gc_dump(&amp;_gc_timer);
1803 
1804     TraceCollectorStats tcs(counters());
1805     TraceMemoryManagerStats tms(heap-&gt;old_gc_manager(), gc_cause);
1806 
1807     if (log_is_enabled(Debug, gc, heap, exit)) {
1808       accumulated_time()-&gt;start();
1809     }
1810 
1811     // Let the size policy know we're starting
1812     size_policy-&gt;major_collection_begin();
1813 
1814 #if COMPILER2_OR_JVMCI
1815     DerivedPointerTable::clear();
1816 #endif
1817 
1818     ref_processor()-&gt;enable_discovery();
1819     ref_processor()-&gt;setup_policy(maximum_heap_compaction);
1820 
1821     bool marked_for_unloading = false;
1822 
1823     marking_start.update();
1824     marking_phase(vmthread_cm, maximum_heap_compaction, &amp;_gc_tracer);
1825 
1826     bool max_on_system_gc = UseMaximumCompactionOnSystemGC
1827       &amp;&amp; GCCause::is_user_requested_gc(gc_cause);
1828     summary_phase(vmthread_cm, maximum_heap_compaction || max_on_system_gc);
1829 
1830 #if COMPILER2_OR_JVMCI
1831     assert(DerivedPointerTable::is_active(), "Sanity");
1832     DerivedPointerTable::set_active(false);
1833 #endif
1834 
1835     // adjust_roots() updates Universe::_intArrayKlassObj which is
1836     // needed by the compaction for filling holes in the dense prefix.
1837     adjust_roots(vmthread_cm);
1838 
1839     compaction_start.update();
1840     compact();
1841 
1842     // Reset the mark bitmap, summary data, and do other bookkeeping.  Must be
1843     // done before resizing.
1844     post_compact();
1845 
1846     // Let the size policy know we're done
1847     size_policy-&gt;major_collection_end(old_gen-&gt;used_in_bytes(), gc_cause);
1848 
1849     if (UseAdaptiveSizePolicy) {
1850       log_debug(gc, ergo)("AdaptiveSizeStart: collection: %d ", heap-&gt;total_collections());
1851       log_trace(gc, ergo)("old_gen_capacity: " SIZE_FORMAT " young_gen_capacity: " SIZE_FORMAT,
1852                           old_gen-&gt;capacity_in_bytes(), young_gen-&gt;capacity_in_bytes());
1853 
1854       // Don't check if the size_policy is ready here.  Let
1855       // the size_policy check that internally.
1856       if (UseAdaptiveGenerationSizePolicyAtMajorCollection &amp;&amp;
1857           AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {
1858         // Swap the survivor spaces if from_space is empty. The
1859         // resize_young_gen() called below is normally used after
1860         // a successful young GC and swapping of survivor spaces;
1861         // otherwise, it will fail to resize the young gen with
1862         // the current implementation.
1863         if (young_gen-&gt;from_space()-&gt;is_empty()) {
1864           young_gen-&gt;from_space()-&gt;clear(SpaceDecorator::Mangle);
1865           young_gen-&gt;swap_spaces();
1866         }
1867 
1868         // Calculate optimal free space amounts
1869         assert(young_gen-&gt;max_gen_size() &gt;
1870           young_gen-&gt;from_space()-&gt;capacity_in_bytes() +
1871           young_gen-&gt;to_space()-&gt;capacity_in_bytes(),
1872           "Sizes of space in young gen are out-of-bounds");
1873 
1874         size_t young_live = young_gen-&gt;used_in_bytes();
1875         size_t eden_live = young_gen-&gt;eden_space()-&gt;used_in_bytes();
1876         size_t old_live = old_gen-&gt;used_in_bytes();
1877         size_t cur_eden = young_gen-&gt;eden_space()-&gt;capacity_in_bytes();
1878         size_t max_old_gen_size = old_gen-&gt;max_gen_size();
1879         size_t max_eden_size = young_gen-&gt;max_gen_size() -
1880           young_gen-&gt;from_space()-&gt;capacity_in_bytes() -
1881           young_gen-&gt;to_space()-&gt;capacity_in_bytes();
1882 
1883         // Used for diagnostics
1884         size_policy-&gt;clear_generation_free_space_flags();
1885 
1886         size_policy-&gt;compute_generations_free_space(young_live,
1887                                                     eden_live,
1888                                                     old_live,
1889                                                     cur_eden,
1890                                                     max_old_gen_size,
1891                                                     max_eden_size,
1892                                                     true /* full gc*/);
1893 
1894         size_policy-&gt;check_gc_overhead_limit(eden_live,
1895                                              max_old_gen_size,
1896                                              max_eden_size,
1897                                              true /* full gc*/,
1898                                              gc_cause,
1899                                              heap-&gt;soft_ref_policy());
1900 
1901         size_policy-&gt;decay_supplemental_growth(true /* full gc*/);
1902 
1903         heap-&gt;resize_old_gen(
1904           size_policy-&gt;calculated_old_free_size_in_bytes());
1905 
1906         heap-&gt;resize_young_gen(size_policy-&gt;calculated_eden_size_in_bytes(),
1907                                size_policy-&gt;calculated_survivor_size_in_bytes());
1908       }
1909 
1910       log_debug(gc, ergo)("AdaptiveSizeStop: collection: %d ", heap-&gt;total_collections());
1911     }
1912 
1913     if (UsePerfData) {
1914       PSGCAdaptivePolicyCounters* const counters = heap-&gt;gc_policy_counters();
1915       counters-&gt;update_counters();
1916       counters-&gt;update_old_capacity(old_gen-&gt;capacity_in_bytes());
1917       counters-&gt;update_young_capacity(young_gen-&gt;capacity_in_bytes());
1918     }
1919 
1920     heap-&gt;resize_all_tlabs();
1921 
1922     // Resize the metaspace capacity after a collection
1923     MetaspaceGC::compute_new_size();
1924 
1925     if (log_is_enabled(Debug, gc, heap, exit)) {
1926       accumulated_time()-&gt;stop();
1927     }
1928 
1929     heap-&gt;print_heap_change(pre_gc_values);
1930 
1931     // Track memory usage and detect low memory
1932     MemoryService::track_memory_usage();
1933     heap-&gt;update_counters();
1934 
1935     heap-&gt;post_full_gc_dump(&amp;_gc_timer);
1936   }
1937 
1938 #ifdef ASSERT
1939   for (size_t i = 0; i &lt; ParallelGCThreads + 1; ++i) {
1940     ParCompactionManager* const cm =
1941       ParCompactionManager::manager_array(int(i));
1942     assert(cm-&gt;marking_stack()-&gt;is_empty(),       "should be empty");
1943     assert(cm-&gt;region_stack()-&gt;is_empty(), "Region stack " SIZE_FORMAT " is not empty", i);
1944   }
1945 #endif // ASSERT
1946 
1947   if (VerifyAfterGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
1948     Universe::verify("After GC");
1949   }
1950 
1951   // Re-verify object start arrays
1952   if (VerifyObjectStartArray &amp;&amp;
1953       VerifyAfterGC) {
1954     old_gen-&gt;verify_object_start_array();
1955   }
1956 
1957   if (ZapUnusedHeapArea) {
1958     old_gen-&gt;object_space()-&gt;check_mangled_unused_area_complete();
1959   }
1960 
1961   NOT_PRODUCT(ref_processor()-&gt;verify_no_references_recorded());
1962 
1963   collection_exit.update();
1964 
1965   heap-&gt;print_heap_after_gc();
1966   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
1967 
1968   log_debug(gc, task, time)("VM-Thread " JLONG_FORMAT " " JLONG_FORMAT " " JLONG_FORMAT,
1969                          marking_start.ticks(), compaction_start.ticks(),
1970                          collection_exit.ticks());
1971 
1972   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
1973 
1974   _gc_timer.register_gc_end();
1975 
1976   _gc_tracer.report_dense_prefix(dense_prefix(old_space_id));
1977   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
1978 
1979   return true;
1980 }
1981 
1982 class PCAddThreadRootsMarkingTaskClosure : public ThreadClosure {
1983 private:
1984   uint _worker_id;
1985 
1986 public:
1987   PCAddThreadRootsMarkingTaskClosure(uint worker_id) : _worker_id(worker_id) { }
1988   void do_thread(Thread* thread) {
1989     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
1990 
1991     ResourceMark rm;
1992 
1993     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(_worker_id);
1994 
1995     PCMarkAndPushClosure mark_and_push_closure(cm);
1996     MarkingCodeBlobClosure mark_and_push_in_blobs(&amp;mark_and_push_closure, !CodeBlobToOopClosure::FixRelocations);
1997 
1998     thread-&gt;oops_do(&amp;mark_and_push_closure, &amp;mark_and_push_in_blobs);
1999 
2000     // Do the real work
2001     cm-&gt;follow_marking_stacks();
2002   }
2003 };
2004 
2005 static void mark_from_roots_work(ParallelRootType::Value root_type, uint worker_id) {
2006   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2007 
2008   ParCompactionManager* cm =
2009     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2010   PCMarkAndPushClosure mark_and_push_closure(cm);
2011 
2012   switch (root_type) {
2013     case ParallelRootType::object_synchronizer:
2014       ObjectSynchronizer::oops_do(&amp;mark_and_push_closure);
2015       break;
2016 
2017     case ParallelRootType::class_loader_data:
2018       {
2019         CLDToOopClosure cld_closure(&amp;mark_and_push_closure, ClassLoaderData::_claim_strong);
2020         ClassLoaderDataGraph::always_strong_cld_do(&amp;cld_closure);
2021       }
2022       break;
2023 
2024     case ParallelRootType::code_cache:
2025       // Do not treat nmethods as strong roots for mark/sweep, since we can unload them.
2026       //ScavengableNMethods::scavengable_nmethods_do(CodeBlobToOopClosure(&amp;mark_and_push_closure));
2027       AOTLoader::oops_do(&amp;mark_and_push_closure);
2028       break;
2029 
2030     case ParallelRootType::sentinel:
2031     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
2032       fatal("Bad enumeration value: %u", root_type);
2033       break;
2034   }
2035 
2036   // Do the real work
2037   cm-&gt;follow_marking_stacks();
2038 }
2039 
2040 static void steal_marking_work(TaskTerminator&amp; terminator, uint worker_id) {
2041   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2042 
2043   ParCompactionManager* cm =
2044     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2045 
2046   oop obj = NULL;
2047   ObjArrayTask task;
2048   do {
2049     while (ParCompactionManager::steal_objarray(worker_id,  task)) {
2050       cm-&gt;follow_array((objArrayOop)task.obj(), task.index());
2051       cm-&gt;follow_marking_stacks();
2052     }
2053     while (ParCompactionManager::steal(worker_id, obj)) {
2054       cm-&gt;follow_contents(obj);
2055       cm-&gt;follow_marking_stacks();
2056     }
2057   } while (!terminator.offer_termination());
2058 }
2059 
2060 class MarkFromRootsTask : public AbstractGangTask {
2061   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2062   StrongRootsScope _strong_roots_scope; // needed for Threads::possibly_parallel_threads_do
2063   OopStorageSetStrongParState&lt;false /* concurrent */, false /* is_const */&gt; _oop_storage_set_par_state;
2064   SequentialSubTasksDone _subtasks;
2065   TaskTerminator _terminator;
2066   uint _active_workers;
2067 
2068 public:
2069   MarkFromRootsTask(uint active_workers) :
2070       AbstractGangTask("MarkFromRootsTask"),
2071       _strong_roots_scope(active_workers),
2072       _subtasks(),
2073       _terminator(active_workers, ParCompactionManager::oop_task_queues()),
2074       _active_workers(active_workers) {
2075     _subtasks.set_n_threads(active_workers);
2076     _subtasks.set_n_tasks(ParallelRootType::sentinel);
2077   }
2078 
2079   virtual void work(uint worker_id) {
2080     for (uint task = 0; _subtasks.try_claim_task(task); /*empty*/ ) {
2081       mark_from_roots_work(static_cast&lt;ParallelRootType::Value&gt;(task), worker_id);
2082     }
2083     _subtasks.all_tasks_completed();
2084 
2085     PCAddThreadRootsMarkingTaskClosure closure(worker_id);
2086     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
2087 
2088     // Mark from OopStorages
2089     {
2090       ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2091       PCMarkAndPushClosure closure(cm);
2092       _oop_storage_set_par_state.oops_do(&amp;closure);
2093       // Do the real work
2094       cm-&gt;follow_marking_stacks();
2095     }
2096 
2097     if (_active_workers &gt; 1) {
2098       steal_marking_work(_terminator, worker_id);
2099     }
2100   }
2101 };
2102 
2103 class PCRefProcTask : public AbstractGangTask {
2104   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2105   ProcessTask&amp; _task;
2106   uint _ergo_workers;
2107   TaskTerminator _terminator;
2108 
2109 public:
2110   PCRefProcTask(ProcessTask&amp; task, uint ergo_workers) :
2111       AbstractGangTask("PCRefProcTask"),
2112       _task(task),
2113       _ergo_workers(ergo_workers),
2114       _terminator(_ergo_workers, ParCompactionManager::oop_task_queues()) {
2115   }
2116 
2117   virtual void work(uint worker_id) {
2118     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2119     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2120 
2121     ParCompactionManager* cm =
2122       ParCompactionManager::gc_thread_compaction_manager(worker_id);
2123     PCMarkAndPushClosure mark_and_push_closure(cm);
2124     ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2125     _task.work(worker_id, *PSParallelCompact::is_alive_closure(),
2126                mark_and_push_closure, follow_stack_closure);
2127 
2128     steal_marking_work(_terminator, worker_id);
2129   }
2130 };
2131 
2132 class RefProcTaskExecutor: public AbstractRefProcTaskExecutor {
2133   void execute(ProcessTask&amp; process_task, uint ergo_workers) {
2134     assert(ParallelScavengeHeap::heap()-&gt;workers().active_workers() == ergo_workers,
2135            "Ergonomically chosen workers (%u) must be equal to active workers (%u)",
2136            ergo_workers, ParallelScavengeHeap::heap()-&gt;workers().active_workers());
2137 
2138     PCRefProcTask task(process_task, ergo_workers);
2139     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2140   }
2141 };
2142 
2143 void PSParallelCompact::marking_phase(ParCompactionManager* cm,
2144                                       bool maximum_heap_compaction,
2145                                       ParallelOldTracer *gc_tracer) {
2146   // Recursively traverse all live objects and mark them
2147   GCTraceTime(Info, gc, phases) tm("Marking Phase", &amp;_gc_timer);
2148 
2149   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2150   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2151 
2152   PCMarkAndPushClosure mark_and_push_closure(cm);
2153   ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2154 
2155   // Need new claim bits before marking starts.
2156   ClassLoaderDataGraph::clear_claimed_marks();
2157 
2158   {
2159     GCTraceTime(Debug, gc, phases) tm("Par Mark", &amp;_gc_timer);
2160 
2161     MarkFromRootsTask task(active_gc_threads);
2162     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2163   }
2164 
2165   // Process reference objects found during marking
2166   {
2167     GCTraceTime(Debug, gc, phases) tm("Reference Processing", &amp;_gc_timer);
2168 
2169     ReferenceProcessorStats stats;
2170     ReferenceProcessorPhaseTimes pt(&amp;_gc_timer, ref_processor()-&gt;max_num_queues());
2171 
2172     if (ref_processor()-&gt;processing_is_mt()) {
2173       ref_processor()-&gt;set_active_mt_degree(active_gc_threads);
2174 
2175       RefProcTaskExecutor task_executor;
2176       stats = ref_processor()-&gt;process_discovered_references(
2177         is_alive_closure(), &amp;mark_and_push_closure, &amp;follow_stack_closure,
2178         &amp;task_executor, &amp;pt);
2179     } else {
2180       stats = ref_processor()-&gt;process_discovered_references(
2181         is_alive_closure(), &amp;mark_and_push_closure, &amp;follow_stack_closure, NULL,
2182         &amp;pt);
2183     }
2184 
2185     gc_tracer-&gt;report_gc_reference_stats(stats);
2186     pt.print_all_references();
2187   }
2188 
2189   // This is the point where the entire marking should have completed.
2190   assert(cm-&gt;marking_stacks_empty(), "Marking should have completed");
2191 
2192   {
2193     GCTraceTime(Debug, gc, phases) tm("Weak Processing", &amp;_gc_timer);
2194     WeakProcessor::weak_oops_do(is_alive_closure(), &amp;do_nothing_cl);
2195   }
2196 
2197   {
2198     GCTraceTime(Debug, gc, phases) tm_m("Class Unloading", &amp;_gc_timer);
2199 
2200     // Follow system dictionary roots and unload classes.
2201     bool purged_class = SystemDictionary::do_unloading(&amp;_gc_timer);
2202 
2203     // Unload nmethods.
2204     CodeCache::do_unloading(is_alive_closure(), purged_class);
2205 
2206     // Prune dead klasses from subklass/sibling/implementor lists.
2207     Klass::clean_weak_klass_links(purged_class);
2208 
2209     // Clean JVMCI metadata handles.
2210     JVMCI_ONLY(JVMCI::do_unloading(purged_class));
2211   }
2212 
2213   _gc_tracer.report_object_count_after_gc(is_alive_closure());
2214 }
2215 
2216 void PSParallelCompact::adjust_roots(ParCompactionManager* cm) {
2217   // Adjust the pointers to reflect the new locations
2218   GCTraceTime(Info, gc, phases) tm("Adjust Roots", &amp;_gc_timer);
2219 
2220   // Need new claim bits when tracing through and adjusting pointers.
2221   ClassLoaderDataGraph::clear_claimed_marks();
2222 
2223   PCAdjustPointerClosure oop_closure(cm);
2224 
2225   // General strong roots.
2226   Threads::oops_do(&amp;oop_closure, NULL);
2227   ObjectSynchronizer::oops_do(&amp;oop_closure);
2228   OopStorageSet::strong_oops_do(&amp;oop_closure);
2229   CLDToOopClosure cld_closure(&amp;oop_closure, ClassLoaderData::_claim_strong);
2230   ClassLoaderDataGraph::cld_do(&amp;cld_closure);
2231 
2232   // Now adjust pointers in remaining weak roots.  (All of which should
2233   // have been cleared if they pointed to non-surviving objects.)
2234   WeakProcessor::oops_do(&amp;oop_closure);
2235 
2236   CodeBlobToOopClosure adjust_from_blobs(&amp;oop_closure, CodeBlobToOopClosure::FixRelocations);
2237   CodeCache::blobs_do(&amp;adjust_from_blobs);
2238   AOT_ONLY(AOTLoader::oops_do(&amp;oop_closure);)
2239 
2240   ref_processor()-&gt;weak_oops_do(&amp;oop_closure);
2241   // Roots were visited so references into the young gen in roots
2242   // may have been scanned.  Process them also.
2243   // Should the reference processor have a span that excludes
2244   // young gen objects?
2245   PSScavenge::reference_processor()-&gt;weak_oops_do(&amp;oop_closure);
2246 }
2247 
2248 // Helper class to print 8 region numbers per line and then print the total at the end.
2249 class FillableRegionLogger : public StackObj {
2250 private:
2251   Log(gc, compaction) log;
2252   static const int LineLength = 8;
2253   size_t _regions[LineLength];
2254   int _next_index;
2255   bool _enabled;
2256   size_t _total_regions;
2257 public:
2258   FillableRegionLogger() : _next_index(0), _enabled(log_develop_is_enabled(Trace, gc, compaction)), _total_regions(0) { }
2259   ~FillableRegionLogger() {
2260     log.trace(SIZE_FORMAT " initially fillable regions", _total_regions);
2261   }
2262 
2263   void print_line() {
2264     if (!_enabled || _next_index == 0) {
2265       return;
2266     }
2267     FormatBuffer&lt;&gt; line("Fillable: ");
2268     for (int i = 0; i &lt; _next_index; i++) {
2269       line.append(" " SIZE_FORMAT_W(7), _regions[i]);
2270     }
2271     log.trace("%s", line.buffer());
2272     _next_index = 0;
2273   }
2274 
2275   void handle(size_t region) {
2276     if (!_enabled) {
2277       return;
2278     }
2279     _regions[_next_index++] = region;
2280     if (_next_index == LineLength) {
2281       print_line();
2282     }
2283     _total_regions++;
2284   }
2285 };
2286 
2287 void PSParallelCompact::prepare_region_draining_tasks(uint parallel_gc_threads)
2288 {
2289   GCTraceTime(Trace, gc, phases) tm("Drain Task Setup", &amp;_gc_timer);
2290 
2291   // Find the threads that are active
2292   uint worker_id = 0;
2293 
2294   // Find all regions that are available (can be filled immediately) and
2295   // distribute them to the thread stacks.  The iteration is done in reverse
2296   // order (high to low) so the regions will be removed in ascending order.
2297 
2298   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2299 
2300   // id + 1 is used to test termination so unsigned  can
2301   // be used with an old_space_id == 0.
2302   FillableRegionLogger region_logger;
2303   for (unsigned int id = to_space_id; id + 1 &gt; old_space_id; --id) {
2304     SpaceInfo* const space_info = _space_info + id;
2305     MutableSpace* const space = space_info-&gt;space();
2306     HeapWord* const new_top = space_info-&gt;new_top();
2307 
2308     const size_t beg_region = sd.addr_to_region_idx(space_info-&gt;dense_prefix());
2309     const size_t end_region =
2310       sd.addr_to_region_idx(sd.region_align_up(new_top));
2311 
2312     for (size_t cur = end_region - 1; cur + 1 &gt; beg_region; --cur) {
2313       if (sd.region(cur)-&gt;claim_unsafe()) {
2314         ParCompactionManager* cm = ParCompactionManager::manager_array(worker_id);
2315         bool result = sd.region(cur)-&gt;mark_normal();
2316         assert(result, "Must succeed at this point.");
2317         cm-&gt;region_stack()-&gt;push(cur);
2318         region_logger.handle(cur);
2319         // Assign regions to tasks in round-robin fashion.
2320         if (++worker_id == parallel_gc_threads) {
2321           worker_id = 0;
2322         }
2323       }
2324     }
2325     region_logger.print_line();
2326   }
2327 }
2328 
2329 class TaskQueue : StackObj {
2330   volatile uint _counter;
2331   uint _size;
2332   uint _insert_index;
2333   PSParallelCompact::UpdateDensePrefixTask* _backing_array;
2334 public:
2335   explicit TaskQueue(uint size) : _counter(0), _size(size), _insert_index(0), _backing_array(NULL) {
2336     _backing_array = NEW_C_HEAP_ARRAY(PSParallelCompact::UpdateDensePrefixTask, _size, mtGC);
2337   }
2338   ~TaskQueue() {
2339     assert(_counter &gt;= _insert_index, "not all queue elements were claimed");
2340     FREE_C_HEAP_ARRAY(T, _backing_array);
2341   }
2342 
2343   void push(const PSParallelCompact::UpdateDensePrefixTask&amp; value) {
2344     assert(_insert_index &lt; _size, "too small backing array");
2345     _backing_array[_insert_index++] = value;
2346   }
2347 
2348   bool try_claim(PSParallelCompact::UpdateDensePrefixTask&amp; reference) {
2349     uint claimed = Atomic::fetch_and_add(&amp;_counter, 1u);
2350     if (claimed &lt; _insert_index) {
2351       reference = _backing_array[claimed];
2352       return true;
2353     } else {
2354       return false;
2355     }
2356   }
2357 };
2358 
2359 #define PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING 4
2360 
2361 void PSParallelCompact::enqueue_dense_prefix_tasks(TaskQueue&amp; task_queue,
2362                                                    uint parallel_gc_threads) {
2363   GCTraceTime(Trace, gc, phases) tm("Dense Prefix Task Setup", &amp;_gc_timer);
2364 
2365   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2366 
2367   // Iterate over all the spaces adding tasks for updating
2368   // regions in the dense prefix.  Assume that 1 gc thread
2369   // will work on opening the gaps and the remaining gc threads
2370   // will work on the dense prefix.
2371   unsigned int space_id;
2372   for (space_id = old_space_id; space_id &lt; last_space_id; ++ space_id) {
2373     HeapWord* const dense_prefix_end = _space_info[space_id].dense_prefix();
2374     const MutableSpace* const space = _space_info[space_id].space();
2375 
2376     if (dense_prefix_end == space-&gt;bottom()) {
2377       // There is no dense prefix for this space.
2378       continue;
2379     }
2380 
2381     // The dense prefix is before this region.
2382     size_t region_index_end_dense_prefix =
2383         sd.addr_to_region_idx(dense_prefix_end);
2384     RegionData* const dense_prefix_cp =
2385       sd.region(region_index_end_dense_prefix);
2386     assert(dense_prefix_end == space-&gt;end() ||
2387            dense_prefix_cp-&gt;available() ||
2388            dense_prefix_cp-&gt;claimed(),
2389            "The region after the dense prefix should always be ready to fill");
2390 
2391     size_t region_index_start = sd.addr_to_region_idx(space-&gt;bottom());
2392 
2393     // Is there dense prefix work?
2394     size_t total_dense_prefix_regions =
2395       region_index_end_dense_prefix - region_index_start;
2396     // How many regions of the dense prefix should be given to
2397     // each thread?
2398     if (total_dense_prefix_regions &gt; 0) {
2399       uint tasks_for_dense_prefix = 1;
2400       if (total_dense_prefix_regions &lt;=
2401           (parallel_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)) {
2402         // Don't over partition.  This assumes that
2403         // PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING is a small integer value
2404         // so there are not many regions to process.
2405         tasks_for_dense_prefix = parallel_gc_threads;
2406       } else {
2407         // Over partition
2408         tasks_for_dense_prefix = parallel_gc_threads *
2409           PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING;
2410       }
2411       size_t regions_per_thread = total_dense_prefix_regions /
2412         tasks_for_dense_prefix;
2413       // Give each thread at least 1 region.
2414       if (regions_per_thread == 0) {
2415         regions_per_thread = 1;
2416       }
2417 
2418       for (uint k = 0; k &lt; tasks_for_dense_prefix; k++) {
2419         if (region_index_start &gt;= region_index_end_dense_prefix) {
2420           break;
2421         }
2422         // region_index_end is not processed
2423         size_t region_index_end = MIN2(region_index_start + regions_per_thread,
2424                                        region_index_end_dense_prefix);
2425         task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),
2426                                               region_index_start,
2427                                               region_index_end));
2428         region_index_start = region_index_end;
2429       }
2430     }
2431     // This gets any part of the dense prefix that did not
2432     // fit evenly.
2433     if (region_index_start &lt; region_index_end_dense_prefix) {
2434       task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),
2435                                             region_index_start,
2436                                             region_index_end_dense_prefix));
2437     }
2438   }
2439 }
2440 
2441 #ifdef ASSERT
2442 // Write a histogram of the number of times the block table was filled for a
2443 // region.
2444 void PSParallelCompact::write_block_fill_histogram()
2445 {
2446   if (!log_develop_is_enabled(Trace, gc, compaction)) {
2447     return;
2448   }
2449 
2450   Log(gc, compaction) log;
2451   ResourceMark rm;
2452   LogStream ls(log.trace());
2453   outputStream* out = &amp;ls;
2454 
2455   typedef ParallelCompactData::RegionData rd_t;
2456   ParallelCompactData&amp; sd = summary_data();
2457 
2458   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2459     MutableSpace* const spc = _space_info[id].space();
2460     if (spc-&gt;bottom() != spc-&gt;top()) {
2461       const rd_t* const beg = sd.addr_to_region_ptr(spc-&gt;bottom());
2462       HeapWord* const top_aligned_up = sd.region_align_up(spc-&gt;top());
2463       const rd_t* const end = sd.addr_to_region_ptr(top_aligned_up);
2464 
2465       size_t histo[5] = { 0, 0, 0, 0, 0 };
2466       const size_t histo_len = sizeof(histo) / sizeof(size_t);
2467       const size_t region_cnt = pointer_delta(end, beg, sizeof(rd_t));
2468 
2469       for (const rd_t* cur = beg; cur &lt; end; ++cur) {
2470         ++histo[MIN2(cur-&gt;blocks_filled_count(), histo_len - 1)];
2471       }
2472       out-&gt;print("Block fill histogram: %u %-4s" SIZE_FORMAT_W(5), id, space_names[id], region_cnt);
2473       for (size_t i = 0; i &lt; histo_len; ++i) {
2474         out-&gt;print(" " SIZE_FORMAT_W(5) " %5.1f%%",
2475                    histo[i], 100.0 * histo[i] / region_cnt);
2476       }
2477       out-&gt;cr();
2478     }
2479   }
2480 }
2481 #endif // #ifdef ASSERT
2482 
2483 static void compaction_with_stealing_work(TaskTerminator* terminator, uint worker_id) {
2484   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), "called outside gc");
2485 
2486   ParCompactionManager* cm =
2487     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2488 
2489   // Drain the stacks that have been preloaded with regions
2490   // that are ready to fill.
2491 
2492   cm-&gt;drain_region_stacks();
2493 
2494   guarantee(cm-&gt;region_stack()-&gt;is_empty(), "Not empty");
2495 
2496   size_t region_index = 0;
2497 
2498   while (true) {
2499     if (ParCompactionManager::steal(worker_id, region_index)) {
2500       PSParallelCompact::fill_and_update_region(cm, region_index);
2501       cm-&gt;drain_region_stacks();
2502     } else if (PSParallelCompact::steal_unavailable_region(cm, region_index)) {
2503       // Fill and update an unavailable region with the help of a shadow region
2504       PSParallelCompact::fill_and_update_shadow_region(cm, region_index);
2505       cm-&gt;drain_region_stacks();
2506     } else {
2507       if (terminator-&gt;offer_termination()) {
2508         break;
2509       }
2510       // Go around again.
2511     }
2512   }
2513   return;
2514 }
2515 
2516 class UpdateDensePrefixAndCompactionTask: public AbstractGangTask {
2517   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2518   TaskQueue&amp; _tq;
2519   TaskTerminator _terminator;
2520   uint _active_workers;
2521 
2522 public:
2523   UpdateDensePrefixAndCompactionTask(TaskQueue&amp; tq, uint active_workers) :
2524       AbstractGangTask("UpdateDensePrefixAndCompactionTask"),
2525       _tq(tq),
2526       _terminator(active_workers, ParCompactionManager::region_task_queues()),
2527       _active_workers(active_workers) {
2528   }
2529   virtual void work(uint worker_id) {
2530     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2531 
2532     for (PSParallelCompact::UpdateDensePrefixTask task; _tq.try_claim(task); /* empty */) {
2533       PSParallelCompact::update_and_deadwood_in_dense_prefix(cm,
2534                                                              task._space_id,
2535                                                              task._region_index_start,
2536                                                              task._region_index_end);
2537     }
2538 
2539     // Once a thread has drained it's stack, it should try to steal regions from
2540     // other threads.
2541     compaction_with_stealing_work(&amp;_terminator, worker_id);
2542   }
2543 };
2544 
2545 void PSParallelCompact::compact() {
2546   GCTraceTime(Info, gc, phases) tm("Compaction Phase", &amp;_gc_timer);
2547 
2548   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2549   PSOldGen* old_gen = heap-&gt;old_gen();
2550   old_gen-&gt;start_array()-&gt;reset();
2551   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2552 
2553   // for [0..last_space_id)
2554   //     for [0..active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)
2555   //         push
2556   //     push
2557   //
2558   // max push count is thus: last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1)
2559   TaskQueue task_queue(last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1));
2560   initialize_shadow_regions(active_gc_threads);
2561   prepare_region_draining_tasks(active_gc_threads);
2562   enqueue_dense_prefix_tasks(task_queue, active_gc_threads);
2563 
2564   {
2565     GCTraceTime(Trace, gc, phases) tm("Par Compact", &amp;_gc_timer);
2566 
2567     UpdateDensePrefixAndCompactionTask task(task_queue, active_gc_threads);
2568     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2569 
2570 #ifdef  ASSERT
2571     // Verify that all regions have been processed before the deferred updates.
2572     for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2573       verify_complete(SpaceId(id));
2574     }
2575 #endif
2576   }
2577 
2578   {
2579     // Update the deferred objects, if any.  Any compaction manager can be used.
2580     GCTraceTime(Trace, gc, phases) tm("Deferred Updates", &amp;_gc_timer);
2581     ParCompactionManager* cm = ParCompactionManager::manager_array(0);
2582     for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2583       update_deferred_objects(cm, SpaceId(id));
2584     }
2585   }
2586 
2587   DEBUG_ONLY(write_block_fill_histogram());
2588 }
2589 
2590 #ifdef  ASSERT
2591 void PSParallelCompact::verify_complete(SpaceId space_id) {
2592   // All Regions between space bottom() to new_top() should be marked as filled
2593   // and all Regions between new_top() and top() should be available (i.e.,
2594   // should have been emptied).
2595   ParallelCompactData&amp; sd = summary_data();
2596   SpaceInfo si = _space_info[space_id];
2597   HeapWord* new_top_addr = sd.region_align_up(si.new_top());
2598   HeapWord* old_top_addr = sd.region_align_up(si.space()-&gt;top());
2599   const size_t beg_region = sd.addr_to_region_idx(si.space()-&gt;bottom());
2600   const size_t new_top_region = sd.addr_to_region_idx(new_top_addr);
2601   const size_t old_top_region = sd.addr_to_region_idx(old_top_addr);
2602 
2603   bool issued_a_warning = false;
2604 
2605   size_t cur_region;
2606   for (cur_region = beg_region; cur_region &lt; new_top_region; ++cur_region) {
2607     const RegionData* const c = sd.region(cur_region);
2608     if (!c-&gt;completed()) {
2609       log_warning(gc)("region " SIZE_FORMAT " not filled: destination_count=%u",
2610                       cur_region, c-&gt;destination_count());
2611       issued_a_warning = true;
2612     }
2613   }
2614 
2615   for (cur_region = new_top_region; cur_region &lt; old_top_region; ++cur_region) {
2616     const RegionData* const c = sd.region(cur_region);
2617     if (!c-&gt;available()) {
2618       log_warning(gc)("region " SIZE_FORMAT " not empty: destination_count=%u",
2619                       cur_region, c-&gt;destination_count());
2620       issued_a_warning = true;
2621     }
2622   }
2623 
2624   if (issued_a_warning) {
2625     print_region_ranges();
2626   }
2627 }
2628 #endif  // #ifdef ASSERT
2629 
2630 inline void UpdateOnlyClosure::do_addr(HeapWord* addr) {
2631   _start_array-&gt;allocate_block(addr);
2632   compaction_manager()-&gt;update_contents(oop(addr));
2633 }
2634 
2635 // Update interior oops in the ranges of regions [beg_region, end_region).
2636 void
2637 PSParallelCompact::update_and_deadwood_in_dense_prefix(ParCompactionManager* cm,
2638                                                        SpaceId space_id,
2639                                                        size_t beg_region,
2640                                                        size_t end_region) {
2641   ParallelCompactData&amp; sd = summary_data();
2642   ParMarkBitMap* const mbm = mark_bitmap();
2643 
2644   HeapWord* beg_addr = sd.region_to_addr(beg_region);
2645   HeapWord* const end_addr = sd.region_to_addr(end_region);
2646   assert(beg_region &lt;= end_region, "bad region range");
2647   assert(end_addr &lt;= dense_prefix(space_id), "not in the dense prefix");
2648 
2649 #ifdef  ASSERT
2650   // Claim the regions to avoid triggering an assert when they are marked as
2651   // filled.
2652   for (size_t claim_region = beg_region; claim_region &lt; end_region; ++claim_region) {
2653     assert(sd.region(claim_region)-&gt;claim_unsafe(), "claim() failed");
2654   }
2655 #endif  // #ifdef ASSERT
2656 
2657   if (beg_addr != space(space_id)-&gt;bottom()) {
2658     // Find the first live object or block of dead space that *starts* in this
2659     // range of regions.  If a partial object crosses onto the region, skip it;
2660     // it will be marked for 'deferred update' when the object head is
2661     // processed.  If dead space crosses onto the region, it is also skipped; it
2662     // will be filled when the prior region is processed.  If neither of those
2663     // apply, the first word in the region is the start of a live object or dead
2664     // space.
2665     assert(beg_addr &gt; space(space_id)-&gt;bottom(), "sanity");
2666     const RegionData* const cp = sd.region(beg_region);
2667     if (cp-&gt;partial_obj_size() != 0) {
2668       beg_addr = sd.partial_obj_end(beg_region);
2669     } else if (dead_space_crosses_boundary(cp, mbm-&gt;addr_to_bit(beg_addr))) {
2670       beg_addr = mbm-&gt;find_obj_beg(beg_addr, end_addr);
2671     }
2672   }
2673 
2674   if (beg_addr &lt; end_addr) {
2675     // A live object or block of dead space starts in this range of Regions.
2676      HeapWord* const dense_prefix_end = dense_prefix(space_id);
2677 
2678     // Create closures and iterate.
2679     UpdateOnlyClosure update_closure(mbm, cm, space_id);
2680     FillClosure fill_closure(cm, space_id);
2681     ParMarkBitMap::IterationStatus status;
2682     status = mbm-&gt;iterate(&amp;update_closure, &amp;fill_closure, beg_addr, end_addr,
2683                           dense_prefix_end);
2684     if (status == ParMarkBitMap::incomplete) {
2685       update_closure.do_addr(update_closure.source());
2686     }
2687   }
2688 
2689   // Mark the regions as filled.
2690   RegionData* const beg_cp = sd.region(beg_region);
2691   RegionData* const end_cp = sd.region(end_region);
2692   for (RegionData* cp = beg_cp; cp &lt; end_cp; ++cp) {
2693     cp-&gt;set_completed();
2694   }
2695 }
2696 
2697 // Return the SpaceId for the space containing addr.  If addr is not in the
2698 // heap, last_space_id is returned.  In debug mode it expects the address to be
2699 // in the heap and asserts such.
2700 PSParallelCompact::SpaceId PSParallelCompact::space_id(HeapWord* addr) {
2701   assert(ParallelScavengeHeap::heap()-&gt;is_in_reserved(addr), "addr not in the heap");
2702 
2703   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
2704     if (_space_info[id].space()-&gt;contains(addr)) {
2705       return SpaceId(id);
2706     }
2707   }
2708 
2709   assert(false, "no space contains the addr");
2710   return last_space_id;
2711 }
2712 
2713 void PSParallelCompact::update_deferred_objects(ParCompactionManager* cm,
2714                                                 SpaceId id) {
2715   assert(id &lt; last_space_id, "bad space id");
2716 
2717   ParallelCompactData&amp; sd = summary_data();
2718   const SpaceInfo* const space_info = _space_info + id;
2719   ObjectStartArray* const start_array = space_info-&gt;start_array();
2720 
2721   const MutableSpace* const space = space_info-&gt;space();
2722   assert(space_info-&gt;dense_prefix() &gt;= space-&gt;bottom(), "dense_prefix not set");
2723   HeapWord* const beg_addr = space_info-&gt;dense_prefix();
2724   HeapWord* const end_addr = sd.region_align_up(space_info-&gt;new_top());
2725 
2726   const RegionData* const beg_region = sd.addr_to_region_ptr(beg_addr);
2727   const RegionData* const end_region = sd.addr_to_region_ptr(end_addr);
2728   const RegionData* cur_region;
2729   for (cur_region = beg_region; cur_region &lt; end_region; ++cur_region) {
2730     HeapWord* const addr = cur_region-&gt;deferred_obj_addr();
2731     if (addr != NULL) {
2732       if (start_array != NULL) {
2733         start_array-&gt;allocate_block(addr);
2734       }
2735       cm-&gt;update_contents(oop(addr));
2736       assert(oopDesc::is_oop_or_null(oop(addr)), "Expected an oop or NULL at " PTR_FORMAT, p2i(oop(addr)));
2737     }
2738   }
2739 }
2740 
2741 // Skip over count live words starting from beg, and return the address of the
2742 // next live word.  Unless marked, the word corresponding to beg is assumed to
2743 // be dead.  Callers must either ensure beg does not correspond to the middle of
2744 // an object, or account for those live words in some other way.  Callers must
2745 // also ensure that there are enough live words in the range [beg, end) to skip.
2746 HeapWord*
2747 PSParallelCompact::skip_live_words(HeapWord* beg, HeapWord* end, size_t count)
2748 {
2749   assert(count &gt; 0, "sanity");
2750 
2751   ParMarkBitMap* m = mark_bitmap();
2752   idx_t bits_to_skip = m-&gt;words_to_bits(count);
2753   idx_t cur_beg = m-&gt;addr_to_bit(beg);
2754   const idx_t search_end = m-&gt;align_range_end(m-&gt;addr_to_bit(end));
2755 
2756   do {
2757     cur_beg = m-&gt;find_obj_beg(cur_beg, search_end);
2758     idx_t cur_end = m-&gt;find_obj_end(cur_beg, search_end);
2759     const size_t obj_bits = cur_end - cur_beg + 1;
2760     if (obj_bits &gt; bits_to_skip) {
2761       return m-&gt;bit_to_addr(cur_beg + bits_to_skip);
2762     }
2763     bits_to_skip -= obj_bits;
2764     cur_beg = cur_end + 1;
2765   } while (bits_to_skip &gt; 0);
2766 
2767   // Skipping the desired number of words landed just past the end of an object.
2768   // Find the start of the next object.
2769   cur_beg = m-&gt;find_obj_beg(cur_beg, search_end);
2770   assert(cur_beg &lt; m-&gt;addr_to_bit(end), "not enough live words to skip");
2771   return m-&gt;bit_to_addr(cur_beg);
2772 }
2773 
2774 HeapWord* PSParallelCompact::first_src_addr(HeapWord* const dest_addr,
2775                                             SpaceId src_space_id,
2776                                             size_t src_region_idx)
2777 {
2778   assert(summary_data().is_region_aligned(dest_addr), "not aligned");
2779 
2780   const SplitInfo&amp; split_info = _space_info[src_space_id].split_info();
2781   if (split_info.dest_region_addr() == dest_addr) {
2782     // The partial object ending at the split point contains the first word to
2783     // be copied to dest_addr.
2784     return split_info.first_src_addr();
2785   }
2786 
2787   const ParallelCompactData&amp; sd = summary_data();
2788   ParMarkBitMap* const bitmap = mark_bitmap();
2789   const size_t RegionSize = ParallelCompactData::RegionSize;
2790 
2791   assert(sd.is_region_aligned(dest_addr), "not aligned");
2792   const RegionData* const src_region_ptr = sd.region(src_region_idx);
2793   const size_t partial_obj_size = src_region_ptr-&gt;partial_obj_size();
2794   HeapWord* const src_region_destination = src_region_ptr-&gt;destination();
2795 
2796   assert(dest_addr &gt;= src_region_destination, "wrong src region");
2797   assert(src_region_ptr-&gt;data_size() &gt; 0, "src region cannot be empty");
2798 
2799   HeapWord* const src_region_beg = sd.region_to_addr(src_region_idx);
2800   HeapWord* const src_region_end = src_region_beg + RegionSize;
2801 
2802   HeapWord* addr = src_region_beg;
2803   if (dest_addr == src_region_destination) {
2804     // Return the first live word in the source region.
2805     if (partial_obj_size == 0) {
2806       addr = bitmap-&gt;find_obj_beg(addr, src_region_end);
2807       assert(addr &lt; src_region_end, "no objects start in src region");
2808     }
2809     return addr;
2810   }
2811 
2812   // Must skip some live data.
2813   size_t words_to_skip = dest_addr - src_region_destination;
2814   assert(src_region_ptr-&gt;data_size() &gt; words_to_skip, "wrong src region");
2815 
2816   if (partial_obj_size &gt;= words_to_skip) {
2817     // All the live words to skip are part of the partial object.
2818     addr += words_to_skip;
2819     if (partial_obj_size == words_to_skip) {
2820       // Find the first live word past the partial object.
2821       addr = bitmap-&gt;find_obj_beg(addr, src_region_end);
2822       assert(addr &lt; src_region_end, "wrong src region");
2823     }
2824     return addr;
2825   }
2826 
2827   // Skip over the partial object (if any).
2828   if (partial_obj_size != 0) {
2829     words_to_skip -= partial_obj_size;
2830     addr += partial_obj_size;
2831   }
2832 
2833   // Skip over live words due to objects that start in the region.
2834   addr = skip_live_words(addr, src_region_end, words_to_skip);
2835   assert(addr &lt; src_region_end, "wrong src region");
2836   return addr;
2837 }
2838 
2839 void PSParallelCompact::decrement_destination_counts(ParCompactionManager* cm,
2840                                                      SpaceId src_space_id,
2841                                                      size_t beg_region,
2842                                                      HeapWord* end_addr)
2843 {
2844   ParallelCompactData&amp; sd = summary_data();
2845 
2846 #ifdef ASSERT
2847   MutableSpace* const src_space = _space_info[src_space_id].space();
2848   HeapWord* const beg_addr = sd.region_to_addr(beg_region);
2849   assert(src_space-&gt;contains(beg_addr) || beg_addr == src_space-&gt;end(),
2850          "src_space_id does not match beg_addr");
2851   assert(src_space-&gt;contains(end_addr) || end_addr == src_space-&gt;end(),
2852          "src_space_id does not match end_addr");
2853 #endif // #ifdef ASSERT
2854 
2855   RegionData* const beg = sd.region(beg_region);
2856   RegionData* const end = sd.addr_to_region_ptr(sd.region_align_up(end_addr));
2857 
2858   // Regions up to new_top() are enqueued if they become available.
2859   HeapWord* const new_top = _space_info[src_space_id].new_top();
2860   RegionData* const enqueue_end =
2861     sd.addr_to_region_ptr(sd.region_align_up(new_top));
2862 
2863   for (RegionData* cur = beg; cur &lt; end; ++cur) {
2864     assert(cur-&gt;data_size() &gt; 0, "region must have live data");
2865     cur-&gt;decrement_destination_count();
2866     if (cur &lt; enqueue_end &amp;&amp; cur-&gt;available() &amp;&amp; cur-&gt;claim()) {
2867       if (cur-&gt;mark_normal()) {
2868         cm-&gt;push_region(sd.region(cur));
2869       } else if (cur-&gt;mark_copied()) {
2870         // Try to copy the content of the shadow region back to its corresponding
2871         // heap region if the shadow region is filled. Otherwise, the GC thread
2872         // fills the shadow region will copy the data back (see
2873         // MoveAndUpdateShadowClosure::complete_region).
2874         copy_back(sd.region_to_addr(cur-&gt;shadow_region()), sd.region_to_addr(cur));
2875         ParCompactionManager::push_shadow_region_mt_safe(cur-&gt;shadow_region());
2876         cur-&gt;set_completed();
2877       }
2878     }
2879   }
2880 }
2881 
2882 size_t PSParallelCompact::next_src_region(MoveAndUpdateClosure&amp; closure,
2883                                           SpaceId&amp; src_space_id,
2884                                           HeapWord*&amp; src_space_top,
2885                                           HeapWord* end_addr)
2886 {
2887   typedef ParallelCompactData::RegionData RegionData;
2888 
2889   ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
2890   const size_t region_size = ParallelCompactData::RegionSize;
2891 
2892   size_t src_region_idx = 0;
2893 
2894   // Skip empty regions (if any) up to the top of the space.
2895   HeapWord* const src_aligned_up = sd.region_align_up(end_addr);
2896   RegionData* src_region_ptr = sd.addr_to_region_ptr(src_aligned_up);
2897   HeapWord* const top_aligned_up = sd.region_align_up(src_space_top);
2898   const RegionData* const top_region_ptr =
2899     sd.addr_to_region_ptr(top_aligned_up);
2900   while (src_region_ptr &lt; top_region_ptr &amp;&amp; src_region_ptr-&gt;data_size() == 0) {
2901     ++src_region_ptr;
2902   }
2903 
2904   if (src_region_ptr &lt; top_region_ptr) {
2905     // The next source region is in the current space.  Update src_region_idx
2906     // and the source address to match src_region_ptr.
2907     src_region_idx = sd.region(src_region_ptr);
2908     HeapWord* const src_region_addr = sd.region_to_addr(src_region_idx);
2909     if (src_region_addr &gt; closure.source()) {
2910       closure.set_source(src_region_addr);
2911     }
2912     return src_region_idx;
2913   }
2914 
2915   // Switch to a new source space and find the first non-empty region.
2916   unsigned int space_id = src_space_id + 1;
2917   assert(space_id &lt; last_space_id, "not enough spaces");
2918 
2919   HeapWord* const destination = closure.destination();
2920 
2921   do {
2922     MutableSpace* space = _space_info[space_id].space();
2923     HeapWord* const bottom = space-&gt;bottom();
2924     const RegionData* const bottom_cp = sd.addr_to_region_ptr(bottom);
2925 
2926     // Iterate over the spaces that do not compact into themselves.
2927     if (bottom_cp-&gt;destination() != bottom) {
2928       HeapWord* const top_aligned_up = sd.region_align_up(space-&gt;top());
2929       const RegionData* const top_cp = sd.addr_to_region_ptr(top_aligned_up);
2930 
2931       for (const RegionData* src_cp = bottom_cp; src_cp &lt; top_cp; ++src_cp) {
2932         if (src_cp-&gt;live_obj_size() &gt; 0) {
2933           // Found it.
2934           assert(src_cp-&gt;destination() == destination,
2935                  "first live obj in the space must match the destination");
2936           assert(src_cp-&gt;partial_obj_size() == 0,
2937                  "a space cannot begin with a partial obj");
2938 
2939           src_space_id = SpaceId(space_id);
2940           src_space_top = space-&gt;top();
2941           const size_t src_region_idx = sd.region(src_cp);
2942           closure.set_source(sd.region_to_addr(src_region_idx));
2943           return src_region_idx;
2944         } else {
2945           assert(src_cp-&gt;data_size() == 0, "sanity");
2946         }
2947       }
2948     }
2949   } while (++space_id &lt; last_space_id);
2950 
2951   assert(false, "no source region was found");
2952   return 0;
2953 }
2954 
2955 void PSParallelCompact::fill_region(ParCompactionManager* cm, MoveAndUpdateClosure&amp; closure, size_t region_idx)
2956 {
2957   typedef ParMarkBitMap::IterationStatus IterationStatus;
2958   ParMarkBitMap* const bitmap = mark_bitmap();
2959   ParallelCompactData&amp; sd = summary_data();
2960   RegionData* const region_ptr = sd.region(region_idx);
2961 
2962   // Get the source region and related info.
2963   size_t src_region_idx = region_ptr-&gt;source_region();
2964   SpaceId src_space_id = space_id(sd.region_to_addr(src_region_idx));
2965   HeapWord* src_space_top = _space_info[src_space_id].space()-&gt;top();
2966   HeapWord* dest_addr = sd.region_to_addr(region_idx);
2967 
2968   closure.set_source(first_src_addr(dest_addr, src_space_id, src_region_idx));
2969 
2970   // Adjust src_region_idx to prepare for decrementing destination counts (the
2971   // destination count is not decremented when a region is copied to itself).
2972   if (src_region_idx == region_idx) {
2973     src_region_idx += 1;
2974   }
2975 
2976   if (bitmap-&gt;is_unmarked(closure.source())) {
2977     // The first source word is in the middle of an object; copy the remainder
2978     // of the object or as much as will fit.  The fact that pointer updates were
2979     // deferred will be noted when the object header is processed.
2980     HeapWord* const old_src_addr = closure.source();
2981     closure.copy_partial_obj();
2982     if (closure.is_full()) {
2983       decrement_destination_counts(cm, src_space_id, src_region_idx,
2984                                    closure.source());
2985       region_ptr-&gt;set_deferred_obj_addr(NULL);
2986       closure.complete_region(cm, dest_addr, region_ptr);
2987       return;
2988     }
2989 
2990     HeapWord* const end_addr = sd.region_align_down(closure.source());
2991     if (sd.region_align_down(old_src_addr) != end_addr) {
2992       // The partial object was copied from more than one source region.
2993       decrement_destination_counts(cm, src_space_id, src_region_idx, end_addr);
2994 
2995       // Move to the next source region, possibly switching spaces as well.  All
2996       // args except end_addr may be modified.
2997       src_region_idx = next_src_region(closure, src_space_id, src_space_top,
2998                                        end_addr);
2999     }
3000   }
3001 
3002   do {
3003     HeapWord* const cur_addr = closure.source();
3004     HeapWord* const end_addr = MIN2(sd.region_align_up(cur_addr + 1),
3005                                     src_space_top);
3006     IterationStatus status = bitmap-&gt;iterate(&amp;closure, cur_addr, end_addr);
3007 
3008     if (status == ParMarkBitMap::incomplete) {
3009       // The last obj that starts in the source region does not end in the
3010       // region.
3011       assert(closure.source() &lt; end_addr, "sanity");
3012       HeapWord* const obj_beg = closure.source();
3013       HeapWord* const range_end = MIN2(obj_beg + closure.words_remaining(),
3014                                        src_space_top);
3015       HeapWord* const obj_end = bitmap-&gt;find_obj_end(obj_beg, range_end);
3016       if (obj_end &lt; range_end) {
3017         // The end was found; the entire object will fit.
3018         status = closure.do_addr(obj_beg, bitmap-&gt;obj_size(obj_beg, obj_end));
3019         assert(status != ParMarkBitMap::would_overflow, "sanity");
3020       } else {
3021         // The end was not found; the object will not fit.
3022         assert(range_end &lt; src_space_top, "obj cannot cross space boundary");
3023         status = ParMarkBitMap::would_overflow;
3024       }
3025     }
3026 
3027     if (status == ParMarkBitMap::would_overflow) {
3028       // The last object did not fit.  Note that interior oop updates were
3029       // deferred, then copy enough of the object to fill the region.
3030       region_ptr-&gt;set_deferred_obj_addr(closure.destination());
3031       status = closure.copy_until_full(); // copies from closure.source()
3032 
3033       decrement_destination_counts(cm, src_space_id, src_region_idx,
3034                                    closure.source());
3035       closure.complete_region(cm, dest_addr, region_ptr);
3036       return;
3037     }
3038 
3039     if (status == ParMarkBitMap::full) {
3040       decrement_destination_counts(cm, src_space_id, src_region_idx,
3041                                    closure.source());
3042       region_ptr-&gt;set_deferred_obj_addr(NULL);
3043       closure.complete_region(cm, dest_addr, region_ptr);
3044       return;
3045     }
3046 
3047     decrement_destination_counts(cm, src_space_id, src_region_idx, end_addr);
3048 
3049     // Move to the next source region, possibly switching spaces as well.  All
3050     // args except end_addr may be modified.
3051     src_region_idx = next_src_region(closure, src_space_id, src_space_top,
3052                                      end_addr);
3053   } while (true);
3054 }
3055 
3056 void PSParallelCompact::fill_and_update_region(ParCompactionManager* cm, size_t region_idx)
3057 {
3058   MoveAndUpdateClosure cl(mark_bitmap(), cm, region_idx);
3059   fill_region(cm, cl, region_idx);
3060 }
3061 
3062 void PSParallelCompact::fill_and_update_shadow_region(ParCompactionManager* cm, size_t region_idx)
3063 {
3064   // Get a shadow region first
3065   ParallelCompactData&amp; sd = summary_data();
3066   RegionData* const region_ptr = sd.region(region_idx);
3067   size_t shadow_region = ParCompactionManager::pop_shadow_region_mt_safe(region_ptr);
3068   // The InvalidShadow return value indicates the corresponding heap region is available,
3069   // so use MoveAndUpdateClosure to fill the normal region. Otherwise, use
3070   // MoveAndUpdateShadowClosure to fill the acquired shadow region.
3071   if (shadow_region == ParCompactionManager::InvalidShadow) {
3072     MoveAndUpdateClosure cl(mark_bitmap(), cm, region_idx);
3073     region_ptr-&gt;shadow_to_normal();
3074     return fill_region(cm, cl, region_idx);
3075   } else {
3076     MoveAndUpdateShadowClosure cl(mark_bitmap(), cm, region_idx, shadow_region);
3077     return fill_region(cm, cl, region_idx);
3078   }
3079 }
3080 
3081 void PSParallelCompact::copy_back(HeapWord *shadow_addr, HeapWord *region_addr)
3082 {
3083   Copy::aligned_conjoint_words(shadow_addr, region_addr, _summary_data.RegionSize);
3084 }
3085 
3086 bool PSParallelCompact::steal_unavailable_region(ParCompactionManager* cm, size_t &amp;region_idx)
3087 {
3088   size_t next = cm-&gt;next_shadow_region();
3089   ParallelCompactData&amp; sd = summary_data();
3090   size_t old_new_top = sd.addr_to_region_idx(_space_info[old_space_id].new_top());
3091   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
3092 
3093   while (next &lt; old_new_top) {
3094     if (sd.region(next)-&gt;mark_shadow()) {
3095       region_idx = next;
3096       return true;
3097     }
3098     next = cm-&gt;move_next_shadow_region_by(active_gc_threads);
3099   }
3100 
3101   return false;
3102 }
3103 
3104 // The shadow region is an optimization to address region dependencies in full GC. The basic
3105 // idea is making more regions available by temporally storing their live objects in empty
3106 // shadow regions to resolve dependencies between them and the destination regions. Therefore,
3107 // GC threads need not wait destination regions to be available before processing sources.
3108 //
3109 // A typical workflow would be:
3110 // After draining its own stack and failing to steal from others, a GC worker would pick an
3111 // unavailable region (destination count &gt; 0) and get a shadow region. Then the worker fills
3112 // the shadow region by copying live objects from source regions of the unavailable one. Once
3113 // the unavailable region becomes available, the data in the shadow region will be copied back.
3114 // Shadow regions are empty regions in the to-space and regions between top and end of other spaces.
3115 //
3116 // For more details, please refer to 4.2 of the VEE'19 paper:
3117 // Haoyu Li, Mingyu Wu, Binyu Zang, and Haibo Chen. 2019. ScissorGC: scalable and efficient
3118 // compaction for Java full garbage collection. In Proceedings of the 15th ACM SIGPLAN/SIGOPS
3119 // International Conference on Virtual Execution Environments (VEE 2019). ACM, New York, NY, USA,
3120 // 108-121. DOI: https://doi.org/10.1145/3313808.3313820
3121 void PSParallelCompact::initialize_shadow_regions(uint parallel_gc_threads)
3122 {
3123   const ParallelCompactData&amp; sd = PSParallelCompact::summary_data();
3124 
3125   for (unsigned int id = old_space_id; id &lt; last_space_id; ++id) {
3126     SpaceInfo* const space_info = _space_info + id;
3127     MutableSpace* const space = space_info-&gt;space();
3128 
3129     const size_t beg_region =
3130       sd.addr_to_region_idx(sd.region_align_up(MAX2(space_info-&gt;new_top(), space-&gt;top())));
3131     const size_t end_region =
3132       sd.addr_to_region_idx(sd.region_align_down(space-&gt;end()));
3133 
3134     for (size_t cur = beg_region; cur &lt; end_region; ++cur) {
3135       ParCompactionManager::push_shadow_region(cur);
3136     }
3137   }
3138 
3139   size_t beg_region = sd.addr_to_region_idx(_space_info[old_space_id].dense_prefix());
3140   for (uint i = 0; i &lt; parallel_gc_threads; i++) {
3141     ParCompactionManager *cm = ParCompactionManager::manager_array(i);
3142     cm-&gt;set_next_shadow_region(beg_region + i);
3143   }
3144 }
3145 
3146 void PSParallelCompact::fill_blocks(size_t region_idx)
3147 {
3148   // Fill in the block table elements for the specified region.  Each block
3149   // table element holds the number of live words in the region that are to the
3150   // left of the first object that starts in the block.  Thus only blocks in
3151   // which an object starts need to be filled.
3152   //
3153   // The algorithm scans the section of the bitmap that corresponds to the
3154   // region, keeping a running total of the live words.  When an object start is
3155   // found, if it's the first to start in the block that contains it, the
3156   // current total is written to the block table element.
3157   const size_t Log2BlockSize = ParallelCompactData::Log2BlockSize;
3158   const size_t Log2RegionSize = ParallelCompactData::Log2RegionSize;
3159   const size_t RegionSize = ParallelCompactData::RegionSize;
3160 
3161   ParallelCompactData&amp; sd = summary_data();
3162   const size_t partial_obj_size = sd.region(region_idx)-&gt;partial_obj_size();
3163   if (partial_obj_size &gt;= RegionSize) {
3164     return; // No objects start in this region.
3165   }
3166 
3167   // Ensure the first loop iteration decides that the block has changed.
3168   size_t cur_block = sd.block_count();
3169 
3170   const ParMarkBitMap* const bitmap = mark_bitmap();
3171 
3172   const size_t Log2BitsPerBlock = Log2BlockSize - LogMinObjAlignment;
3173   assert((size_t)1 &lt;&lt; Log2BitsPerBlock ==
3174          bitmap-&gt;words_to_bits(ParallelCompactData::BlockSize), "sanity");
3175 
3176   size_t beg_bit = bitmap-&gt;words_to_bits(region_idx &lt;&lt; Log2RegionSize);
3177   const size_t range_end = beg_bit + bitmap-&gt;words_to_bits(RegionSize);
3178   size_t live_bits = bitmap-&gt;words_to_bits(partial_obj_size);
3179   beg_bit = bitmap-&gt;find_obj_beg(beg_bit + live_bits, range_end);
3180   while (beg_bit &lt; range_end) {
3181     const size_t new_block = beg_bit &gt;&gt; Log2BitsPerBlock;
3182     if (new_block != cur_block) {
3183       cur_block = new_block;
3184       sd.block(cur_block)-&gt;set_offset(bitmap-&gt;bits_to_words(live_bits));
3185     }
3186 
3187     const size_t end_bit = bitmap-&gt;find_obj_end(beg_bit, range_end);
3188     if (end_bit &lt; range_end - 1) {
3189       live_bits += end_bit - beg_bit + 1;
3190       beg_bit = bitmap-&gt;find_obj_beg(end_bit + 1, range_end);
3191     } else {
3192       return;
3193     }
3194   }
3195 }
3196 
3197 ParMarkBitMap::IterationStatus MoveAndUpdateClosure::copy_until_full()
3198 {
3199   if (source() != copy_destination()) {
3200     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3201     Copy::aligned_conjoint_words(source(), copy_destination(), words_remaining());
3202   }
3203   update_state(words_remaining());
3204   assert(is_full(), "sanity");
3205   return ParMarkBitMap::full;
3206 }
3207 
3208 void MoveAndUpdateClosure::copy_partial_obj()
3209 {
3210   size_t words = words_remaining();
3211 
3212   HeapWord* const range_end = MIN2(source() + words, bitmap()-&gt;region_end());
3213   HeapWord* const end_addr = bitmap()-&gt;find_obj_end(source(), range_end);
3214   if (end_addr &lt; range_end) {
3215     words = bitmap()-&gt;obj_size(source(), end_addr);
3216   }
3217 
3218   // This test is necessary; if omitted, the pointer updates to a partial object
3219   // that crosses the dense prefix boundary could be overwritten.
3220   if (source() != copy_destination()) {
3221     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3222     Copy::aligned_conjoint_words(source(), copy_destination(), words);
3223   }
3224   update_state(words);
3225 }
3226 
3227 void MoveAndUpdateClosure::complete_region(ParCompactionManager *cm, HeapWord *dest_addr,
3228                                            PSParallelCompact::RegionData *region_ptr) {
3229   assert(region_ptr-&gt;shadow_state() == ParallelCompactData::RegionData::NormalRegion, "Region should be finished");
3230   region_ptr-&gt;set_completed();
3231 }
3232 
3233 ParMarkBitMapClosure::IterationStatus
3234 MoveAndUpdateClosure::do_addr(HeapWord* addr, size_t words) {
3235   assert(destination() != NULL, "sanity");
3236   assert(bitmap()-&gt;obj_size(addr) == words, "bad size");
3237 
3238   _source = addr;
3239   assert(PSParallelCompact::summary_data().calc_new_pointer(source(), compaction_manager()) ==
3240          destination(), "wrong destination");
3241 
3242   if (words &gt; words_remaining()) {
3243     return ParMarkBitMap::would_overflow;
3244   }
3245 
3246   // The start_array must be updated even if the object is not moving.
3247   if (_start_array != NULL) {
3248     _start_array-&gt;allocate_block(destination());
3249   }
3250 
3251   if (copy_destination() != source()) {
3252     DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)
3253     Copy::aligned_conjoint_words(source(), copy_destination(), words);
3254   }
3255 
3256   oop moved_oop = (oop) copy_destination();
3257   compaction_manager()-&gt;update_contents(moved_oop);
3258   assert(oopDesc::is_oop_or_null(moved_oop), "Expected an oop or NULL at " PTR_FORMAT, p2i(moved_oop));
3259 
3260   update_state(words);
3261   assert(copy_destination() == cast_from_oop&lt;HeapWord*&gt;(moved_oop) + moved_oop-&gt;size(), "sanity");
3262   return is_full() ? ParMarkBitMap::full : ParMarkBitMap::incomplete;
3263 }
3264 
3265 void MoveAndUpdateShadowClosure::complete_region(ParCompactionManager *cm, HeapWord *dest_addr,
3266                                                  PSParallelCompact::RegionData *region_ptr) {
3267   assert(region_ptr-&gt;shadow_state() == ParallelCompactData::RegionData::ShadowRegion, "Region should be shadow");
3268   // Record the shadow region index
3269   region_ptr-&gt;set_shadow_region(_shadow);
3270   // Mark the shadow region as filled to indicate the data is ready to be
3271   // copied back
3272   region_ptr-&gt;mark_filled();
3273   // Try to copy the content of the shadow region back to its corresponding
3274   // heap region if available; the GC thread that decreases the destination
3275   // count to zero will do the copying otherwise (see
3276   // PSParallelCompact::decrement_destination_counts).
3277   if (((region_ptr-&gt;available() &amp;&amp; region_ptr-&gt;claim()) || region_ptr-&gt;claimed()) &amp;&amp; region_ptr-&gt;mark_copied()) {
3278     region_ptr-&gt;set_completed();
3279     PSParallelCompact::copy_back(PSParallelCompact::summary_data().region_to_addr(_shadow), dest_addr);
3280     ParCompactionManager::push_shadow_region_mt_safe(_shadow);
3281   }
3282 }
3283 
3284 UpdateOnlyClosure::UpdateOnlyClosure(ParMarkBitMap* mbm,
3285                                      ParCompactionManager* cm,
3286                                      PSParallelCompact::SpaceId space_id) :
3287   ParMarkBitMapClosure(mbm, cm),
3288   _space_id(space_id),
3289   _start_array(PSParallelCompact::start_array(space_id))
3290 {
3291 }
3292 
3293 // Updates the references in the object to their new values.
3294 ParMarkBitMapClosure::IterationStatus
3295 UpdateOnlyClosure::do_addr(HeapWord* addr, size_t words) {
3296   do_addr(addr);
3297   return ParMarkBitMap::incomplete;
3298 }
3299 
3300 FillClosure::FillClosure(ParCompactionManager* cm, PSParallelCompact::SpaceId space_id) :
3301   ParMarkBitMapClosure(PSParallelCompact::mark_bitmap(), cm),
3302   _start_array(PSParallelCompact::start_array(space_id))
3303 {
3304   assert(space_id == PSParallelCompact::old_space_id,
3305          "cannot use FillClosure in the young gen");
3306 }
3307 
3308 ParMarkBitMapClosure::IterationStatus
3309 FillClosure::do_addr(HeapWord* addr, size_t size) {
3310   CollectedHeap::fill_with_objects(addr, size);
3311   HeapWord* const end = addr + size;
3312   do {
3313     _start_array-&gt;allocate_block(addr);
3314     addr += oop(addr)-&gt;size();
3315   } while (addr &lt; end);
3316   return ParMarkBitMap::incomplete;
3317 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
